#-*- coding:utf-8 -*-
import base64
import tempfile
import streamlit as st
import pandas as pd
import os
from script.correlation import plot_corr_sample, plot_corr_gene, create_gene_network
from script.pca import run_pca
from script.differential import run_deseq, plot_volcano
from script.enrich import run_enrich, plot_kegg_chart, plot_go_chart
from script.heatmap import run_heatmap
# streamlit run app.py
# 设置工作目录
workdir = '/Users/dongjiacheng/Desktop/web/omicpy'
# 设置页面标题
st.title("Bulk RNA-seq 分析工具")
st.markdown("###### 欢迎，这是一个基于Python的组学分析工具，可以生成高质量的图片与交互式的页面")
# 分割线
st.markdown("---")
# 设置侧边栏
st.sidebar.markdown("### 选择功能页面")
page = st.sidebar.radio("功能列表", ["相关性分析", "主成分分析", "差异分析", "富集分析", "热图绘制"])
# 文件下载功能
def get_binary_file_downloader_html(bin_file, file_label='File'):
    with open(bin_file, 'rb') as f:
        data = f.read()
    bin_str = base64.b64encode(data).decode()
    href = f'<a href="data:file/octet-stream;base64,{bin_str}" download="{os.path.basename(bin_file)}">{file_label}</a>'
    return href
@st.cache_resource
def get_corr_data_sample_csv():
    corr_data_path = os.path.join(workdir, 'input_file', 'expression_matrix_corr.csv')
    with open(corr_data_path, "r", encoding="utf-8") as f:  # Open as regular text file
        data = f.read()
    return data
def corr_page():
    # 下载示例CSV文件
    corr_file = get_corr_data_sample_csv()
    st.download_button(
        label="下载示例文件",
        data=corr_file,
        file_name='expression_matrix_corr.csv',
        mime="text/csv;charset=utf-8",
    )
    uploaded_file = st.file_uploader("上传标准化后的表达转录水平数据", type=["csv"])
    if uploaded_file is not None:
        data = pd.read_csv(uploaded_file)
        st.write(data) 
        # 用户输入区域
        st.sidebar.markdown("### 选择你的参数")
        width = st.sidebar.number_input("设置图片宽度", 900, 2000, 900)
        height = st.sidebar.number_input("设置图片高度", 600, 2000, 600)
        color = st.sidebar.selectbox("选择颜色方案", ['gnbu', 'geyser', 'portland', 'Temps'])
        method = st.sidebar.selectbox("选择相关性计算方法", ['pearson', 'spearman'])
        if st.button("生成样本相关性热图"):
            with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name
            # Assuming plot_corr_sample returns a Plotly figure object and not file paths
            output_image_path, output_html_path = plot_corr_sample(workdir, 
                                                                   tmp_file_path, 
                                                                   width, 
                                                                   height, 
                                                                   color, 
                                                                   method)
            st.image(output_image_path)
            st.markdown(get_binary_file_downloader_html(output_image_path, 
                                                        'Download Image'), 
                                                        unsafe_allow_html=True)
            st.markdown(get_binary_file_downloader_html(output_html_path, 
                                                        'Download HTML'), 
                                                        unsafe_allow_html=True)
            os.remove(tmp_file_path)
        if st.button("生成基因相关性热图"):
            with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name
            # Assuming plot_corr_gene returns a Plotly figure object and not file paths
            output_image_path, output_html_path = plot_corr_gene(workdir, 
                                                                 tmp_file_path, 
                                                                 width, height, 
                                                                 color, 
                                                                 method)
            st.image(output_image_path)
            st.markdown(get_binary_file_downloader_html(output_image_path,
                                                         'Download Image'), 
                                                         unsafe_allow_html=True)
            st.markdown(get_binary_file_downloader_html(output_html_path, 
                                                        'Download HTML'),
                                                          unsafe_allow_html=True)
            os.remove(tmp_file_path)
        if st.button("生成基因网络图"):
            with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name
            # Assuming create_gene_network returns a Plotly figure object and not file paths
            output_image_path, output_html_path = create_gene_network(workdir, 
                                                                      tmp_file_path, 
                                                                      width, 
                                                                      height)
            st.image(output_image_path)
            st.markdown(get_binary_file_downloader_html(output_image_path, 
                                                        'Download Image'), 
                                                        unsafe_allow_html=True)
            st.markdown(get_binary_file_downloader_html(output_html_path, 
                                                        'Download HTML'), 
                                                        unsafe_allow_html=True)
            os.remove(tmp_file_path)
@st.cache_resource
def get_pca_data_sample1_csv():
    pca_data_path = os.path.join(workdir, 
                                 'input_file', 
                                 'expression_matrix_pca.csv')
    with open(pca_data_path, "r", encoding="utf-8") as f:
        data = f.read()
    return data
@st.cache_resource  
def get_pca_data_sample2_csv():
    pca_data_path = os.path.join(workdir, 
                                 'input_file', 
                                 'sample_info_pca.csv')
    with open(pca_data_path, "r", encoding="utf-8") as f:
        data = f.read()
    return data
def pca_page():
    st.markdown("如果你不知道上传什么文件，可以点击下方的下载示例CSV文件按钮")
    col1, col2 = st.columns(2)
    with col1:
        st.download_button(
            label="下载基因表达水平数据(ReadCount)",
            data=get_pca_data_sample1_csv(),
            file_name="expression_matrix_pca.csv",
            mime="text/csv;charset=utf-8",
        )
    with col2:
        st.download_button(
            label="下载样本分组数据",
            data=get_pca_data_sample2_csv(),
            file_name="sample_info_pca.csv",
            mime="text/csv;charset=utf-8",
        )
    # 文件上传
    uploaded_file1 = st.file_uploader("上传表达转录水平数据", type=["csv"])
    uploaded_file2 = st.file_uploader("上传样本分组数据", type=["csv"])
    if uploaded_file1 and uploaded_file2:
        data1 = pd.read_csv(uploaded_file1, index_col=0)
        data2 = pd.read_csv(uploaded_file2, index_col=0)
        st.write(data1)
        st.write(data2)
        if st.button("生成主成分分析图"):
            if uploaded_file1 and uploaded_file2:
                with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp_file1, tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp_file2:
                    tmp_file1.write(uploaded_file1.getvalue())
                    tmp_file2.write(uploaded_file2.getvalue())
                    tmp_file_path1 = tmp_file1.name
                    tmp_file_path2 = tmp_file2.name
                # Assuming run_pca returns the output of the R script
                output_image_path = os.path.join(workdir,
                                                 'output_file' ,
                                                 'pca.png')
                output_html_path = os.path.join(workdir,
                                                'output_file' ,
                                                'pca.html')
                output = run_pca(workdir, 
                                 tmp_file_path1, 
                                 tmp_file_path2, 
                                 output_image_path, 
                                 output_html_path)
                st.image(output_image_path)
                st.markdown(get_binary_file_downloader_html(output_image_path, 
                                                            'Download Image'),
                                                              unsafe_allow_html=True)
                st.markdown(get_binary_file_downloader_html(output_html_path, 
                                                            'Download HTML'), 
                                                            unsafe_allow_html=True)
                os.remove(tmp_file_path1)
                os.remove(tmp_file_path2)
@st.cache_resource
def get_diff_data_sample_csv():
    diff_data_path = os.path.join(workdir, 'input_file', 'expression_matrix_deseq2.csv')
    with open(diff_data_path, "r", encoding="utf-8") as f:
        data = f.read()
    return data
def diff_page():
    # 下载示例CSV文件
    diff_file = get_diff_data_sample_csv()
    st.download_button(
        label="下载示例文件",
        data=diff_file,
        file_name='expression_matrix_deseq2.csv',
        mime="text/csv;charset=utf-8",
    )
    uploaded_file = st.file_uploader("上传标准化后的表达转录水平数据", type=["csv"])
    if uploaded_file is not None:
        data = pd.read_csv(uploaded_file)
        st.write(data)
        # 用户输入区域
        st.sidebar.markdown("### 选择你的参数")
        width = int(st.sidebar.number_input("设置图片宽度", 900, 2000, 1200))
        height = st.sidebar.number_input("设置图片高度", 600, 2000, 1000)
        repetition = st.sidebar.number_input("设置重复次数", 3)
        p_threshold = st.sidebar.number_input("设置p值阈值", 0.05)
        logFC_threshold = st.sidebar.number_input("设置log2fc阈值", 1)
        color_schemes = st.sidebar.selectbox("选择颜色方案", ['1', '2', '3', '4'])
        bubble_size = st.sidebar.number_input("设置气泡大小", 1, 20, 8)
        opacity = st.sidebar.number_input("设置透明度", 0.1, 1.0, 0.8)
        # 是否显示上下调基因信息
        up_donw_info = st.sidebar.checkbox("是否显示上下调基因信息", value=True)
        # 让用户输入基因列表
        genelist = st.sidebar.text_input("输入基因列表", value='')
        if st.button("进行差异基因分析"):
            with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name
            # Assuming plot_volcano returns a Plotly figure object and not file paths
            output_deseq2_result_path = os.path.join(workdir, 'output_file', 'deseq2_result.tsv')
            run_deseq(workdir, tmp_file_path,output_deseq2_result_path, repetition)
            output_image_path = os.path.join(workdir, 'output_file', 'volcano.png')
            output_html_path = os.path.join(workdir, 'output_file', 'volcano.html')
            output_up_genelist_path = os.path.join(workdir, 'output_file', 'upregulated_genes.txt')
            output_down_genelist_path = os.path.join(workdir, 'output_file', 'downregulated_genes.txt')
            plot_volcano(workdir, output_deseq2_result_path, genelist, width, height, p_threshold, logFC_threshold, color_schemes, bubble_size, opacity, up_donw_info)
            st.write(pd.read_csv(output_deseq2_result_path, sep='\t'))
            st.image(output_image_path)
            st.markdown(get_binary_file_downloader_html(output_image_path, 'Download Image'), unsafe_allow_html=True)
            st.markdown(get_binary_file_downloader_html(output_html_path, 'Download HTML'), unsafe_allow_html=True)
            st.markdown(get_binary_file_downloader_html(output_up_genelist_path, 'Download Up Genelist'), unsafe_allow_html=True)
            st.markdown(get_binary_file_downloader_html(output_down_genelist_path, 'Download Down Genelist'), unsafe_allow_html=True)
            os.remove(tmp_file_path)
@st.cache_resource
def get_enrich_data_sample_genelist_csv():
    enrich_data_path = os.path.join(workdir, 'input_file', 'gene_list_nc.txt')
    with open(enrich_data_path, "r", encoding="utf-8") as f:
        data = f.read()
    return data
@st.cache_resource
def get_enrich_data_sample_go_csv():
    enrich_data_path = os.path.join(workdir, 'input_file', 'go_gene_nc.txt')
    with open(enrich_data_path, "r", encoding="utf-8") as f:
        data = f.read()
    return data
@st.cache_resource
def get_enrich_data_sample_kegg1_csv():
    enrich_data_path = os.path.join(workdir, 'input_file', 'kegg_pathway_nc.txt')
    with open(enrich_data_path, "r", encoding="utf-8") as f:
        data = f.read()
    return data
@st.cache_resource
def get_enrich_data_sample_kegg2_csv():
    enrich_data_path = os.path.join(workdir, 'input_file', 'kegg_gene_nc.txt')
    with open(enrich_data_path, "r", encoding="utf-8") as f:
        data = f.read()
    return data
def enrich_page():
    # 下载示例CSV文件
    enrich_file1 = get_enrich_data_sample_genelist_csv()
    enrich_file2 = get_enrich_data_sample_go_csv()
    enrich_file3 = get_enrich_data_sample_kegg1_csv()
    enrich_file4 = get_enrich_data_sample_kegg2_csv()
    col1, col2, col3, col4= st.columns(4)
    with col1:
        st.download_button(
            label="下载基因列表文件",
            data=enrich_file1,
            file_name='gene_list_nc.txt',
            mime="text/txt;charset=utf-8",
        )
    with col2:
        st.download_button(
            label="下载GO基因注释",
            data=enrich_file2,
            file_name='go_gene_nc.txt',
            mime="text/txt;charset=utf-8",
        )
    with col3:
        st.download_button(
            label="下载KEGG通路注释",
            data=enrich_file3,
            file_name='kegg_pathway_nc.txt',
            mime="text/txt;charset=utf-8",
        )
    with col4:
        st.download_button(
            label="下载KEGG基因注释",
            data=enrich_file4,
            file_name='kegg_gene_nc.txt',
            mime="text/txt;charset=utf-8",
        )
    # 文件上传
    uploaded_file1 = st.file_uploader("上传基因列表文件", type=["txt"])
    uploaded_file2 = st.file_uploader("上传GO注释文件", type=["txt"])
    uploaded_file3 = st.file_uploader("上传KEGG通路注释文件", type=["txt"])
    uploaded_file4 = st.file_uploader("上传KEGG通路基因注释文件", type=["txt"])
    if uploaded_file1 and uploaded_file2 and uploaded_file3:
        data1 = pd.read_csv(uploaded_file1,sep='\t')
        data2 = pd.read_csv(uploaded_file2,sep='\t')
        data3 = pd.read_csv(uploaded_file3,sep='\t')
        data4 = pd.read_csv(uploaded_file4,sep='\t')
        st.write(data1)
        st.write(data2)
        st.write(data3)
        st.write(data4)
        # 用户输入区域
        st.sidebar.markdown("### 选择你的参数")
        width = int(st.sidebar.number_input("设置图片宽度", 900, 2000, 1200))
        height = st.sidebar.number_input("设置图片高度", 600, 2000, 1000)
        enrich_type = st.sidebar.selectbox("选择富集分析类型", ['GO', 'KEGG'])
        p_adjust = st.sidebar.number_input("设置p值阈值", 0.01, 1.0, 0.05)
        font_size = st.sidebar.number_input("设置字体大小", 15)
        chart_num = st.sidebar.number_input("设置最大显示数量", 30)
        chart_size = st.sidebar.number_input("设置图表显示大小", 30)
        pic_type = st.sidebar.selectbox("选择图表类型", ['bubble', 'bar'])
        color = st.sidebar.selectbox("选择颜色方案", ['gnbu', 'geyser', 'portland', 'Temps'])
        function_type = st.sidebar.selectbox("选择功能类型", ['All', 'CC', 'MF', 'BP'])
        if st.button("进行基因富集分析"):
            if enrich_type == 'GO':
                with tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as tmp_file1, 
                    tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as 
                    tmp_file2, tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as tmp_file3:
                    tmp_file1.write(uploaded_file1.getvalue())
                    tmp_file2.write(uploaded_file2.getvalue())
                    tmp_file3.write(uploaded_file3.getvalue())
                    tmp_file_path1 = tmp_file1.name
                    tmp_file_path2 = tmp_file2.name
                    tmp_file_path3 = tmp_file3.name
                output_go_enrich_result_path = os.path.join(workdir, 
                                                            'output_file', 
                                                            'go.tsv')
                output_go_image_path = os.path.join(workdir, 
                                                    'output_file', 
                                                    'go.png')
                output_go_html_path = os.path.join(workdir, 
                                                   'output_file', 
                                                   'go.html')
                run_enrich(workdir, 
                           tmp_file_path1, 
                           output_go_enrich_result_path, 
                           p_adjust, 
                           enrich_type, 
                           tmp_file_path2, 
                           tmp_file_path3)
                plot_go_chart(workdir, output_go_enrich_result_path, width, height, p_adjust,font_size, chart_num, chart_size, pic_type, color, function_type)
                st.write(pd.read_csv(output_go_enrich_result_path, sep='\t'))
                st.image(output_go_image_path)
                st.markdown(get_binary_file_downloader_html(output_go_image_path, 'Download Image'), unsafe_allow_html=True)
                st.markdown(get_binary_file_downloader_html(output_go_html_path, 'Download HTML'), unsafe_allow_html=True)
                os.remove(tmp_file_path1)
                os.remove(tmp_file_path2)
                os.remove(tmp_file_path3)
            if enrich_type == 'KEGG':
                with tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as tmp_file1, tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as tmp_file2, tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as tmp_file3, tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as tmp_file4:
                    tmp_file1.write(uploaded_file1.getvalue())
                    tmp_file2.write(uploaded_file2.getvalue())
                    tmp_file3.write(uploaded_file3.getvalue())
                    tmp_file4.write(uploaded_file4.getvalue())
                    tmp_file_path1 = tmp_file1.name
                    tmp_file_path2 = tmp_file2.name
                    tmp_file_path3 = tmp_file3.name
                    tmp_file_path4 = tmp_file4.name
                output_kegg_enrich_result_path = os.path.join(workdir, 'output_file', 'kegg.tsv')
                output_kegg_image_path = os.path.join(workdir, 'output_file', 'kegg.png')
                output_kegg_html_path = os.path.join(workdir, 'output_file', 'kegg.html')
                run_enrich(workdir, tmp_file_path1, output_kegg_enrich_result_path, p_adjust, enrich_type, tmp_file_path3, tmp_file_path4)
                plot_kegg_chart(workdir, output_kegg_enrich_result_path, width, height, p_adjust, font_size, chart_num, chart_size, color, pic_type, function_type)
                st.write(pd.read_csv(output_kegg_enrich_result_path, sep='\t'))
                st.image(output_kegg_image_path)
                st.markdown(get_binary_file_downloader_html(output_kegg_image_path, 'Download Image'), unsafe_allow_html=True)
                st.markdown(get_binary_file_downloader_html(output_kegg_html_path, 'Download HTML'), unsafe_allow_html=True)
                os.remove(tmp_file_path1)
                os.remove(tmp_file_path2)
                os.remove(tmp_file_path3)
                os.remove(tmp_file_path4)
@st.cache_resource
def heatmap_data_sample_csv():
    heatmap_data_path = os.path.join(workdir, 'input_file', 'expression_matrix_heatmap.csv')
    with open(heatmap_data_path, "r", encoding="utf-8") as f:
        data = f.read()
    return data
def heatmap_page():
    # 下载示例CSV文件
    heatmap_file = heatmap_data_sample_csv()
    st.download_button(
        label="下载示例文件",
        data=heatmap_file,
        file_name='expression_matrix_heatmap.csv',
        mime="text/csv;charset=utf-8",
    )
    uploaded_file = st.file_uploader("上传标准化后的表达转录水平数据", type=["csv"])
    if uploaded_file is not None:
        data = pd.read_csv(uploaded_file)
        st.write(data)
        # 用户输入区域
        st.sidebar.markdown("### 选择你的参数")
        color_down = st.sidebar.text_input("设置下调颜色", value='default')
        color_up = st.sidebar.text_input("设置上调颜色", value='default')
        color_mid = st.sidebar.text_input("设置中性颜色", value='default')
        show_border = st.sidebar.checkbox("是否显示边框", value=True)
        scale = st.sidebar.selectbox("选择标准化方式", ['log2', 'z-score'])
        cluster_row = st.sidebar.checkbox("是否行聚类分析", value=True)
        cluster_cols = st.sidebar.checkbox("是否列聚类分析", value=False)
        cellwidth = st.sidebar.number_input("设置单元格宽度", 20)
        cellheight = st.sidebar.number_input("设置单元格高度", 20)
        fontsize = st.sidebar.number_input("设置字体大小", 10)
        if st.button("生成热图"):
            with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name
            output_image_path = os.path.join(workdir, 'output_file', 'heatmap.png')
            run_heatmap(workdir, tmp_file_path, output_image_path, color_up, color_down, color_mid, show_border, scale, cluster_row, cluster_cols, cellwidth, cellheight, fontsize)
            st.image(output_image_path)
            st.markdown(get_binary_file_downloader_html(output_image_path, 'Download Image'), unsafe_allow_html=True)
            os.remove(tmp_file_path)
if page == "相关性分析":
    corr_page()
elif page == "主成分分析":
    pca_page() 
elif page == "差异分析":
    diff_page()
elif page == "富集分析":
    enrich_page()
elif page == "热图绘制":
    heatmap_page()
else:
    corr_page()  # 默认加载相关性分析界面
import plotly.express as px
import plotly.graph_objects as corr_network
def plot_corr_sample(workdir, gene_exp_input_path, width=900, height=600, color='geyser', method='pearson'):
    # 读取数据
    df_corr = pd.read_csv(gene_exp_input_path)
    df_corr = df_corr.iloc[:, 1:]
    df_corr = df_corr.round(2)
    # 计算df_corr的样本数量
    sample_num = df_corr.shape[1]
    # 选择计算相关性的方法
    if method == 'pearson':
        # 计算样本之间的相关性（使用皮尔森系数）
        correlation_matrix = df_corr.corr(method='pearson')
    else:
        correlation_matrix = df_corr.corr(method='spearman')
    # 创建注释文本，用于标记相关性值
    annotations = []
    font_size_control = int(10 - sample_num/10)
    for i, row in enumerate(correlation_matrix.values):
        for j, value in enumerate(row):
            if abs(value) > 0.95:  # 用星星标记大于0.8的相关性值
                text = "★★"
                font_size = font_size_control
            elif abs(value) > 0.8:  # 显示大于0.6的相关性值
                text = "★"
                font_size = font_size_control
            else:
                continue  # 不显示小于或等于0.7的值
            annotations.append(dict(
                x=j, y=i, text=text, xref='x1', yref='y1',
                font=dict(color='white' if abs(value) > 0.5 else 'black', size=font_size),
                showarrow=False))
    # 绘制热图
    fig = px.imshow(correlation_matrix,
                    color_continuous_scale=color,
                    zmin=-1, zmax=1,
                    labels=dict(color='Correlation'))
    # 更新布局以将图例放置在图的右侧
    fig.update_layout(
        width=width,
        height=height,
        annotations=annotations,
    )
    # 添加解释性标注到图外的空白处
    fig.add_annotation(
        xref="paper", yref="paper",
        x=1.05, y=1.07,  # 在图外的空白处定位
        text="★★:Correlation > 0.95<br>★: Correlation > 0.8",
        showarrow=False,
        align="left"
    )
    # 保存为高清图片
    output_image_path = os.path.join(workdir, "output_file", "corr_sample.png")
    fig.write_image(output_image_path, scale=4)
    # 保存为html文件
    output_html_path = os.path.join(workdir, "output_file", "corr_sample.html")
    fig.write_html(output_html_path)
    # 返回路径
    return output_image_path, output_html_path
def plot_corr_gene(workdir, gene_exp_input_path, width=900, height=600, color='geyser', method='pearson'):
    # 读取数据
    df_corr = pd.read_csv(gene_exp_input_path)
    # 预处理,只保留前30个基因
    gene_ids = df_corr.iloc[:30, 0]
    df_corr = df_corr.iloc[:30, 1:]
    df_corr = df_corr.round(2)
    # 计算df_corr有多少行
    sample_num = df_corr.shape[0]
    # 选择计算相关性的方法
    if method == 'pearson':
        # 计算样本之间的相关性（使用皮尔森系数）
        correlation_matrix = df_corr.T.corr(method='pearson')
    else:
        correlation_matrix = df_corr.T.corr(method='spearman')
    # 创建注释文本，用于标记相关性值
    annotations = []
    font_size_control = 10 - abs(int(sample_num/5))
    for i, row in enumerate(correlation_matrix.values):
        for j, value in enumerate(row):
            if abs(value) > 0.9:  # 用星星标记大于0.8的相关性值
                text = "★★"
                font_size = font_size_control
            elif abs(value) > 0.7:  # 显示大于0.7的相关性值
                text = "★"
                font_size = font_size_control
            else:
                continue  # 不显示小于或等于0.7的值
            annotations.append(dict(
                x=j, y=i, text=text, xref='x1', yref='y1',
                font=dict(color='white' if abs(value) > 0.5 else 'black', size=font_size),
                showarrow=False))    
    # 绘制相关性矩阵的热图
    fig = px.imshow(correlation_matrix,
                    color_continuous_scale=color,
                    zmin=-1,  # 设置颜色比例尺的最小值
                    zmax=1,  # 设置颜色比例尺的最大值
                    x=gene_ids, # 设置x轴为基因id
                    y=gene_ids) # 设置y轴为基因id
    # 更新布局以将图例放置在图的右侧
    fig.update_layout(
        width=width,
        height=height,
        annotations=annotations,
    )
    # 添加解释性标注到图外的空白处
    fig.add_annotation(
        xref="paper", yref="paper",
        x=1.05, y=1.07,  # 在图外的空白处定位
        text="★★:Correlation > 0.8<br>★: Correlation > 0.6",
        showarrow=False,
        align="left"
    )
    # 保存为高清图片
    output_image_path = os.path.join(workdir, "output_file", "corr_gene.png")
    fig.write_image(output_image_path, scale=4)
    # 保存为html文件
    output_html_path = os.path.join(workdir, "output_file", "corr_gene.html")
    fig.write_html(output_html_path)
    # 返回路径
    return output_image_path, output_html_path
def create_gene_network(workdir, gene_exp_input_path, width=1200, height=900, bubble_size=4, threshold=0.6, k_value=0.5, iterations=10, color='gnbu', method='pearson'):
    """
    根据输入文件路径中的标准化基因表达水平矩阵，绘制基因相关性网络图
    Args:
        gene_exp_input_path (str): 标准化基因表达水平矩阵的文件路径
        workdir (str): 输出文件的工作目录
        ...其他参数...
    """
    # 读取数据
    df_corr = pd.read_csv(gene_exp_input_path)
    # 预处理
    df_corr = df_corr.rename(columns={df_corr.columns[0]: 'gene_id'})
    df_corr = df_corr.set_index('gene_id')
    df_corr = df_corr.round(2)
    # df_corr 只保留前100个基因
    df_corr = df_corr.iloc[:100, :]
    # 选择计算相关性的方法
    if method == 'pearson':
        # 计算样本之间的相关性（使用皮尔森系数）
        correlation_matrix = df_corr.transpose().corr(method='pearson')
    else:
        correlation_matrix = df_corr.transpose().corr(method='spearman')
    # 使用networkx创建一个网络图
    G = nx.Graph()
    for gene1 in df_corr.index:
        for gene2 in df_corr.index:
            if gene1 != gene2:
                G.add_edge(gene1, gene2, weight=correlation_matrix.loc[gene1, gene2])
    threshold = threshold # 相关性系数
    edges = [(u, v) for (u, v, d) in G.edges(data=True) if abs(d['weight']) > threshold]
    G = G.edge_subgraph(edges).copy()  # 使用edges创建一个新的图，并使用copy()避免状态问题
    # pos设置,k越小则点越紧,iterations越大则点越稳定
    pos = nx.spring_layout(G, k=k_value, iterations=iterations)
    # 将位置作为节点属性添加到G中
    for node in G.nodes():
        G.nodes[node]['pos'] = pos[node]
    # 使用plotly创建网络图
    edge_x = []
    edge_y = []
    for edge in G.edges():
        x0, y0 = G.nodes[edge[0]]['pos']
        x1, y1 = G.nodes[edge[1]]['pos']
        edge_x.append(x0)
        edge_x.append(x1)
        edge_x.append(None)
        edge_y.append(y0)
        edge_y.append(y1)
        edge_y.append(None)
    # 分别创建正相关和负相关的边
    edge_x_pos, edge_y_pos = [], []
    edge_x_neg, edge_y_neg = [], []
    # 根据权重将边分为正负两组
    for edge in G.edges(data=True):
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        if edge[2]['weight'] > 0:
            edge_x_pos.extend([x0, x1, None])
            edge_y_pos.extend([y0, y1, None])
        else:
            edge_x_neg.extend([x0, x1, None])
            edge_y_neg.extend([y0, y1, None])
    edge_trace_pos = corr_network.Scatter(
        x=edge_x_pos, y=edge_y_pos,
        line=dict(width=0.3, color='red'),
        hoverinfo='none',
        mode='lines'
    )
    edge_trace_neg = corr_network.Scatter(
        x=edge_x_neg, y=edge_y_neg,
        line=dict(width=0.3, color='blue'),
        hoverinfo='none',
        mode='lines'
    )
    node_x = []
    node_y = []
    for node in G.nodes():
        x, y = G.nodes[node]['pos']
        node_x.append(x)
        node_y.append(y)
    node_trace = corr_network.Scatter(
        x=node_x, y=node_y,
        mode='markers',
        hoverinfo='text',
        marker=dict(
            showscale=True,
            colorscale=color,
            colorbar=dict(
                thickness=15,
                title='Number of Significantly Correlated Nodes',
                xanchor='left',
                titleside='right'
            ),
            line_width=2))
    node_adjacencies = []
    node_text = []
    node_sizes = []  # 添加一个列表来存储基于节点连接数的大小
    # 根据基因数量设置节点大小
    if df_corr.shape[0] > 50:
        # 计算每个节点的连接数并设置节点大小
        for node, adjacencies in enumerate(G.adjacency()):
            node_adjacencies.append(len(adjacencies[1]))
            node_text.append(adjacencies[0])
            node_degree = len(adjacencies[1])
            scaled_size = 15 + (node_degree * bubble_size/10)
            node_sizes.append(scaled_size)
    else:
        # 计算每个节点的连接数并设置节点大小
        for node, adjacencies in enumerate(G.adjacency()):
            node_adjacencies.append(len(adjacencies[1]))
            node_text.append(adjacencies[0])
            node_degree = len(adjacencies[1])
            scaled_size = 15 + (node_degree * bubble_size/2)
            node_sizes.append(scaled_size)
    # norm = plt.Normalize(vmin=min(node_adjacencies), vmax=max(node_adjacencies)) # 将连接数映射到0-1范围  
    node_trace.marker.color = node_adjacencies
    node_trace.text = node_text
    node_trace.marker.size = node_sizes  # 更新marker的大小
    # 创建图形
    fig = corr_network.Figure(data = [edge_trace_pos, edge_trace_neg, node_trace],
                    layout=corr_network.Layout(
                        title='',
                        showlegend=False,
                        hovermode='closest',
                        margin=dict(b=0, l=0, r=0, t=40),
                        annotations=[
                            dict(
                                text="",
                                showarrow=False,
                                xref="paper", yref="paper",
                                x=0.005, y=-0.002)
                        ],
                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))
                    )
    # 定义fig的布局，设置宽度和高度
    fig.update_layout(
        autosize=False,
        width=width, 
        height=height,
        template="plotly_white"
    )
    # 保存为高清图片
    output_image_path = os.path.join(workdir, "output_file", "corr_gene_network.png")
    fig.write_image(output_image_path, scale=4)
    # 保存为html文件
    output_html_path = os.path.join(workdir, "output_file", "corr_gene_network.html")
    fig.write_html(output_html_path)
    # 返回路径
    return output_image_path, output_html_path
def run_deseq(workdir, input_path, output_path, repetition):
    """
    运行R脚本，将输入文件进行差异分析，生成差异分析结果表格
    Args:
        workdir (str): 工作目录
        input_path (str): 输入文件路径
        output_path (str): 输出文件路径
        repetition (int): 重复次数
    """
    # R脚本的路径，需要师哥你改路径
    script_path = os.path.join(workdir,'Deseq2.R')
    # Rscript Deseq2.R -i 'input_file/expression_matrix_deseq2.csv' -o 'output_file/deseq2.tsv' -n 3
    cmd = [
        'Rscript', script_path,
        '--input', input_path,
        '--output', output_path,
        '--repetition', str(repetition),
    ]
    # 执行R脚本并捕获输出
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        return e.stderrs
def plot_volcano(workdir, deseq2_relust, genelist=None, width=1200, height=900, p_threshold=0.05, logFC_threshold=1, color_schemes=1, bubble_size=5, opacity=0.8, up_donw_info=True, x_fix=False, y_fix=False):
    """根据差异分析结果绘制火山图
    Args:
        workdir (str): 工作目录
        deseq2_relust (pd.DataFrame): 差异分析结果表格
        genelist (list): 需要标记的基因列表. Defaults to None.
        width (int): 图像宽度. Defaults to 1200.
        height (int): 图像高度. Defaults to 900.
        p_threshold (float): 显著性阈值. Defaults to 0.05.
        logFC_threshold (int): logFC阈值. Defaults to 1.
        color_schemes (int): 颜色方案. Defaults to 1.
        bubble_size (int): 气泡大小. Defaults to 5.
        opacity (float): 气泡透明度. Defaults to 0.8.
        up_donw_info (bool): 是否显示上调和下调基因的数量. Defaults to True.
        x_fix (bool): 是否固定x轴范围. Defaults to False.
        y_fix (bool): 是否固定y轴范围. Defaults to False.
    """
    # 读取数据
    deseq2_relust = pd.read_csv(deseq2_relust, sep='\t')
    # 预处理
    deseq2_relust = deseq2_relust[['Gene', 'log2FoldChange', 'padj']].copy()
    deseq2_relust.columns = ['Symbol', 'logFC', 'P.adjust']
    deseq2_relust.dropna(inplace=True)
    # 对logFC范围进行修正
    if x_fix == True: 
        deseq2_relust['logFC'] = np.clip(deseq2_relust['logFC'], -6, 6)
    deseq2_relust['P.adjust'] = deseq2_relust['P.adjust'].replace(0, 1e-300)
    pvalue = -np.log10(deseq2_relust['P.adjust'])
    # 对y轴范围进行修正
    if y_fix == True:
        pvalue= np.clip(pvalue, 0, 100)
    # 提取数据
    gene_names = deseq2_relust['Symbol'].values
    logFC = deseq2_relust['logFC'].values
    # 根据阈值筛选差异显著的基因，给差异最显著的基因添加标签
    significant = (np.abs(logFC) > logFC_threshold) & (deseq2_relust['P.adjust'] < p_threshold)
    upregulated = significant & (logFC > 0)
    downregulated = significant & (logFC < 0)
    nonsignificant = ~significant
    # 颜色设置
    colors = {
        1: ('#f08d1a', '#7fa4ca'),
        2: ('#f26c6a', '#54a857'),
        3: ('#c42121', '#15609b'),
        4: ('#df7415', '#3d9241')
    }
    up_color, down_color = colors.get(color_schemes, ('#f08d1a', '#7fa4ca'))
    # 绘图
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=logFC[upregulated], y=pvalue[upregulated], mode='markers',
                            marker=dict(color=up_color, size=bubble_size, sizemode='area',symbol='circle',opacity=opacity,
                                        # line_dict=1
                                        line=dict(color='black',width=0.4)), name='Up',
                            text=gene_names[upregulated]))
    fig.add_trace(go.Scatter(x=logFC[downregulated], y=pvalue[downregulated], mode='markers',
                            marker=dict(color=down_color, size=bubble_size, sizemode='area',symbol='circle',opacity=opacity,
                                        line=dict(color='black',width=0.4)), name='Down',
                            text=gene_names[downregulated]))
    fig.add_trace(go.Scatter(x=logFC[nonsignificant], y=pvalue[nonsignificant], mode='markers',
                            marker=dict(color='#A9A9A9', size=bubble_size, sizemode='area',symbol='circle',opacity=opacity,
                                        line=dict(color='black',width=0.4)), name='Nonsignificant',
                            text=gene_names[nonsignificant]))
    if up_donw_info == True:
    # 计算上调和下调基因的数量
        upregulated_num = np.sum(upregulated)
        downregulated_num = np.sum(downregulated)
        # 在图像中右上角添加上调基因的数量注释
        fig.add_annotation(xref="paper", yref="paper",
                        x=1, y=1, showarrow=False,
                        xanchor='right', yanchor='top',
                        text="Up: {} genes".format(upregulated_num),
                        font=dict(size=16),
                        align="right",
                        bgcolor="white",
                        borderpad=4)
        # 在图像中左上角添加下调基因的数量注释
        fig.add_annotation(xref="paper", yref="paper",
                        x=0, y=1, showarrow=False,
                        xanchor='left', yanchor='top',
                        text="Down: {} genes".format(downregulated_num),
                        font=dict(size=16),
                        align="left",
                        bgcolor="white",
                        borderpad=4)
    # 添加火山图的阈值线
    x_min = np.min(logFC)
    x_max = np.max(logFC)
    fig.update_layout(shapes=[
        dict(type="line", x0=x_min, x1=x_max, y0=-np.log10(p_threshold), y1=-np.log10(p_threshold), line=dict(color="Black", width=1, dash="dash")),
        dict(type="line", x0=logFC_threshold, x1=logFC_threshold, y0=0, y1=max(pvalue)+10, line=dict(color="Black", width=1, dash="dash")),
        dict(type="line", x0=-logFC_threshold, x1=-logFC_threshold, y0=0, y1=max(pvalue)+10, line=dict(color="Black", width=1, dash="dash"))
    ])
    # 添加基因名的注释
    if genelist is not None:
        highlighted_genes = np.isin(gene_names, genelist)
        fig.add_trace(go.Scatter(
            x=logFC[highlighted_genes], 
            y=pvalue[highlighted_genes], 
            mode='markers+text',
            marker=dict(color='red', size=bubble_size*1.2, symbol='circle', opacity=1),
            # 字体大小
            textfont=dict(size=12),
            name='Marker Genes',
            text=gene_names[highlighted_genes],
            textposition="top center"
        ))
    # 设置图像布局，并限制 y 轴的范围
    fig.update_layout(
        xaxis_title='log2 Fold Change',
        yaxis_title='-log10(p-value)',
        title='DE Analysis Volcano Plot',
        template="plotly_white",
        height=height,
        width=width,
    )
    if y_fix == True:
        fig.update_layout(yaxis_range=[0, 110])
    # 保存上调和下调基因列表
    upregulated_genes_path = os.path.join(workdir, 'output_file', 'upregulated_genes.txt')
    downregulated_genes_path = os.path.join(workdir, 'output_file', 'downregulated_genes.txt')
    deseq2_relust[upregulated]['Symbol'].to_csv(upregulated_genes_path, index=False, header=False)
    deseq2_relust[downregulated]['Symbol'].to_csv(downregulated_genes_path, index=False, header=False)
    # 保存为png，scale设置为4
    output_image_path = os.path.join(workdir, "output_file", "volcano.png")
    fig.write_image(output_image_path, scale=4)
    # 保存为html
    output_html_path = os.path.join(workdir, "output_file", "volcano.html")
    fig.write_html(output_html_path)
    # 返回路径
    return upregulated_genes_path, downregulated_genes_path, output_image_path, output_html_path
"""
Different Expression analysis in Python
"""
import numpy as np
import pandas as pd
import scanpy as sc
import statsmodels.api as sm
import matplotlib.patches as mpatches
import matplotlib.pyplot as plt
import matplotlib
from typing import Union,Tuple
def Matrix_ID_mapping(data:pd.DataFrame,gene_ref_path:str)->pd.DataFrame:
    pair=pd.read_csv(gene_ref_path,sep='\t',index_col=0)
    ret_gene=list(set(data.index.tolist()) & set(pair.index.tolist()))
    data=data.loc[ret_gene]
    data=data_drop_duplicates_index(data)
    new_index=[]
    for i in ret_gene:
        a=pair.loc[i,'symbol']
        if str(a)=='nan':
            new_index.append(i)
        else:
            new_index.append(a)
    data.index=new_index
    return data
def deseq2_normalize(data:pd.DataFrame)->pd.DataFrame:
    avg1=data.apply(np.log,axis=1).mean(axis=1).replace([np.inf,-np.inf],np.nan).dropna()
    data1=data.loc[avg1.index]
    data_log=data1.apply(np.log,axis=1)
    scale=data_log.sub(avg1.values,axis=0).median(axis=0).apply(np.exp)
    return data/scale
def estimateSizeFactors(data:pd.DataFrame)->pd.Series:
    avg1=data.apply(np.log,axis=1).mean(axis=1).replace([np.inf,-np.inf],np.nan).dropna()
    data1=data.loc[avg1.index]
    data_log=data1.apply(np.log,axis=1)
    scale=data_log.sub(avg1.values,axis=0).median(axis=0).apply(np.exp)
    return scale
def estimateDispersions(counts:pd.DataFrame)->pd.Series:
    # Step 1: Calculate mean and variance of counts for each gene
    mean_counts = np.mean(counts, axis=1)
    var_counts = np.var(counts, axis=1)
    # Step 2: Fit trend line to variance-mean relationship using GLM
    mean_expr = sm.add_constant(np.log(mean_counts))
    mod = sm.GLM(np.log(var_counts), mean_expr, family=sm.families.Gamma())
    res = mod.fit()
    fitted_var = np.exp(res.fittedvalues)
    # Step 3: Calculate residual variance for each gene
    disp = fitted_var / var_counts
    return disp
def data_drop_duplicates_index(data:pd.DataFrame)->pd.DataFrame:
    index=data.index
    data=data.loc[~index.duplicated(keep='first')]
    return data
class pyDEG(object):
    def __init__(self,raw_data:pd.DataFrame) -> None:
        self.raw_data=raw_data
        self.data=raw_data.copy()
    def drop_duplicates_index(self)->pd.DataFrame:
        self.data=data_drop_duplicates_index(self.data)
        return self.data
    def normalize(self)->pd.DataFrame:
        self.size_factors=estimateSizeFactors(self.data)
        self.data=deseq2_normalize(self.data)
        return self.data
    def foldchange_set(self,fc_threshold:int=-1,pval_threshold:float=0.05,logp_max:int=6,fold_threshold:int=0):
        if fc_threshold==-1:
            foldp=np.histogram(self.result['log2FC'].dropna())
            foldchange=(foldp[1][np.where(foldp[1]>0)[0][fold_threshold]]+foldp[1][np.where(foldp[1]>0)[0][fold_threshold+1]])/2
        else:
            foldchange=fc_threshold
        print('... Fold change threshold: %s'%foldchange)
        fc_max,fc_min=foldchange,0-foldchange
        self.fc_max,self.fc_min=fc_max,fc_min
        self.pval_threshold=pval_threshold
        self.result['sig']='normal'
        self.result.loc[((self.result['log2FC']>fc_max)&(self.result['qvalue']<pval_threshold)),'sig']='up'
        self.result.loc[((self.result['log2FC']<fc_min)&(self.result['qvalue']<pval_threshold)),'sig']='down'
        self.result.loc[self.result['-log(qvalue)']>logp_max,'-log(qvalue)']=logp_max
    def plot_volcano(self,figsize:tuple=(4,4),title:str='',titlefont:dict={'weight':'normal','size':14,},
                     up_color:str='#e25d5d',down_color:str='#7388c1',normal_color:str='#d7d7d7',
                     legend_bbox:tuple=(0.8, -0.2),legend_ncol:int=2,legend_fontsize:int=12,
                     plot_genes:list=None,plot_genes_num:int=10,plot_genes_fontsize:int=10,
                     ticks_fontsize:int=12)->matplotlib.axes._axes.Axes:
        fig, ax = plt.subplots(figsize=figsize)
        result=self.result.copy()
        #首先绘制正常基因
        ax.scatter(x=result[result['sig']=='normal']['log2FC'],
                y=result[result['sig']=='normal']['-log(qvalue)'],
                color=normal_color,#颜色
                alpha=.5,#透明度
                )
        #接着绘制上调基因
        ax.scatter(x=result[result['sig']=='up']['log2FC'],
                y=result[result['sig']=='up']['-log(qvalue)'],
                color=up_color,#选择色卡第15个颜色
                alpha=.5,#透明度
                )
        #绘制下调基因
        ax.scatter(x=result[result['sig']=='down']['log2FC'],
                y=result[result['sig']=='down']['-log(qvalue)'],
                color=down_color,#颜色
                alpha=.5,#透明度
                )
        ax.plot([result['log2FC'].min(),result['log2FC'].max()],#辅助线的x值起点与终点
        [-np.log10(self.pval_threshold),-np.log10(self.pval_threshold)],#辅助线的y值起点与终点
        linewidth=2,#辅助线的宽度
        linestyle="--",#辅助线类型：虚线
        color='black'#辅助线的颜色
        )
        ax.plot([self.fc_max,self.fc_max],
                [result['-log(qvalue)'].min(),result['-log(qvalue)'].max()],
                linewidth=2, 
                linestyle="--",
                color='black')
        ax.plot([self.fc_min,self.fc_min],
                [result['-log(qvalue)'].min(),result['-log(qvalue)'].max()],
                linewidth=2, 
                linestyle="--",
                color='black')
        #设置横标签与纵标签
        ax.set_ylabel(r'$-log_{10}(qvalue)$',titlefont)                                    
        ax.set_xlabel(r'$log_{2}FC$',titlefont)
        #设置标题
        ax.set_title(title,titlefont)
        #绘制图注
        #legend标签列表，上面的color即是颜色列表
        labels = ['up:{0}'.format(len(result[result['sig']=='up'])),
                'down:{0}'.format(len(result[result['sig']=='down']))]  
        #用label和color列表生成mpatches.Patch对象，它将作为句柄来生成legend
        color = ['#e25d5d','#174785']
        patches = [mpatches.Patch(color=color[i], label="{:s}".format(labels[i]) ) for i in range(len(color))] 
        ax.legend(handles=patches,
            bbox_to_anchor=legend_bbox, 
            ncol=legend_ncol,
            fontsize=legend_fontsize)
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['bottom'].set_visible(True)
        ax.spines['left'].set_visible(True)
        from adjustText import adjust_text
        if plot_genes is not None:
            hub_gene=plot_genes
        else:
            up_result=result.loc[result['sig']=='up']
            down_result=result.loc[result['sig']=='down']
            hub_gene=up_result.sort_values('qvalue').index[:plot_genes_num//2].tolist()+down_result.sort_values('qvalue').index[:plot_genes_num//2].tolist()
        color_dict={
        'up':'#a51616',
            'down':'#174785',
            'normal':'grey'
        }
        texts=[ax.text(result.loc[i,'log2FC'], 
               result.loc[i,'-log(qvalue)'],
               i,
               fontdict={'size':plot_genes_fontsize,'weight':'bold','color':color_dict[result.loc[i,'sig']]}
               ) for i in hub_gene if 'ENSG' not in i]
        adjust_text(texts,only_move={'text': 'xy'},arrowprops=dict(arrowstyle='->', color='red'),)
        ax.set_xticks([round(i,2) for i in ax.get_xticks()[1:-1]],#获取x坐标轴内容
              [round(i,2) for i in ax.get_xticks()[1:-1]],#更新x坐标轴内容
              fontsize=ticks_fontsize,
              fontweight='normal'
              )
        return ax
    def plot_boxplot(self,genes:list,treatment_groups:list,control_groups:list,
                     treatment_name:str='Treatment',control_name:str='Control',
                     figsize:tuple=(4,3),palette:list=["#a64d79","#674ea7"],
                     title:str='Gene Expression',fontsize:int=12,legend_bbox:tuple=(1, 0.55),legend_ncol:int=1,
                     **kwarg)->Tuple[matplotlib.figure.Figure,matplotlib.axes._axes.Axes]:
        p_data=pd.DataFrame(columns=['Value','Gene','Type'])
        for gene in genes:
            plot_data1=pd.DataFrame()
            plot_data1['Value']=self.data[treatment_groups].loc[gene].values
            plot_data1['Gene']=gene
            plot_data1['Type']=treatment_name
            plot_data2=pd.DataFrame()
            plot_data2['Value']=self.data[control_groups].loc[gene].values
            plot_data2['Gene']=gene
            plot_data2['Type']=control_name
            plot_data=pd.concat([plot_data1,plot_data2],axis=0)
            p_data=pd.concat([p_data,plot_data],axis=0)
        fig,ax=plot_boxplot(p_data,hue='Type',x_value='Gene',y_value='Value',palette=palette,
                          figsize=figsize,fontsize=fontsize,title=title,
                          legend_bbox=legend_bbox,legend_ncol=legend_ncol, **kwarg)
        return fig,ax
    def ranking2gsea(self,rank_max:int=200,rank_min:int=274)->pd.DataFrame:
        result=self.result.copy()
        result['fcsign']=np.sign(result['log2FC'])
        result['logp']=-np.log10(result['pvalue'])
        result['metric']=result['logp']/result['fcsign']
        rnk=pd.DataFrame()
        rnk['gene_name']=result.index
        rnk['rnk']=result['metric'].values
        rnk=rnk.sort_values(by=['rnk'],ascending=False)
        k=1
        total=0
        for i in range(len(rnk)):
            if rnk.loc[i,'rnk']==np.inf: 
                total+=1
        #200跟274根据你的数据进行更改，保证inf比你数据最大的大，-inf比数据最小的小就好
        for i in range(len(rnk)):
            if rnk.loc[i,'rnk']==np.inf: 
                rnk.loc[i,'rnk']=rank_max+(total-k)
                k+=1
            elif rnk.loc[i,'rnk']==-np.inf: 
                rnk.loc[i,'rnk']=-(rank_min+k)
                k+=1
        return rnk
    def deg_analysis(self,group1:list,group2:list,
                     method:str='DEseq2',alpha:float=0.05,
                     multipletests_method:str='fdr_bh',n_cpus:int=8,
                     cooks_filter:bool=True, independent_filter:bool=True)->pd.DataFrame:
        from pydeseq2.dds import DeseqDataSet
        from pydeseq2.ds import DeseqStats
        if method=='ttest':
            from scipy.stats import ttest_ind
            from statsmodels.stats.multitest import multipletests
            data=self.data
            g1_mean=data[group1].mean(axis=1)
            g2_mean=data[group2].mean(axis=1)
            g=(g2_mean+g1_mean)/2
            g=g.loc[g>0].min()
            fold=(g1_mean+g)/(g2_mean+g)
            #log2fold=np.log2(fold)
            ttest = ttest_ind(data[group1].T.values, data[group2].T.values)
            pvalue=ttest[1]
            qvalue = multipletests(np.nan_to_num(np.array(pvalue),0), alpha=0.5, 
                               method=multipletests_method, is_sorted=False, returnsorted=False)
            #qvalue=fdrcorrection(np.nan_to_num(np.array(pvalue),0), alpha=0.05, method='indep', is_sorted=False)
            genearray = np.asarray(pvalue)
            result = pd.DataFrame({'pvalue':genearray,'qvalue':qvalue[1],'FoldChange':fold})
            result=result.loc[~result['pvalue'].isnull()]
            result['-log(pvalue)'] = -np.log10(result['pvalue'])
            result['-log(qvalue)'] = -np.log10(result['qvalue'])
            result['BaseMean']=(g1_mean+g2_mean)/2
            result['log2(BaseMean)']=np.log2((g1_mean+g2_mean)/2)
            result['log2FC'] = np.log2(result['FoldChange'])
            result['abs(log2FC)'] = abs(np.log2(result['FoldChange']))
            result['size']  =np.abs(result['FoldChange'])/10
            #result=result[result['padj']<alpha]
            result['sig']='normal'
            result.loc[result['qvalue']<alpha,'sig']='sig'
            self.result=result
            return result
        elif method=='wilcox':
            #raise ValueError('The method is not supported.')
            from scipy.stats import ranksums
            from statsmodels.stats.multitest import multipletests
            data=self.data
            g1_mean=data[group1].mean(axis=1)
            g2_mean=data[group2].mean(axis=1)
            fold=(g1_mean+0.00001)/(g2_mean+0.00001)
            #log2fold=np.log2(fold)
            wilcox = ranksums(data[group1].T.values, data[group2].T.values)
            pvalue=wilcox[1]
            #qvalue=fdrcorrection(np.nan_to_num(np.array(pvalue),0), alpha=0.05, method='indep', is_sorted=False)
            qvalue = multipletests(np.nan_to_num(np.array(pvalue),0), alpha=0.5, 
                               method=multipletests_method, is_sorted=False, returnsorted=False)
            genearray = np.asarray(pvalue)
            result = pd.DataFrame({'pvalue':genearray,'qvalue':qvalue[1],'FoldChange':fold})
            result=result.loc[~result['pvalue'].isnull()]
            result['-log(pvalue)'] = -np.log10(result['pvalue'])
            result['-log(qvalue)'] = -np.log10(result['qvalue'])
            result['BaseMean']=(g1_mean+g2_mean)/2
            result['log2(BaseMean)']=np.log2((g1_mean+g2_mean)/2)
            result['log2FC'] = np.log2(result['FoldChange'])
            result['abs(log2FC)'] = abs(np.log2(result['FoldChange']))
            result['size']  =np.abs(result['FoldChange'])/10
            #result=result[result['padj']<alpha]
            result['sig']='normal'
            result.loc[result['qvalue']<alpha,'sig']='sig'
            self.result=result
            return result
        elif method=='DEseq2':
            import pydeseq2
            counts_df=self.data[group1+group2].T
            clinical_df=pd.DataFrame(index=group1+group2)
            clinical_df['condition']=['Treatment']*len(group1)+['Control']*len(group2)
            #Determining the version of pydeseq2 smaller than 0.4
            if pydeseq2.__version__<='0.3.5':
                dds = DeseqDataSet(
                    counts=counts_df,
                    clinical=clinical_df,
                    design_factors="condition",  # compare samples based on the "condition"
                    ref_level=["condition", "Control"],
                    # column ("B" vs "A")
                    refit_cooks=True,
                    n_cpus=n_cpus,
                )
            else:
                dds = DeseqDataSet(
                    counts=counts_df,
                    metadata=clinical_df,
                    design_factors="condition",  # compare samples based on the "condition"
                    #ref_level=["condition", "Control"],
                    # column ("B" vs "A")
                    refit_cooks=True,
                    n_cpus=n_cpus,
                )
            dds.fit_size_factors()
            dds.fit_genewise_dispersions()
            dds.fit_dispersion_trend()
            dds.fit_dispersion_prior()
            print(
                f"logres_prior={dds.uns['_squared_logres']}, sigma_prior={dds.uns['prior_disp_var']}"
            )
            dds.fit_MAP_dispersions()
            dds.fit_LFC()
            dds.calculate_cooks()
            if dds.refit_cooks:
                # Replace outlier counts
                dds.refit()
            stat_res = DeseqStats(dds, alpha=alpha, cooks_filter=cooks_filter, independent_filter=independent_filter)
            stat_res.run_wald_test()
            if stat_res.cooks_filter:
                stat_res._cooks_filtering()
            if stat_res.independent_filter:
                stat_res._independent_filtering()
            else:
                stat_res._p_value_adjustment()
            self.stat_res=stat_res
            stat_res.summary()
            result=stat_res.results_df
            result['qvalue']=result['padj']
            result['-log(pvalue)'] = -np.log10(result['pvalue'])
            result['-log(qvalue)'] = -np.log10(result['padj'])
            result['BaseMean']=result['baseMean']
            result['log2(BaseMean)']=np.log2(result['baseMean']+1)
            result['log2FC'] = result['log2FoldChange']
            result['abs(log2FC)'] = abs(result['log2FC'])
            #result['size']  =np.abs(result['FoldChange'])/10
            #result=result[result['padj']<alpha]
            result['sig']='normal'
            result.loc[result['qvalue']<alpha,'sig']='sig'
            self.result=result
            return result
        else:
            raise ValueError('The method is not supported.')
def run_enrich(workdir, input_path, output_path, p_adjust, enrich_type, file1, file2):
    # 根据enrich_type确定脚本路径
    if enrich_type == 'GO':
        script_name = 'go_enrich.R'
    elif enrich_type == 'KEGG':
        script_name = 'kegg_enrich.R'
    else:
        raise ValueError("enrich_type must be 'GO' or 'KEGG'")
    script_path = os.path.join(workdir, script_name)
    if enrich_type == 'GO':
        cmd = [
            'Rscript', script_path,
            '--input', input_path,
            '--go_file', file1,
            '--output', output_path,
            '--p_adjust', str(p_adjust),
        ]
    elif enrich_type == 'KEGG':
        cmd = [
            'Rscript', script_path,
            '--input', input_path,
            '--output', output_path,
            '--p_adjust', str(p_adjust),
            '--kegg_file1', file1,
            '--kegg_file2', file2
        ]
    # 执行R脚本并捕获输出
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        return e.stderr
chunkSize = 100
#Function to index flat matrix as squareform matrix
def dist_index(i, j, matrix):
    if i == j:
        return(0.0)
    l = len(matrix)
    n = 0.5*(np.sqrt((8*l)+1)+1)
    index = int(l - binom(n-min(i, j), 2) + (max(i, j) - min(i, j) - 1))
    return(matrix[index])
#Function to index flat matrix as squareform matrix
def dist_multi_index(_array, matrix):
    results = np.zeros((len(_array), len(_array)))
    for i in range(len(_array)):
        for j in range(i, len(_array)):
            score = dist_index(_array[i], _array[j], matrix)
            results[i,j] = score
            results[j,i] = score
    return(results)
#Function to index rows of flat matrix as squareform matrix
def get_rows(_array, matrix):
    l = len(matrix)
    n = int(0.5*(np.sqrt((8*l)+1)+1))
    if _array.dtype != "bool":
        results = np.zeros((len(_array), n))
        for row, i in enumerate(_array):
            for j in range(n):
                index = int(l - binom(n - min(i, j), 2) + (max(i, j) - min(i, j) - 1))
                results[row,j] = matrix[index]
        return(results)
    else:
        results = np.zeros((np.sum(_array), n))
        row = 0
        for i, b in enumerate(_array):
            if b == True:
                for j in range(n):
                    index = int(l - binom(n - min(i, j), 2) + (max(i, j) - min(i, j) - 1))
                    results[row, j] = matrix[index]
                row += 1
        return(results)
#The following are supporting function for GetClusters.
def CoreSize(BranchSize, minClusterSize):
    BaseCoreSize = minClusterSize / 2 + 1
    if BaseCoreSize < BranchSize:
        CoreSize = BaseCoreSize + np.sqrt(BranchSize - BaseCoreSize)
    else:
        CoreSize = BranchSize
    return(int(CoreSize))
# This assumes the diagonal of the distance matrix
# is zero, BranchDist is a square matrix whose dimension is at least 2.
def CoreScatter(BranchDist, minClusterSize):
    nPoints = BranchDist.shape[0]
    PointAverageDistances = np.sum(BranchDist, axis=1) / (nPoints - 1)
    CoreSize = minClusterSize / 2 + 1
    if CoreSize < nPoints:
        EffCoreSize = CoreSize + np.sqrt(nPoints - CoreSize)
        order = np.argsort(PointAverageDistances)
        Core = order[np.arange(EffCoreSize)]
    else:
        Core = np.arange(nPoints)
        EffCoreSize = nPoints
    CoreAverageDistances = np.sum(BranchDist[Core, Core], axis=1) / (EffCoreSize - 1)
    return(np.mean(CoreAverageDistances))
def interpolate(data, index):
    i = np.round(index)
    n = len(data)
    if i < 0: return(data[0])
    if i >= n: return(data[-1])
    r = index - i
    return(data[i-1] * (1 - r) + data[i] * r)
def cutreeHybrid(link, distM,
                 cutHeight = None, minClusterSize = 20, deepSplit = 1,
                 maxCoreScatter = None, minGap = None,
                 maxAbsCoreScatter = None, minAbsGap = None,
                 minSplitHeight = None, minAbsSplitHeight = None,
                 externalBranchSplitFnc = None, minExternalSplit = None,
                 externalSplitOptions = [],
                 externalSplitFncNeedsDistance = None,
                 assumeSimpleExternalSpecification = True,
                 pamStage = True, pamRespectsDendro = True,
                 useMedoids = False,
                 maxPamDist = None,
                 respectSmallClusters = True,
                 verbose = 2, indent = 0):
    dendro_height = get_heights(link)
    dendro_merge = get_merges(link)
    if maxPamDist == None:
        maxPamDist = cutHeight
    nMerge = len(dendro_height)
    refQuantile = 0.05
    refMerge = np.round(nMerge * refQuantile)
    if refMerge < 1: refMerge = 1
    refHeight = dendro_height[int(refMerge) - 1]
    if cutHeight == None:
        cutHeight = 0.99 * (np.max(dendro_height) - refHeight) + refHeight
        print("..cutHeight not given, setting it to", cutHeight, 
              " ===>  99% of the (truncated) height range in dendro.")
    else:
        if cutHeight > np.max(dendro_height): cutHeight = np.max(dendro_height)
    if maxPamDist == None: maxPamDist = cutHeight
    nMergeBelowCut = np.sum(dendro_height <= cutHeight)
    if nMergeBelowCut < minClusterSize:
        print("cutHeight set too low; no merges below the cut.")
        return(np.zeros(nMerge+1))
    # fill in this section once understood better
    if externalBranchSplitFnc != None:
        nExternalSplits = len(externalBranchSplitFnc)
        if len(minExternalSplit) < 1:
            raise AttributeError("minExternalBranchSplit must be given.")
        if assumeSimpleExternalSpecification and nExternalSplits == 1:
            pass
    else:
        nExternalSplits = 0
    MxBranches = nMergeBelowCut
    branch_isBasic = np.repeat(True, MxBranches)
    branch_isTopBasic = np.repeat(True, MxBranches)
    branch_failSize = np.repeat(False, MxBranches)
    branch_rootHeight = np.repeat(np.nan, MxBranches)
    branch_size = np.repeat(2, MxBranches)
    branch_nMerge = np.repeat(1, MxBranches)
    branch_nSingletons = np.repeat(2, MxBranches)
    branch_nBasicClusters = np.repeat(0, MxBranches)
    branch_mergedInto = np.repeat(0, MxBranches)
    branch_attachHeight = np.repeat(np.nan, MxBranches)
    #branch_singletons = np.zeros(MxBranches)
    branch_singletons = [np.nan] * MxBranches
    #branch_basicClusters = pd.Series(np.zeros(MxBranches))
    branch_basicClusters = [np.nan] * MxBranches
    #branch_mergingHeights = pd.Series(np.zeros(MxBranches))
    branch_mergingHeights = [np.nan] * MxBranches
    #branch_singletonHeights = pd.Series(np.zeros(MxBranches))
    branch_singletonHeights = [np.nan] * MxBranches
    nBranches = 0
    spyIndex = None
    if os.path.isfile(".dynamicTreeCutSpyFile"):
        spyIndex = pd.read_csv(".dynamicTreeCutSpyFile")
        print("Found 'spy file' with indices of objects to watch for.")
        spyIndex = spyIndex.iloc[:,1].values
    defMCS = np.array([0.64, 0.73, 0.82, 0.91, 0.95])
    defMG = (1 - defMCS) * 3 / 4.0
    nSplitDefaults = len(defMCS)
    if type(deepSplit) == bool: deepSplit = int(deepSplit) * (nSplitDefaults - 2)
    deepSplit = deepSplit + 1
    if deepSplit < 1 or deepSplit > nSplitDefaults:
        raise IndexError("Parameter deepSplit (value", deepSplit,
                         ") out of range: allowable range is 0 through",
                         nSplitDefaults - 1)
    if maxCoreScatter == None: maxCoreScatter = interpolate(defMCS, deepSplit)
    if minGap == None: minGap = interpolate(defMG, deepSplit)
    if maxAbsCoreScatter == None:
        maxAbsCoreScatter = refHeight + maxCoreScatter * (cutHeight - refHeight)
    if minAbsGap == None:
        minAbsGap = minGap * (cutHeight - refHeight)
    if minSplitHeight == None: minSplitHeight = 0
    if minAbsSplitHeight == None:
        minAbsSplitHeight = refHeight + minSplitHeight * (cutHeight - refHeight)
    nPoints = nMerge + 1
    IndMergeToBranch = np.repeat(0, nMerge)
    onBranch = np.repeat(0, nPoints)
    RootBranch = 0
    mergeDiagnostics = dict(smI = np.repeat(np.nan, nMerge), smSize = np.repeat(np.nan, nMerge), 
                            smCrSc = np.repeat(np.nan, nMerge), smGap = np.repeat(np.nan, nMerge), 
                            lgI = np.repeat(np.nan, nMerge), lgSize = np.repeat(np.nan, nMerge), 
                            lgCrSc = np.repeat(np.nan, nMerge), lgGap = np.repeat(np.nan, nMerge),
                            merged = np.repeat(np.nan, nMerge))
    if nExternalSplits > 0:
        #externalMergeDiags = pd.DataFrame(np.repeat(np.nan, nMerge*nExternalSplits).reshape(nMerge, nExternalSplits))
        #externalMergeDiags.columns = paste("externalBranchSplit", nExternalSplits, sep = ".")
        pass
    extender = np.zeros(chunkSize, dtype=int)
    for merge in range(nMerge):
        if dendro_height[merge] <= cutHeight:
            # are both merged objects singletons?
            if dendro_merge[merge, 0] < 0 and dendro_merge[merge, 1] < 0:
                nBranches = nBranches + 1
                branch_isBasic[nBranches - 1] = True
                branch_isTopBasic[nBranches - 1] = True
                branch_singletons[nBranches - 1] = np.append(-dendro_merge[merge,], extender)
                branch_basicClusters[nBranches - 1] = extender
                branch_mergingHeights[nBranches - 1] = np.append(np.repeat(dendro_height[merge], 2), extender)
                branch_singletonHeights[nBranches - 1] = np.append(np.repeat(dendro_height[merge], 2), extender)
                IndMergeToBranch[merge] = nBranches
                RootBranch = nBranches
            elif sign(dendro_merge[merge,0]) * sign(dendro_merge[merge,1]) < 0:
                clust = IndMergeToBranch[int(np.max(dendro_merge[merge,])) - 1]
                if clust == 0: raise ValueError("a previous merge has no associated cluster. Sorry!")
                gene = -np.min(dendro_merge[merge,])
                ns = branch_nSingletons[clust - 1] + 1
                nm = branch_nMerge[clust - 1] + 1
                if branch_isBasic[clust - 1]:
                    if ns > len(branch_singletons[clust - 1]):
                        branch_singletons[clust - 1] = np.append(branch_singletons[clust - 1], extender)
                        branch_singletonHeights[clust - 1] = np.append(branch_singletonHeights[clust - 1], extender)
                    branch_singletons[clust - 1][ns - 1] = gene
                    branch_singletonHeights[clust - 1][ns - 1] = dendro_height[merge]
                else:
                    onBranch[int(gene) - 1] = clust
                if nm >= len(branch_mergingHeights[clust - 1]):
                    branch_mergingHeights[clust - 1] = np.append(branch_mergingHeights[clust - 1], extender)
                branch_mergingHeights[clust - 1][nm - 1] = dendro_height[merge]
                branch_size[clust - 1] = branch_size[clust - 1] + 1
                branch_nMerge[clust - 1] = nm
                branch_nSingletons[clust - 1] = ns
                IndMergeToBranch[merge] = clust
                RootBranch = clust
            else:
                # attempt to merge two branches:
                clusts = IndMergeToBranch[dendro_merge[merge,] - 1]
                sizes = branch_size[clusts - 1]
                # Note: for 2 elements, rank and order are the same.
                rnk = rankdata(sizes, method = "ordinal")
                small = clusts[rnk[0] - 1]
                large = clusts[rnk[1] - 1]
                sizes = sizes[rnk - 1]
                branch1 = np.nan if np.any(np.isnan(branch_singletons[large - 1])) else branch_singletons[large - 1][np.arange(sizes[1])]
                branch2 = np.nan if np.any(np.isnan(branch_singletons[small - 1])) else branch_singletons[small - 1][np.arange(sizes[0])]
                spyMatch = False
                if spyIndex != None:
                    n1 = len(set(branch1) & set(spyIndex))
                    if n1 / len(branch1) > 0.99 and n1 / len(spyIndex) > 0.99:
                        print("Found spy match for branch 1 on merge", merge)
                        spyMatch = True
                    n2 = len(set(branch2) & set(spyIndex))
                    if n2 / len(branch1) > 0.99 and n2 / len(spyIndex) > 0.99:
                        print("Found spy match for branch 2 on merge", merge)
                        spyMatch = True
                if branch_isBasic[small - 1]:
                    coresize = CoreSize(branch_nSingletons[small - 1], minClusterSize)
                    Core = np.array(branch_singletons[small - 1][np.arange(int(coresize))], dtype=int)
                    # SmAveDist = mean(apply(distM[Core, Core], 2, sum)/(coresize-1))
                    SmAveDist = np.mean(np.sum(dist_multi_index(Core - 1, distM), axis=1) / (coresize - 1))     
                else:
                    SmAveDist = 0
                if branch_isBasic[large - 1]:
                    coresize = CoreSize(branch_nSingletons[large - 1], minClusterSize)
                    Core = np.array(branch_singletons[large - 1][np.arange(int(coresize))], dtype=int)
                    LgAveDist = np.mean(np.sum(dist_multi_index(Core - 1, distM), axis=1) / (coresize -1 ))
                else:
                    LgAveDist = 0
                for key in mergeDiagnostics:
                    if key == "smI":
                        mergeDiagnostics[key][merge] = small
                    elif key == "smSize":
                        mergeDiagnostics[key][merge] = branch_size[small - 1]
                    elif key == "smCrSc":
                        mergeDiagnostics[key][merge] = SmAveDist
                    elif key == "smGap":
                        mergeDiagnostics[key][merge] = dendro_height[merge] - SmAveDist
                    elif key == "lgI":
                        mergeDiagnostics[key][merge] = large
                    elif key == "lgSize":
                        mergeDiagnostics[key][merge] = branch_size[large - 1]
                    elif key == "lgCrSc":
                        mergeDiagnostics[key][merge] = LgAveDist
                    elif key == "lgGap":
                        mergeDiagnostics[key][merge] = dendro_height[merge] - LgAveDist
                    elif key == "merged":
                        mergeDiagnostics[key][merge] = np.nan
                # We first check each cluster separately for being too small, too diffuse, or too shallow:
                SmallerScores = [branch_isBasic[small - 1], 
                                 branch_size[small - 1] < minClusterSize,
                                 SmAveDist > maxAbsCoreScatter, 
                                 dendro_height[merge] - SmAveDist < minAbsGap,
                                 dendro_height[merge] < minAbsSplitHeight]
                if SmallerScores[0] * np.sum(SmallerScores[1:]) > 0:
                    DoMerge = True
                    SmallerFailSize = ~np.logical_or(SmallerScores[2], SmallerScores[3])  # Smaller fails only due to size
                else:
                    LargerScores = [branch_isBasic[large - 1], 
                                    branch_size[large - 1] < minClusterSize,
                                    LgAveDist > maxAbsCoreScatter, 
                                    dendro_height[merge] - LgAveDist < minAbsGap,
                                    dendro_height[merge] < minAbsSplitHeight]
                    if LargerScores[0] * np.sum(LargerScores[1:]) > 0:
                        # Actually: the large one is the one to be merged
                        DoMerge = True
                        SmallerFailSize = ~np.logical_or(LargerScores[2], LargerScores[3])  # cluster fails only due to size
                        x = small
                        small = large
                        large = x
                        sizes = sizes[::-1]
                    else:
                        DoMerge = False # None of the two satisfies merging criteria
                if DoMerge:
                    mergeDiagnostics["merged"][merge] = 1
                if ~DoMerge and nExternalSplits > 0 and branch_isBasic[small - 1] and branch_isBasic[large - 1]:
                    if verbose > 4: print("Entering external split code on merge ", merge)
                    branch1 = branch_singletons[large - 1][np.arange(sizes[1])]
                    branch2 = branch_singletons[small - 1][np.arange(sizes[0])]
                    if verbose > 4 or spyMatch: print("  ..branch lengths: ", sizes[0], ", ", sizes[1])
                    #if (any(is.na(branch1)) || any(branch1==0)) browser();
                    #if (any(is.na(branch2)) || any(branch2==0)) browser();
                    ##### fix after External Splits is understood better
                    es = 0
                    while es < nExternalSplits and ~DoMerge:
                        es = es + 1
                        args = externalSplitOptions[es - 1]
                        args = [args, list(branch1 = branch1, branch2 = branch2)]
                        #extSplit = do.call(externalBranchSplitFnc[es], args)
                        if spyMatch:
                            print(" .. external criterion ", es, ": ", extSplit)
                        DoMerge = extSplit < minExternalSplit[es - 1]
                        externalMergeDiags[merge, es - 1] = extSplit
                        if DoMerge:
                            mergeDiagnostics_merged[merge] = 2
                        else:
                            mergeDiagnostics_merged[merge] = 0
                if DoMerge:
                    # merge the small into the large cluster and close it.
                    branch_failSize[small - 1] = SmallerFailSize
                    branch_mergedInto[small - 1] = large
                    branch_attachHeight[small - 1] = dendro_height[merge]
                    branch_isTopBasic[small - 1] = False
                    nss = branch_nSingletons[small - 1]
                    nsl = branch_nSingletons[large - 1]
                    ns = nss + nsl
                    if branch_isBasic[large - 1]: 
                        nExt = np.ceil(  (ns - len(branch_singletons[large - 1])) / chunkSize  )
                        if nExt > 0:
                            if verbose > 5:
                                print("Extending singletons for branch", large, "by", nExt, " extenders.")
                            branch_singletons[large - 1] = np.append(branch_singletons[large - 1], np.repeat(extender, nExt))
                            branch_singletonHeights[large - 1] = np.append(branch_singletonHeights[large - 1], np.repeat(extender, nExt))
                        branch_singletons[large - 1][np.arange(nsl,ns)] = branch_singletons[small - 1][np.arange(nss)]
                        branch_singletonHeights[large - 1][np.arange(nsl,ns)] = branch_singletonHeights[small - 1][np.arange(nss)]
                        branch_nSingletons[large - 1] = ns
                    else:
                        if ~branch_isBasic[small - 1]:
                            raise ValueError("merging two composite clusters. Sorry!")
                        onBranch[ branch_singletons[small - 1][branch_singletons[small - 1] != 0] - 1 ] = large
                    nm = branch_nMerge[large - 1] + 1
                    if nm > len(branch_mergingHeights[large - 1]):
                        branch_mergingHeights[large - 1] = np.append(branch_mergingHeights[large - 1], extender)
                    branch_mergingHeights[large - 1][nm - 1] = dendro_height[merge]
                    branch_nMerge[large - 1] = nm
                    branch_size[large - 1] = branch_size[small - 1] + branch_size[large - 1]
                    IndMergeToBranch[merge] = large
                    RootBranch = large
                else:
                    # start or continue a composite cluster.
                    # If large is basic and small is not basic, switch them.
                    if branch_isBasic[large - 1] and ~branch_isBasic[small - 1]:
                        x = large
                        large = small
                        small = x
                        sizes = sizes[::-1]
                    # Note: if pamRespectsDendro, need to start a new composite cluster every time two branches merge,
                    # otherwise will not have the necessary information.
                    # Otherwise, if the large cluster is already composite, I can simply merge both clusters into 
                    # one of the non-composite clusters.
                    if branch_isBasic[large - 1] or pamStage and pamRespectsDendro:
                        nBranches = nBranches + 1
                        branch_attachHeight[[large - 1, small - 1]] = dendro_height[merge]
                        branch_mergedInto[[large - 1, small - 1]] = nBranches
                        if branch_isBasic[small - 1]:
                            addBasicClusters = small # add basic clusters
                        else:
                            addBasicClusters = branch_basicClusters[small - 1]
                        if branch_isBasic[large - 1]:
                            addBasicClusters = np.append(addBasicClusters, large)
                        else:
                            addBasicClusters = np.append(addBasicClusters, branch_basicClusters[large - 1])
                        # print(paste("  Starting a composite cluster with number", nBranches));
                        branch_isBasic[nBranches - 1] = False
                        branch_isTopBasic[nBranches - 1] = False
                        branch_basicClusters[nBranches - 1] = addBasicClusters
                        branch_mergingHeights[nBranches - 1] = np.append(np.repeat(dendro_height[merge], 2), extender)
                        branch_nMerge[nBranches - 1] = 2
                        branch_size[nBranches - 1] = np.sum(sizes)
                        branch_nBasicClusters[nBranches - 1] = len(addBasicClusters)
                        IndMergeToBranch[merge] = nBranches
                        RootBranch = nBranches
                    else:
                        # Add small branch to the large one 
                        addBasicClusters = small if branch_isBasic[small - 1] else branch_basicClusters[small - 1]
                        nbl = branch_nBasicClusters[large - 1]
                        #small might be an int
                        try:
                            nb = branch_nBasicClusters[large - 1] + len(addBasicClusters)
                        except TypeError:
                            nb = branch_nBasicClusters[large - 1] + 1
                        if nb > len(branch_basicClusters[large - 1]):
                            nExt = np.ceil(  ( nb - len(branch_basicClusters[large - 1])) / chunkSize)
                            branch_basicClusters[large - 1] = np.append(branch_basicClusters[large - 1], np.repeat(extender, nExt))
                        branch_basicClusters[large - 1][np.arange(nbl,nb)] = addBasicClusters
                        branch_nBasicClusters[large - 1] = nb
                        branch_size[large - 1] = branch_size[large - 1] + branch_size[small - 1]
                        nm = branch_nMerge[large - 1] + 1
                        if nm > len(branch_mergingHeights[large - 1]):
                            branch_mergingHeights[large - 1] = np.append(branch_mergingHeights[large - 1], extender) 
                        branch_mergingHeights[large - 1][nm - 1] = dendro_height[merge]
                        branch_nMerge[large - 1] = nm
                        branch_attachHeight[small - 1] = dendro_height[merge]
                        branch_mergedInto[small - 1] = large
                        IndMergeToBranch[merge] = large
                        RootBranch = large
    if verbose > 2: print("..Going through detected branches and marking clusters..")
    isCluster = np.repeat(False, nBranches)
    SmallLabels = np.repeat(0, nPoints)
    for clust in range(nBranches):
        if np.isnan(branch_attachHeight[clust]): branch_attachHeight[clust] = cutHeight
        if branch_isTopBasic[clust]:
            coresize = CoreSize(branch_nSingletons[clust], minClusterSize)
            Core = branch_singletons[clust][np.arange(coresize)]
            CoreScatter = np.mean(np.sum(dist_multi_index(Core - 1, distM), axis=1) / (coresize - 1))
            isCluster[clust] = np.logical_and(np.logical_and(branch_isTopBasic[clust],
                                                             branch_size[clust] >= minClusterSize),
                                              np.logical_and(CoreScatter < maxAbsCoreScatter,
                                                             branch_attachHeight[clust] - CoreScatter > minAbsGap))
        else:
            CoreScatter = 0
        if branch_failSize[clust]: SmallLabels[branch_singletons[clust][branch_singletons[clust] != 0] - 1] = clust + 1
    if not respectSmallClusters: SmallLabels = np.repeat(0, nPoints)
    if verbose > 2: print(spaces, "..Assigning Tree Cut stage labels..")
    Colors = np.repeat(0, nPoints)
    coreLabels = np.repeat(0, nPoints)
    clusterBranches = np.arange(nBranches)[isCluster]
    branchLabels = np.repeat(0, nBranches)
    color = 0
    for clust in clusterBranches:
        color = color + 1
        Colors[branch_singletons[clust][branch_singletons[clust] != 0] - 1] = color
        SmallLabels[branch_singletons[clust][branch_singletons[clust] != 0] - 1] = 0
        coresize = CoreSize(branch_nSingletons[clust], minClusterSize)
        Core = branch_singletons[clust][np.arange(coresize)]
        coreLabels[Core - 1] = color
        branchLabels[clust] = color
    Labeled = np.arange(nPoints)[Colors != 0]
    Unlabeled = np.arange(nPoints)[Colors == 0]
    nUnlabeled = len(Unlabeled)
    UnlabeledExist = nUnlabeled > 0
    if len(Labeled) > 0:
        LabelFac = factor(Colors[Labeled])
        nProperLabels = nlevels(LabelFac)
    else:
        nProperLabels = 0
    if pamStage and UnlabeledExist and nProperLabels > 0:
        if verbose > 2: print(spaces, "..Assigning PAM stage labels..")
        nPAMed = 0
        # Assign some of the grey genes to the nearest module. Define nearest as the distance to the medoid,
        # that is the point in the cluster that has the lowest average distance to all other points in the
        # cluster. First get the medoids.
        if useMedoids:
            Medoids = np.repeat(0, nProperLabels)
            ClusterRadii = np.repeat(0.0, nProperLabels)
            for cluster in range(1, nProperLabels + 1):
                InCluster = np.arange(1,nPoints+1)[Colors == cluster]
                DistInCluster = dist_multi_index(InCluster - 1, distM)
                #DistInCluster = distM[InCluster, InCluster]
                DistSums = np.sum(DistInCluster, axis=1)
                Medoids[cluster - 1] = InCluster[np.argmin(DistSums)]
                ClusterRadii[cluster - 1] = np.max(DistInCluster[:, np.argmin(DistSums)])
            # If small clusters are to be respected, assign those first based on medoid-medoid distances.
            if respectSmallClusters:
                FSmallLabels = factor(SmallLabels)
                SmallLabLevs = levels(FSmallLabels)
                nSmallClusters = nlevels(FSmallLabels) - (SmallLabLevs[0] == 0)
                if nSmallClusters > 0 :
                    for sclust in SmallLabLevs[SmallLabLevs != 0]:
                        InCluster = np.arange(nPoints)[SmallLabels == sclust]
                        if pamRespectsDendro:
                            onBr = np.unique(onBranch[InCluster])
                            if len(onBr) > 1:
                                raise ValueError("Internal error: objects in a small cluster are marked to belong",
                                                 "\nto several large branches:")
                            if onBr > 0:
                                basicOnBranch = branch_basicClusters[onBr[0] - 1]
                                labelsOnBranch = branchLabels[basicOnBranch - 1]
                            else:
                                labelsOnBranch = None
                        else:
                            labelsOnBranch = np.arange(1, nProperLabels + 1)
                        # printFlush(paste("SmallCluster", sclust, "has", length(InCluster), "elements."));
                        DistInCluster = dist_multi_index(InCluster, distM)
                        #DistInCluster = distM[InCluster, InCluster]
                        if len(labelsOnBranch) > 0:
                            if len(InCluster) > 1:
                                DistSums = apply(np.sum, DistInCluster, 1)
                                smed = InCluster[np.argmin(DistSums)]
                                DistToMeds = get_rows(Medoids[labelsOnBranch - 1][Medoids[labelsOnBranch - 1] != 0] - 1, distM)[:, smed]
                                closest = np.argmin(DistToMeds)
                                DistToClosest = DistToMeds[closest]
                                closestLabel = labelsOnBranch[closest]
                                if DistToClosest < ClusterRadii[closestLabel - 1] or DistToClosest <  maxPamDist:
                                    Colors[InCluster] = closestLabel
                                    nPAMed = nPAMed + len(InCluster)
                                else: Colors[InCluster] = -1  # This prevents individual points from being assigned later 
                        else:
                            Colors[InCluster] = -1
                # Assign leftover unlabeled objects to clusters with nearest medoids
                Unlabeled = np.arange(nPoints)[Colors == 0]
                if len(Unlabeled > 0):
                    for obj in Unlabeled:
                        if pamRespectsDendro:
                            onBr = onBranch[obj]
                            if onBr > 0:
                                basicOnBranch = branch_basicClusters[onBr - 1]
                                labelsOnBranch = branchLabels[basicOnBranch - 1]
                            else:
                                labelsOnBranch = None
                        else:
                            labelsOnBranch = np.arange(nProperLabels)
                        if labelsOnBranch != None:
                            UnassdToMedoidDist = get_rows(Medoids[labelsOnBranch - 1] - 1, distM)[:,obj]
                            #UnassdToMedoidDist = distM[Medoids[labelsOnBranch], obj]
                            nearest= np.argmin(UnassdToMedoidDist)
                            NearestCenterDist = UnassdToMedoidDist[nearest]
                            nearestMed = labelsOnBranch[nearest]
                            if NearestCenterDist < ClusterRadii[nearestMed - 1] or NearestCenterDist < maxPamDist:
                                Colors[obj] = nearestMed
                                nPAMed = nPAMed + 1
                UnlabeledExist = np.sum(Colors == 0) > 0
        else: # Instead of medoids, use average distances
        # This is the default method, so I will try to tune it for speed a bit.
            ClusterDiam = np.repeat(0, nProperLabels)
            for cluster in range(nProperLabels):
                InCluster = np.arange(nPoints)[Colors == cluster]
                nInCluster = len(InCluster)
                DistInCluster = dist_multi_index(InCluster, distM)
                #DistInCluster = distM[InCluster, InCluster]
                if nInCluster > 1:
                    AveDistInClust = np.sum(DistInCluster, axis=1) / (nInCluster - 1)
                    ClusterDiam[cluster] = np.max(AveDistInClust)
                else:
                    ClusterDiam[cluster] = 0
            # If small clusters are respected, assign them first based on average cluster-cluster distances.
            ColorsX = Colors.copy()
            if respectSmallClusters:
                FSmallLabels = factor(SmallLabels) #### think about
                SmallLabLevs = levels(FSmallLabels) ##### think about
                nSmallClusters = nlevels(FSmallLabels) - (SmallLabLevs[0] == 0)
                if nSmallClusters > 0:
                    if pamRespectsDendro:
                        for sclust in SmallLabLevs[SmallLabLevs != 0]:
                            InCluster = np.arange(nPoints)[SmallLabels == sclust]
                            onBr = np.unique(onBranch[InCluster])
                            if len(onBr) > 1:
                                raise ValueError("objects in a small cluster are marked to belong",
                                                 "\nto several large branches:")
                            if onBr > 0:
                                basicOnBranch = branch_basicClusters[onBr[0] - 1]
                                labelsOnBranch = branchLabels[basicOnBranch - 1]
                                useObjects = np.in1d(ColorsX, np.unique(labelsOnBranch))
                                DistSClustClust = get_rows(InCluster, distM)[:,useObjects]
                                #DistSClustClust = distM[InCluster, useObjects]
                                MeanDist = np.mean(DistSClustClust, axis=0)
                                useColorsFac = factor(ColorsX[useObjects]) ### think about
                                MeanMeanDist = tapply(MeanDist, useColorsFac, np.mean) ## think about
                                nearest = np.argmin(MeanMeanDist)
                                NearestDist = MeanMeanDist[nearest]
                                nearestLabel = levels(useColorsFac)[nearest] ## think about
                                if NearestDist < ClusterDiam[nearestLabel - 1] or NearestDist <  maxPamDist:
                                    Colors[InCluster] = nearestLabel
                                    nPAMed = nPAMed + len(InCluster)
                                else:
                                    Colors[InCluster] = -1  # This prevents individual points from being assigned later
                    else:
                        labelsOnBranch = np.arange(nProperLabels)
                        useObjects = np.arange(nPoints)[ColorsX != 0]
                        for sclust in SmallLabLevs[SmallLabLevs != 0]:
                            InCluster = np.arange(nPoints)[SmallLabels == sclust]
                            DistSClustClust = get_rows(InCluster, distM)[:,useObjects]
                            #DistSClustClust = distM[InCluster, useObjects]
                            MeanDist = np.mean(DistSClustClust, axis=0)
                            useColorsFac = factor(ColorsX[useObjects]) ### think about
                            MeanMeanDist = tapply(MeanDist, useColorsFac, np.mean) ### think about
                            nearest = np.argmin(MeanMeanDist)
                            NearestDist = MeanMeanDist[nearest]
                            nearestLabel = levels(useColorsFac)[nearest] ## think about
                            if NearestDist < ClusterDiam[nearestLabel - 1] or NearestDist <  maxPamDist:
                                Colors[InCluster] = nearestLabel
                                nPAMed = nPAMed + len(InCluster)
                            else:
                                Colors[InCluster] = -1  # This prevents individual points from being assigned later
            # Assign leftover unlabeled objects to clusters with nearest medoids
            Unlabeled = np.arange(nPoints)[Colors == 0]
            #ColorsX = Colors;
            if len(Unlabeled) > 0:
                if pamRespectsDendro:
                    unlabOnBranch = Unlabeled[onBranch[Unlabeled] > 0]
                    for obj in unlabOnBranch:
                        onBr = onBranch[obj]
                        basicOnBranch = branch_basicClusters[onBr - 1]
                        labelsOnBranch = branchLabels[basicOnBranch - 1]
                        useObjects = np.in1d(ColorsX, np.unique(labelsOnBranch))
                        useColorsFac = factor(ColorsX[useObjects]) ### think about
                        #UnassdToClustDist = tapply(distM[useObjects, obj], useColorsFac, mean) ### think about
                        UnassdToClustDist = tapply(get_rows(useObjects, distM)[:,obj], useColorsFac, np.mean) ### think about
                        nearest = np.argmin(UnassdToClustDist)
                        NearestClusterDist = UnassdToClustDist[nearest]
                        nearestLabel = levels(useColorsFac)[nearest] ### think about
                        if NearestClusterDist < ClusterDiam[nearestLabel - 1] or NearestClusterDist < maxPamDist:
                            Colors[obj] = nearestLabel
                            nPAMed = nPAMed + 1
                else:
                    useObjects = np.arange(nPoints)[ColorsX != 0]
                    useColorsFac = factor(ColorsX[useObjects]) ## think about
                    nUseColors = nlevels(useColorsFac) ### think about
                    UnassdToClustDist = tapply_df(get_rows(useObjects, distM)[:,Unlabeled], useColorsFac, np.mean, 1)
                    #UnassdToClustDist = df_apply.apply(distM[useObjects, Unlabeled], 1, tapply, useColorsFac, mean) ### think about
                    # Fix dimensions for the case when there's only one cluster
                    #dim(UnassdToClustDist) = np.append(nUseColors, len(Unlabeled)) ### think about
                    nearest = apply(np.argmin, UnassdToClustDist, 1)
                    nearestDist = apply(np.min, UnassdToClustDist, 1)
                    nearestLabel = levels(useColorsFac)[nearest - 1] ### think about
                    assign = np.logical_or(nearestDist < ClusterDiam[nearestLabel - 1], nearestDist < maxPamDist)
                    Colors[Unlabeled[assign]] = nearestLabel[assign]
                    nPAMed = nPAMed + np.sum(assign)
        if verbose > 2: print("....assigned", nPAMed, "objects to existing clusters.")
    # Relabel labels such that 1 corresponds to the largest cluster etc.
    Colors[Colors < 0] = 0
    UnlabeledExist = np.sum(Colors == 0) > 0
    NumLabs = Colors + 1
    Sizes = table(NumLabs) ### think about
    Sizes.index=np.arange(len(Sizes))
    if UnlabeledExist:
        if len(Sizes) > 1:
            SizeRank = np.append(1, rankdata(-Sizes[1:len(Sizes)], method="ordinal")+1)
        else:
            SizeRank = 1
        OrdNumLabs = SizeRank[NumLabs - 1]
    else:
        SizeRank = rankdata(-Sizes[np.arange(len(Sizes))], method="ordinal")
        OrdNumLabs = SizeRank[NumLabs - 2]
    ordCoreLabels = OrdNumLabs - UnlabeledExist
    ordCoreLabels[coreLabels == 0] = 0
    if verbose > 0: print( "..done.")
    results = dict(labels = OrdNumLabs-UnlabeledExist,
                   cores = ordCoreLabels,
                   smallLabels = SmallLabels,
                   onBranch = onBranch,
                   mergeDiagnostics = mergeDiagnostics if nExternalSplits==0 else pd.DataFrame({'x':mergeDiagnostics, 'y':externalMergeDiags}),
                   mergeCriteria = dict(maxCoreScatter = maxCoreScatter, minGap = minGap, 
                                        maxAbsCoreScatter = maxAbsCoreScatter, minAbsGap = minAbsGap, 
                                        minExternalSplit = minExternalSplit),
                   branches  = dict(nBranches = nBranches, # Branches = Branches, 
                                    IndMergeToBranch = IndMergeToBranch,
                                    RootBranch = RootBranch, isCluster = isCluster, 
                                    nPoints = nMerge+1))
    return(results)
def sign(value):
    #python version of R's sign
    if value > 0:
        return(1)
    elif value < 0:
        return(-1)
    else:
        return(0)
def paste(string, n, sep=""):
    #python version of R's paste
    results = []
    for i in range(n):
        results.append(string + sep + str(i))
    return(results)
def get_heights(Z):
    #python verison of R's dendro$height
    #height = np.zeros(len(dendro["dcoord"]))
    #for i, d in enumerate(dendro["dcoord"]):
        #height[i] = d[1]
    clusternode = to_tree(Z, True)
    #height = np.array([c.dist for c in clusternode[1]])
    height = np.array([c.dist for c in clusternode[1] if c.is_leaf() != True])
    #height.sort()
    return(height)
def get_merges(z):
    #python version of R's dendro$merge
    n = z.shape[0]
    merges = np.zeros((z.shape[0], 2), dtype=int)
    for i in range(z.shape[0]):
        for j in range(2):
            if z[i][j] <= n:
                merges[i][j] = -(z[i][j] + 1)
            else:
                cluster = z[i][j] - n
                merges[i][j] = cluster
    return(merges)
def factor(vector):
    return(vector)
def nlevels(vector):
    #python version of R's nlevels
    return(len(np.unique(vector)))
def levels(vector):
    #python version of R's levels
    return(np.unique(vector))
def tapply(vector, index, function): #can add **args, **kwargs
    #python version of R's tapply
    factors = np.unique(index)
    #results = pd.Series(np.repeat(np.nan, len(factors)))
    results = np.repeat(np.nan, len(factors))
    #results.index = factors
    for i, k in enumerate(factors):
        subset = vector[index == k]
        #results.iloc[i] = function(subset)
        results[i] = function(subset)
    return(results)
def tapply_df(df, index, function, axis=0): #can add **args, **kwargs
    #python version of R's tapply
    factors = np.unique(index)
    if axis == 1:
        #results = pd.DataFrame(np.zeros((len(factors), df.shape[1])))
        results = np.zeros((len(factors), df.shape[1]))
    else:
        #results = pd.DataFrame(np.zeros((df.shape[0], len(factors))))
        results = np.zeros((df.shape[0], len(factors)))
    #results.index = factors
    if axis == 1:
        for j in range(df.shape[1]):
            for i, k in enumerate(factors):
                subset = df[index == k, j]
                #results.iloc[i, j] = function(subset)
                results[i, j] = function(subset)
    else:
        for i in range(df.shape[0]):
            for j, k in enumerate(factors):
                subset = df[i, index == k]
                #results.iloc[i, j] = function(subset)
                results[i, j] = function(subset)
    return(results)
class pyWGCNA(object):
    def __init__(self,data:pd.DataFrame,save_path:str=''):
        self.data=data.fillna(0)
        self.data_len=len(data)
        self.data_index=data.index
        self.save_path=save_path
    def mad_filtered(self,gene_num:int=5000):
        from statsmodels import robust #import package
        gene_mad=self.data.T.apply(robust.mad) #use function to calculate MAD
        self.data=self.data.loc[gene_mad.sort_values(ascending=False).index[:gene_num]]
    def calculate_correlation_direct(self,method:str='pearson',save:bool=False):
        print('...correlation coefficient matrix is being calculated')
        self.result=self.data.T.corr(method)
        if save==True:
            self.result.to_csv(self.save_path+'/'+'direction correlation matrix.csv')
            print("...direction correlation have been saved")
    def calculate_correlation_indirect(self,save:bool=False):
        print('...indirect correlation matrix is being calculated')
        np.fill_diagonal(self.result.values, 0)
        arr=abs(self.result)
        arr_len=len(arr)
        self.temp=np.zeros(arr_len)
        for i in range(1,3):    
            self.temp=self.temp+(arr**i)/i
        from sklearn import preprocessing
        min_max_scaler = preprocessing.MinMaxScaler()
        X_minMax = min_max_scaler.fit_transform(self.temp)
        self.normal_corelation=pd.DataFrame(X_minMax,index=self.temp.index,columns=self.temp.columns)
        if save==True:
            self.temp.to_csv(self.save_path+'/'+'indirection correlation matrix.csv')
            print("...indirection correlation have been saved")
    def calculate_soft_threshold(self,threshold_range:int=12,
                                 plot:bool=True,save:bool=False,
                                 figsize:tuple=(6,3))->pd.DataFrame:
        print('...soft_threshold is being calculated')
        soft=6
        re1=pd.DataFrame(columns=['beta','r2','meank'])
        for j in range(1,threshold_range):
            #print('Now is:',j)
            result_i=np.float_power(self.temp,j)
            tt_0=np.sum(abs(result_i),axis=0)-1
            n=np.histogram(tt_0), #
            x=n[0][0]
            y=[]
            for i in range(len(n[0][1])-1):
                y.append((n[0][1][i]+n[0][1][i+1])/2)
            x=np.log10(x)
            y=np.log10(y)
            res=stats.linregress(x, y)
            r2=np.float_power(res.rvalue,2)
            k=tt_0.mean()
            re1.loc[j]={'beta':j,'r2':r2,'meank':k}
        for i in re1['r2']:
            if i>0.85:
                soft=re1[re1['r2']==i]['beta'].iloc[0]
                break
        self.re1=re1
        self.soft=soft
        print('...appropriate soft_thresholds:',soft)
        if plot==True:
            fig, ax = plt.subplots(1,2,figsize=figsize)
            ax[0].scatter(re1['beta'],re1['r2'],c='b')
            ax[0].plot([0,threshold_range],[0.95,0.95],c='r')
            ax[0].set_xlim(0,threshold_range)
            ax[0].set_ylabel('r2',fontsize=14)
            ax[0].set_xlabel('beta',fontsize=14)
            ax[0].set_title('Best Soft threshold')
            ax[1].scatter(re1['beta'],re1['meank'],c='r')
            ax[1].set_ylabel('meank',fontsize=14)
            ax[1].set_xlabel('beta',fontsize=14)
            ax[1].set_title('Best Soft threshold')
            fig.tight_layout()
        if save==True:
            fig.savefig(self.save_path+'/'+'soft_threshold_hist.png',dpi=300)
        return re1
    def calculate_corr_matrix(self):
        self.cor=np.float_power(self.temp,self.soft)
        np.fill_diagonal(self.cor.values, 1.0)
    def calculate_distance(self,trans:bool=True):
        #distance
        print("...distance have being calculated")
        if trans==True:
            self.distances = pdist(1-self.cor, "euclidean")
        else:
            self.distances = pdist(self.cor, "euclidean")
    def calculate_geneTree(self,linkage_method:str='ward'):
        #geneTree
        print("...geneTree have being calculated")
        self.geneTree=linkage(self.distances, linkage_method)
    def calculate_dynamicMods(self,minClusterSize:int=30,
                  deepSplit:int=2,):
        #dynamicMods
        print("...dynamicMods have being calculated")
        self.dynamicMods=cutreeHybrid(self.geneTree,distM=self.distances,minClusterSize = minClusterSize,
                                            deepSplit = deepSplit, pamRespectsDendro = False)
        print("...total:",len(set(self.dynamicMods['labels'])))
    def calculate_gene_module(self,figsize:tuple=(25,10),save:bool=True,
                              colorlist:list=None)->pd.DataFrame:
        plt.figure(figsize=figsize)
        grid = plt.GridSpec(3, 1, wspace=0.5, hspace=0.1)
        plt.subplot(grid[0:2,0])
        hierarchy.set_link_color_palette(['#000000'])
        dn=hierarchy.dendrogram(self.geneTree,color_threshold=0, above_threshold_color='black')
        plt.tick_params( \
            axis='x',
            which='both',
            bottom='off',
            top='off',
            labelbottom='off')
        #mol
        #ivl=dn['ivl']
        mod=[self.dynamicMods['labels'][int(x)] for x in dn['ivl']]
        plot_mod=np.array([mod])
        gene_name=[self.cor.index[int(x)] for x in dn['ivl']]
        mol=pd.DataFrame(columns=['ivl','module','name'])
        mol['ivl']=dn['ivl']
        mol['module']=mod
        mol['name']=gene_name
        res=[]
        for i in plot_mod[0]:
            if i in res:
                continue
            else:
                res.append(i)
        res_len=len(res)
        if colorlist!=None:
            colorlist_cmap=LinearSegmentedColormap.from_list('Custom', colorlist[:res_len], len(colorlist[:res_len]))
            color_dict=dict(zip(range(1,res_len+1),colorlist[:res_len]))
            mol['color']=mol['module'].map(color_dict)
            plt.subplot(grid[2,0])
            ax1=plt.pcolor(plot_mod,cmap=colorlist_cmap)
        else:
            if res_len>28:
                colorlist=sc.pl.palettes.default_102
            else:
                colorlist=pyomic_palette()
            colorlist_cmap=LinearSegmentedColormap.from_list('Custom', colorlist[:res_len], len(colorlist[:res_len]))
            color_dict=dict(zip(range(1,res_len+1),colorlist[:res_len]))
            mol['color']=mol['module'].map(color_dict)
            plt.subplot(grid[2,0])
            ax1=plt.pcolor(plot_mod,cmap=colorlist_cmap)
        self.mol=mol
        if save==True:
            plt.savefig(self.save_path+'/'+'module_tree.png',dpi=300)
        return mol
    def get_sub_module(self,mod_list:list)->pd.DataFrame:
        mol=self.mol
        return mol[mol['module'].isin(mod_list)]
    def get_sub_network(self,mod_list:list,correlation_threshold=0.95)->nx.Graph:
        module1=self.get_sub_module(mod_list)
        gene_net1=abs(self.normal_corelation.loc[module1.name.tolist(),module1.name.tolist()])
        G = nx.Graph()
        for i in gene_net1.index:
            for j in gene_net1.columns:
                if gene_net1.loc[i,j]>correlation_threshold:
                    G.add_weighted_edges_from([(i, j, gene_net1.loc[i,j])])
        return G
    def plot_matrix(self,cmap='RdBu_r',save:bool=True,figsize:tuple=(8,9),
                    legene_ncol:int=2,legene_bbox_to_anchor:tuple=(5, 2.95),legene_fontsize:int=12,):
        module_color=self.mol.copy()
        module_color.index=module_color['name']
        a=sns.clustermap(self.normal_corelation,#standard_scale=1,
                    cmap=cmap,yticklabels=False,xticklabels=False,#method='ward',metric='euclidean',
                    #row_cluster=False,col_cluster=False,
                    col_linkage=self.geneTree,row_linkage=self.geneTree,
                    col_colors=module_color.loc[self.normal_corelation.index,'color'].values,
                    row_colors=module_color.loc[self.normal_corelation.index,'color'].values,
                    cbar_kws={"shrink": .5},square=True,
                    dendrogram_ratio=(.1, .2),
                    cbar_pos=(-.1, .32, .03, .2),figsize=figsize
        )
        #add the color of the module
        for i in range(len(set(self.mol['module']))):
            t1=self.mol.loc[self.mol['module']==i+1]
            a.ax_heatmap.add_patch(plt.Rectangle((t1.index[0],t1.index[0]),len(t1),len(t1),fill=False,color='black',lw=1))
        color=[]
        labels=[]
        for i in module_color['module'].unique():
            t1=module_color.loc[module_color['module']==i].iloc[0]
            color.append(t1['color'])
            labels.append(i)
        from matplotlib import patches as mpatches
        #用label和color列表生成mpatches.Patch对象，它将作为句柄来生成legend
        patches = [ mpatches.Patch(color=color[i], label="ME-{}".format(labels[i]) ) for i in range(len(labels)) ] 
        plt.legend(handles=patches,bbox_to_anchor=legene_bbox_to_anchor, ncol=legene_ncol,fontsize=legene_fontsize)
        if save==True:
            plt.savefig(self.save_path+'/'+'module_matrix.png',dpi=300,bbox_inches = 'tight')
        return a
    def plot_sub_network(self,mod_list:list,
                         correlation_threshold:float=0.95,
                         plot_genes=None,
                         plot_gene_num:int=5,**kwargs)->Tuple[matplotlib.figure.Figure,matplotlib.axes._axes.Axes]:
        module1=self.get_sub_module(mod_list)
        G_type_dict=dict(zip(module1['name'],[str(i) for i in module1['module']]))
        G_color_dict=dict(zip(module1['name'],module1['color']))
        G=self.get_sub_network(mod_list,correlation_threshold)
        G_color_dict=dict(zip(G.nodes,[G_color_dict[i] for i in G.nodes]))
        G_type_dict=dict(zip(G.nodes,[G_type_dict[i] for i in G.nodes]))
        degree_dict = dict(G.degree(G.nodes()))
        de_pd=pd.DataFrame(degree_dict.values(),index=degree_dict.keys(),columns=['Degree'])
        hub_gene=[]
        if plot_genes!=None:
            hub_gene=plot_genes
        else:
            for i in mod_list:
                ret_gene=list(set(de_pd.index) & set(module1.loc[module1['module']==i]['name'].tolist()))
                hub_gene1=de_pd.loc[ret_gene,'Degree'].sort_values(ascending=False)[:plot_gene_num].index.tolist()
                hub_gene+=hub_gene1
        fig,ax=plot_network(G,G_type_dict,G_color_dict,plot_node=hub_gene,**kwargs)
        return fig,ax
    def analysis_meta_correlation(self,meta_data:pd.DataFrame)->Tuple[pd.DataFrame,pd.DataFrame]:
        print("...PCA analysis have being done")
        data=self.data
        module=self.mol
        pcamol=pd.DataFrame(columns=data.columns)
        set_index=set(module['module'])
        for j in set_index:
            newdata=pd.DataFrame(columns=data.columns)
            for i in list(module[module['module']==j].dropna()['name']):
                newdata.loc[i]=data[data.index==i].values[0]
            from sklearn.decomposition import PCA
            pca = PCA(n_components=1) 
            reduced_X = pca.fit_transform(newdata.T)
            tepcamol=pd.DataFrame(reduced_X.T,columns=data.columns)
            pcamol.loc[j]=tepcamol.values[0]
        pcamol.index=set_index
        print("...co-analysis have being done")
        from scipy.stats import spearmanr,pearsonr,kendalltau
        # seed random number generator
        # calculate spearman's correlation
        new_meta=pd.DataFrame()
        new_meta.index=meta_data.index
        for j in meta_data.columns:
            if meta_data[j].dtype=='int64':
                new_meta=pd.concat([new_meta,meta_data[j]],axis=1)
            elif meta_data[j].dtype!='float32': 
                new_meta=pd.concat([new_meta,meta_data[j]],axis=1)
            elif meta_data[j].dtype!='float64':
                new_meta=pd.concat([new_meta,meta_data[j]],axis=1)
            else:
                one_hot = pd.get_dummies(meta_data[j], prefix=j)
                new_meta=pd.concat([new_meta,one_hot],axis=1)
        result_1=pd.DataFrame(columns=new_meta.columns)
        result_p=pd.DataFrame(columns=new_meta.columns)
        for j in new_meta.columns:
            co=[]
            pvv=[]
            for i in range(len(pcamol)):   
                tempcor=pd.DataFrame(columns=['x','y'])
                tempcor['x']=list(new_meta[j])
                tempcor['y']=list(pcamol.iloc[i])
                tempcor=tempcor.dropna()
                coef,pv=pearsonr(tempcor['x'],tempcor['y'])
                co.append(coef)
                pvv.append(pv)
            result_1[j]=co
            result_p[j]=pvv
                #print(coef)
        result_1=abs(result_1)
        result_1.index=set_index
        return result_1,result_p
    def plot_meta_correlation(self,cor_matrix:tuple,
                              label_fontsize:int=10,label_colors:str='red')->matplotlib.axes._axes.Axes:
        cor=cor_matrix[0].copy()
        cor_p=cor_matrix[1].copy()
        cor['module']=cor.index
        cor_p['module']=cor_p.index
        df_melt = pd.melt(cor, id_vars='module', var_name='meta', value_name='correlation')
        df_melt2 = pd.melt(cor_p, id_vars='module', var_name='meta', value_name='pvalue')
        df_melt['pvalue']=df_melt2['pvalue']
        df_melt['logp']=-np.log10(df_melt2['pvalue'])
        df_melt.index=[str(i) for i in df_melt.index]
        df_melt['module']=[str(i) for i in df_melt['module']]
        #new_keys = {'item_key': 'module','group_key': 'meta','sizes_key': 'logp','color_key': 'correlation'}
        import dotplot
        new_keys = {'item_key': 'meta','group_key': 'module','sizes_key': 'logp','color_key': 'correlation'}
        dp = dotplot.DotPlot.parse_from_tidy_data(df_melt, **new_keys)
        #fig, ax = plt.subplots(figsize=(10,2))
        ax = dp.plot(size_factor=10, cmap='Spectral_r',vmax=1,vmin=0,
                    dot_title = '–log10(Pvalue)', colorbar_title = 'Correlation',)
        xlabs=ax.get_axes()[0].get_xticklabels()
        ax.get_axes()[0].set_xticklabels(xlabs,fontsize=label_fontsize)
        ylabs=ax.get_axes()[0].get_yticklabels()
        ax.get_axes()[0].set_yticklabels(ylabs,fontsize=label_fontsize)
        #去除配体
        [i.set_text(i.get_text().split('_')[1]) for i in xlabs]
        ax.get_axes()[0].set_xticklabels(xlabs,fontsize=label_fontsize)
        #上色        
        ax.get_axes()[0].tick_params(axis='both',colors=label_colors, which='both')
        return ax
    def Analysis_cocharacter(self,
                        character,save=True):
        print("...PCA analysis have being done")
        data=self.data
        module=self.mol
        pcamol=pd.DataFrame(columns=data.columns)
        set_index=set(module['module'])
        for j in set_index:
            newdata=pd.DataFrame(columns=data.columns)
            for i in list(module[module['module']==j].dropna()['name']):
                newdata.loc[i]=data[data.index==i].values[0]
            from sklearn.decomposition import PCA
            pca = PCA(n_components=1) 
            reduced_X = pca.fit_transform(newdata.T)
            tepcamol=pd.DataFrame(reduced_X.T,columns=data.columns)
            pcamol.loc[j]=tepcamol.values[0]
        pcamol.index=set_index
        print("...co-analysis have being done")
        from scipy.stats import spearmanr,pearsonr,kendalltau
        # seed random number generator
        # calculate spearman's correlation
        result_1=pd.DataFrame(columns=character.columns)
        result_p=pd.DataFrame(columns=character.columns)
        for j in character.columns:
            co=[]
            pvv=[]
            for i in range(len(pcamol)):   
                tempcor=pd.DataFrame(columns=['x','y'])
                tempcor['x']=list(character[j])
                tempcor['y']=list(pcamol.iloc[i])
                tempcor=tempcor.dropna()
                coef,pv=pearsonr(tempcor['x'],tempcor['y'])
                co.append(coef)
                pvv.append(pv)
            result_1[j]=co
            result_p[j]=pvv
                #print(coef)
        result_1=abs(result_1)
        result_1.index=set_index
        plt.figure(figsize=(10,10))
        sns.heatmap(result_1,vmin=0, vmax=1,center=1,annot=True,square=True)
        if save==True:
            plt.savefig(self.save_path+'/'+'co_character.png',dpi=300)
        return result_1
class Bulk2:
    def __init__(self,bulk_data:pd.DataFrame,single_data:anndata.AnnData,
                 celltype_key:str,bulk_group=None,max_single_cells:int=5000,
                 top_marker_num:int=500,ratio_num:int=1,gpu:Union[int,str]=0):
        self.bulk_data=bulk_data
        self.single_data=single_data
        if self.single_data.shape[0]>max_single_cells:
            print(f"......random select {max_single_cells} single cells")
            import random
            cell_idx=random.sample(self.single_data.obs.index.tolist(),max_single_cells)
            self.single_data=self.single_data[cell_idx,:]
        self.celltype_key=celltype_key
        self.bulk_group=bulk_group
        self.input_data=None
        #self.input_data=bulk2single_data_prepare(bulk_data,single_data,celltype_key)
        #self.cell_target_num = data_process(self.input_data, top_marker_num, ratio_num)
        test2=single_data.to_df()
        sc_ref=pd.DataFrame(columns=test2.columns)
        sc_ref_index=[]
        for celltype in list(set(single_data.obs[celltype_key])):
            sc_ref.loc[celltype]=single_data[single_data.obs[celltype_key]==celltype].to_df().sum()
            sc_ref_index.append(celltype)
        sc_ref.index=sc_ref_index
        self.sc_ref=sc_ref
        if gpu=='mps' and torch.backends.mps.is_available():
            print('Note that mps may loss will be nan, used it when torch is supported')
            self.used_device = torch.device("mps")
        else:
            self.used_device = torch.device(f"cuda:{gpu}") if gpu >= 0 and torch.cuda.is_available() else torch.device('cpu')
        self.history=[]
    def predicted_fraction(self,sep='\t', scaler='mms',
                        datatype='counts', genelenfile=None,
                        mode='overall', adaptive=True, variance_threshold=0.98,
                        save_model_name=None,
                        batch_size=128, epochs=128, seed=1,scale_size=2):
        from ..tape import Deconvolution
        sc_ref=self.sc_ref.copy()
        SignatureMatrix, CellFractionPrediction = \
            Deconvolution(sc_ref, self.bulk_data.T, sep=sep, scaler=scaler,
                        datatype=datatype, genelenfile=genelenfile,
                        mode=mode, adaptive=adaptive, variance_threshold=variance_threshold,
                        save_model_name=save_model_name,
                        batch_size=batch_size, epochs=epochs, seed=seed)
        if self.bulk_group!=None:
            cell_total_num=self.single_data.shape[0]*self.bulk_data[self.bulk_group].mean(axis=1).sum()/self.single_data.to_df().sum().sum()
            print('Predicted Total Cell Num:',cell_total_num)
            self.cell_target_num=dict(pd.Series(CellFractionPrediction.loc[self.bulk_group].mean()*cell_total_num*scale_size).astype(int))
        else:
            cell_total_num=self.single_data.shape[0]*self.bulk_data.mean(axis=1).sum()/self.single_data.to_df().sum().sum()
            print('Predicted Total Cell Num:',cell_total_num)
            self.cell_target_num=dict(pd.Series(CellFractionPrediction.mean()*cell_total_num*scale_size).astype(int))
        return SignatureMatrix, CellFractionPrediction
    def bulk_preprocess_lazy(self,)->None:
        print("......drop duplicates index in bulk data")
        self.bulk_data=data_drop_duplicates_index(self.bulk_data)
        print("......deseq2 normalize the bulk data")
        self.bulk_data=deseq2_normalize(self.bulk_data)
        print("......log10 the bulk data")
        self.bulk_data=np.log10(self.bulk_data+1)
        print("......calculate the mean of each group")
        if self.bulk_group is None:
            self.bulk_seq_group=self.bulk_data
            return None
        else:
            data_dg_v=self.bulk_data[self.bulk_group].mean(axis=1)
            data_dg=pd.DataFrame(index=data_dg_v.index)
            data_dg['group']=data_dg_v
            self.bulk_seq_group=data_dg
        return None
    def single_preprocess_lazy(self,target_sum:int=1e4)->None:
        print("......normalize the single data")
        sc.pp.normalize_total(self.single_data, target_sum=target_sum)
        print("......log1p the single data")
        sc.pp.log1p(self.single_data)
        return None
    def prepare_input(self,):
        print("......prepare the input of bulk2single")
        self.input_data=bulk2single_data_prepare(self.bulk_seq_group,
                                                 self.single_data,
                                                 self.celltype_key)
    def train(self,
            vae_save_dir:str='save_model',
            vae_save_name:str='vae',
            generate_save_dir:str='output',
            generate_save_name:str='output',
            batch_size:int=512,
            learning_rate:int=1e-4,
            hidden_size:int=256,
            epoch_num:int=5000,
            patience:int=50,save:bool=True)->torch.nn.Module:
        if self.input_data==None:
            self.prepare_input()
        single_cell, label, breed_2_list, index_2_gene, cell_number_target_num, \
        nclass, ntrain, feature_size = self.__get_model_input(self.input_data, self.cell_target_num)
        print('...begin vae training')
        vae_net,history = train_vae(single_cell,
                            label,
                            self.used_device,
                            batch_size,
                            feature_size=feature_size,
                            epoch_num=epoch_num,
                            learning_rate=learning_rate,
                            hidden_size=hidden_size,
                            patience=patience,)
        print('...vae training done!')
        if save:
            path_save = os.path.join(vae_save_dir, f"{vae_save_name}.pth")
            if not os.path.exists(vae_save_dir):
                os.makedirs(vae_save_dir)
            torch.save(vae_net.state_dict(), path_save)
            print(f"...save trained vae in {path_save}.")
            import pickle
            #save cell_target_num
            with open(os.path.join(vae_save_dir, f"{vae_save_name}_cell_target_num.pkl"), 'wb') as f:
                pickle.dump(self.cell_target_num, f)
        self.vae_net=vae_net
        self.history=history
        return vae_net
        print('generating....')
        generate_sc_meta, generate_sc_data = generate_vae(vae_net, -1,
                                                          single_cell, label, breed_2_list,
                                                          index_2_gene, cell_number_target_num, used_device)
        sc_g=anndata.AnnData(generate_sc_data.T)
        sc_g.obs[self.celltype_key] = generate_sc_meta.loc[sc_g.obs.index,self.celltype_key].values
        sc_g.write_h5ad(os.path.join(generate_save_dir, f"{generate_save_name}.h5ad"), compression='gzip')
        self.__save_generation(generate_sc_meta, generate_sc_data,
                               generate_save_dir, generate_save_name)
        return sc_g
    def save(self,vae_save_dir:str='save_model',
            vae_save_name:str='vae',):
        path_save = os.path.join(vae_save_dir, f"{vae_save_name}.pth")
        if not os.path.exists(vae_save_dir):
            os.makedirs(vae_save_dir)
        torch.save(self.vae_net.state_dict(), path_save)
        import pickle
        #save cell_target_num
        with open(os.path.join(vae_save_dir, f"{vae_save_name}_cell_target_num.pkl"), 'wb') as f:
            pickle.dump(self.cell_target_num, f)
        print(f"...save trained vae in {path_save}.")
    def generate(self)->anndata.AnnData:
        single_cell, label, breed_2_list, index_2_gene, cell_number_target_num, \
        nclass, ntrain, feature_size = self.__get_model_input(self.input_data, self.cell_target_num)
        print('...generating')
        generate_sc_meta, generate_sc_data = generate_vae(self.vae_net, -1,
                                                          single_cell, label, breed_2_list,
                                                          index_2_gene, cell_number_target_num, self.used_device)
        generate_sc_meta.set_index('Cell',inplace=True)
        sc_g=anndata.AnnData(generate_sc_data.T)
        sc_g.obs[self.celltype_key] = generate_sc_meta.loc[sc_g.obs.index,'Cell_type'].values
        return sc_g
    def load_fraction(self,fraction_path:str):
        #load cell_target_num
        import pickle
        with open(os.path.join(fraction_path), 'rb') as f:
            self.cell_target_num = pickle.load(f)
    def load(self,vae_load_dir:str,hidden_size:int=256):
        single_cell, label, breed_2_list, index_2_gene, cell_number_target_num, \
        nclass, ntrain, feature_size = self.__get_model_input(self.input_data, self.cell_target_num)
        print(f'loading model from {vae_load_dir}')
        vae_net = load_vae(feature_size, hidden_size, vae_load_dir, self.used_device)
        self.vae_net=vae_net
    def load_and_generate(self,
                              vae_load_dir:str,  # load_dir
                              hidden_size:int=256)->anndata.AnnData:
        single_cell, label, breed_2_list, index_2_gene, cell_number_target_num, \
        nclass, ntrain, feature_size = self.__get_model_input(self.input_data, self.cell_target_num)
        print(f'loading model from {vae_load_dir}')
        vae_net = load_vae(feature_size, hidden_size, vae_load_dir, self.used_device)
        print('...generating')
        generate_sc_meta, generate_sc_data = generate_vae(vae_net, -1,
                                                          single_cell, label, breed_2_list,
                                                          index_2_gene, cell_number_target_num, self.used_device)
        generate_sc_meta.set_index('Cell',inplace=True)
        #return generate_sc_meta, generate_sc_data
        sc_g=anndata.AnnData(generate_sc_data.T)
        sc_g.obs[self.celltype_key] = generate_sc_meta.loc[sc_g.obs.index,'Cell_type'].values
        print('...generating done!')
        return sc_g
    def filtered(self,generate_adata,highly_variable_genes:bool=True,max_value:float=10,
                     n_comps:int=100,svd_solver:str='auto',leiden_size:int=50):
        generate_adata.raw = generate_adata
        if highly_variable_genes:
            sc.pp.highly_variable_genes(generate_adata, min_mean=0.0125, max_mean=3, min_disp=0.5)
            generate_adata = generate_adata[:, generate_adata.var.highly_variable]
        sc.pp.scale(generate_adata, max_value=max_value)
        sc.tl.pca(generate_adata, n_comps=n_comps, svd_solver=svd_solver)
        sc.pp.neighbors(generate_adata, use_rep="X_pca")
        sc.tl.leiden(generate_adata)
        filter_leiden=list(generate_adata.obs['leiden'].value_counts()[generate_adata.obs['leiden'].value_counts()<leiden_size].index)
        print("The filter leiden is ",filter_leiden)
        generate_adata=generate_adata[~generate_adata.obs['leiden'].isin(filter_leiden)]
        self.generate_adata=generate_adata.copy()
        return generate_adata
    def plot_loss(self,figsize:tuple=(4,4))->Tuple[matplotlib.figure.Figure,matplotlib.axes._axes.Axes]:
        fig, ax = plt.subplots(figsize=figsize)
        ax.plot(range(len(self.history)),self.history)
        ax.set_title('Beta-VAE')
        ax.set_ylabel('Loss')
        ax.set_xlabel('Epochs')
        return fig,ax
    def __get_model_input(self, data, cell_target_num):
        # input：data， celltype， bulk & output: label, dic, single_cell
        single_cell = data["input_sc_data"].values.T  # single cell data (600 * 6588)
        index_2_gene = (data["input_sc_data"].index).tolist()
        breed = data["input_sc_meta"]['Cell_type']
        breed_np = breed.values
        breed_set = set(breed_np)
        breed_2_list = list(breed_set)
        dic = {}  # breed_set to index {'B cell': 0, 'Monocyte': 1, 'Dendritic cell': 2, 'T cell': 3}
        label = []  # the label of cell (with index correspond)
        nclass = len(breed_set)
        ntrain = single_cell.shape[0]
        # FeaSize = single_cell.shape[1]
        feature_size = single_cell.shape[1]
        assert nclass == len(cell_target_num.keys()), "cell type num no match!!!"
        for i in range(len(breed_set)):
            dic[breed_2_list[i]] = i
        cell = data["input_sc_meta"]["Cell"].values
        for i in range(cell.shape[0]):
            label.append(dic[breed_np[i]])
        label = np.array(label)
        # label index the data size of corresponding target
        cell_number_target_num = {}
        for k, v in cell_target_num.items():
            cell_number_target_num[dic[k]] = v
        return single_cell, label, breed_2_list, index_2_gene, cell_number_target_num, nclass, ntrain, feature_size
    def __save_generation(self, generate_sc_meta, generate_sc_data, generate_save_dir,
                          generate_save_name, ):
        # saving.....
        if not os.path.exists(generate_save_dir):
            os.makedirs(generate_save_dir)
        path_label_generate_csv = os.path.join(generate_save_dir, f"{generate_save_name}_sc_celltype.csv")
        path_cell_generate_csv = os.path.join(generate_save_dir, f"{generate_save_name}_sc_data.csv")
        generate_sc_meta.to_csv(path_label_generate_csv)
        generate_sc_data.to_csv(path_cell_generate_csv)
        print(f"saving to {path_label_generate_csv} and {path_cell_generate_csv}.")
def Trans_corr_matrix(data,
                    method='pearson',
                    cmap='seismic'):
    data_len=len(data)
    data_index=data.index
    #correlation coefficient
    start = datetime.datetime.now()
    print('...correlation coefficient matrix is being calculated')
    result=data.T.corr(method)
    end = datetime.datetime.now()
    result.to_csv('direction correlation matrix.csv')
    print("...direction correlation have been saved")
    print("...calculate time",end-start)
    #indirect correlation add
    start = datetime.datetime.now()
    print('...indirect correlation matrix is being calculated')
    np.fill_diagonal(result.values, 0)
    arr=abs(result)
    arr_len=len(arr)
    temp=np.zeros(arr_len)
    for i in range(1,3):    
        temp=temp+(arr**i)/i
    end = datetime.datetime.now()
    temp.to_csv('indirection correlation matrix.csv')
    print("...indirection correlation have been saved")
    print("...calculate time",end-start)
    #cal soft_threshold
    plt.rc('font', family='Arial')
    plt.rcParams["font.weight"] = "bold"
    plt.rcParams["font.weight"] = "12"
    plt.rcParams["axes.labelweight"] = "bold"
    my_dpi=300
    fig=plt.figure(figsize=(2000/my_dpi, 1000/my_dpi), dpi=my_dpi)
    start = datetime.datetime.now()
    print('...soft_threshold is being calculated')
    soft=6
    re1=pd.DataFrame(columns=['beta','r2','meank'])
    for j in range(1,12):
        result_i=np.float_power(temp,j)
        tt_0=np.sum(abs(result_i),axis=0)-1
        n=plt.hist(x = tt_0), #
        x=n[0][0]
        y=[]
        for i in range(len(n[0][1])-1):
            y.append((n[0][1][i]+n[0][1][i+1])/2)
        x=np.log10(x)
        y=np.log10(y)
        res=stats.linregress(x, y)
        r2=np.float_power(res.rvalue,2)
        k=tt_0.mean()
        re1=re1.append({'beta':j,'r2':r2,'meank':k},ignore_index=True)
    for i in re1['r2']:
        if i>0.85:
            soft=re1[re1['r2']==i]['beta'].iloc[0]
            break
    print('...appropriate soft_thresholds:',soft)
    plt.savefig('soft_threshold_hist.png',dpi=300)
    #select soft_threhold
    my_dpi=300
    fig=plt.figure(figsize=(2000/my_dpi, 1000/my_dpi), dpi=my_dpi)
    grid = plt.GridSpec(1, 4, wspace=1, hspace=0.1)
    #fig, (ax0, ax1) = plt.subplots(2, 1)
    plt.subplot(grid[0,0:2])
    cmap1=sns.color_palette(cmap)
    plt.subplot(1,2, 1)
    p1=sns.regplot(x=re1["beta"], y=re1['r2'], fit_reg=False, marker="o", color=cmap1[0])
    p1.axhline(y=0.9,ls=":",c=cmap1[5])
    plt.subplot(grid[0,2:4])
    p1=sns.regplot(x=re1["beta"], y=re1['meank'], fit_reg=False, marker="o", color=cmap1[4])
    plt.savefig('soft_threshold_select.png',dpi=300)
    test=np.float_power(temp,soft)
    np.fill_diagonal(test.values, 1.0)
    return test
def Select_module(data,
                  linkage_method='ward',
                  minClusterSize=30,
                  deepSplit=2,
                  cmap='seismic',
                  trans=True,
                 ):
    #distance
    print("...distance have being calculated")
    if trans==True:
       distances = pdist(1-data, "euclidean")
    else:
        distances = pdist(data, "euclidean")
    #geneTree
    print("...geneTree have being calculated")
    geneTree=linkage(distances, linkage_method)
    #dynamicMods
    print("...dynamicMods have being calculated")
    dynamicMods=dynamicTree.cutreeHybrid(geneTree,distM=distances,minClusterSize = minClusterSize,deepSplit = deepSplit, pamRespectsDendro = False)
    #return dynamicMods
    print("...total:",len(set(dynamicMods['labels'])))
    plt.rc('font', family='Arial')
    plt.rcParams["font.weight"] = "bold"
    plt.rcParams["font.weight"] = "12"
    plt.rcParams["axes.labelweight"] = "bold"
    plt.figure(figsize=(25, 10))
    grid = plt.GridSpec(3, 1, wspace=0.5, hspace=0.1)
    #fig, (ax0, ax1) = plt.subplots(2, 1)
    plt.subplot(grid[0:2,0])
    hierarchy.set_link_color_palette(['#000000'])
    dn=hierarchy.dendrogram(geneTree,color_threshold=0, above_threshold_color='black')
    plt.tick_params( \
        axis='x',
        which='both',
        bottom='off',
        top='off',
        labelbottom='off')
    #mol
    x=dn['ivl']
    y=[dynamicMods['labels'][int(x)] for x in dn['ivl']]
    yy=np.array([y])
    z=[data.index[int(x)] for x in dn['ivl']]
    mol=pd.DataFrame(columns=['ivl','module','name'])
    mol['ivl']=x
    mol['module']=y
    mol['name']=z
    plt.subplot(grid[2,0])
    ax1=plt.pcolor(yy,cmap=cmap)
    plt.savefig('module_tree.png',dpi=300)
    return mol
def Analysis_cocharacter(data,
                        character,
                        module):
    print("...PCA analysis have being done")
    pcamol=pd.DataFrame(columns=data.columns)
    set_index=set(module['module'])
    for j in set_index:
        newdata=pd.DataFrame(columns=data.columns)
        for i in list(module[module['module']==j].dropna()['name']):
            newdata=newdata.append(data[data.index==i])
        from sklearn.decomposition import PCA
        pca = PCA(n_components=1) 
        reduced_X = pca.fit_transform(newdata.T)
        tepcamol=pd.DataFrame(reduced_X.T,columns=data.columns)
        pcamol=pcamol.append(tepcamol,ignore_index=True)
    pcamol.index=set_index
    print("...co-analysis have being done")
    from scipy.stats import spearmanr,pearsonr,kendalltau
    # seed random number generator
    # calculate spearman's correlation
    result_1=pd.DataFrame(columns=character.columns)
    result_p=pd.DataFrame(columns=character.columns)
    for j in character.columns:
        co=[]
        pvv=[]
        for i in range(len(pcamol)):   
            tempcor=pd.DataFrame(columns=['x','y'])
            tempcor['x']=list(character[j])
            tempcor['y']=list(pcamol.iloc[i])
            tempcor=tempcor.dropna()
            coef,pv=pearsonr(tempcor['x'],tempcor['y'])
            co.append(coef)
            pvv.append(pv)
        result_1[j]=co
        result_p[j]=pvv
            #print(coef)
    result_1=abs(result_1)
    result_1.index=set_index
    plt.rc('font', family='Arial')
    plt.rcParams["font.weight"] = "bold"
    plt.rcParams["font.weight"] = "12"
    plt.rcParams["axes.labelweight"] = "bold"
    plt.figure(figsize=(10,10))
    sns.heatmap(result_1,vmin=0, vmax=1,center=1,annot=True,square=True)
    plt.savefig('co_character.png',dpi=300)
    return result_1
def table(vector):
    factors = np.unique(vector)
    results = pd.Series(np.zeros(len(factors), dtype=int))
    results.index = factors
    for i, k in enumerate(factors):
        results.iloc[i] = np.sum(vector == k)
    return(results)
def plot_kegg_chart(workdir, kegg_result, width=1280, height=720, p_adjust=0.05, font_size=15, chart_num=30, chart_size=30, color='rdbu_r',pic_type='bubble', funciton_type='All'):
    # 读取数据
    kegg_result = pd.read_csv(kegg_result, sep='\t')
    # 数据预处理
    kegg_result = kegg_result.copy()
    kegg_result = kegg_result[['ID', 'Description', 'GeneRatio', 'p.adjust', 'Count']]
    kegg_result.columns = ["ID", "Pathway", "GeneRatio","P.adjust", 'Count']
    kegg_result["GeneRatio"] = kegg_result["GeneRatio"].apply(lambda x: round(eval(x), 3))  # GeneRatio列输出处理为浮点数
    kegg_result['P.adjust'] = kegg_result['P.adjust'].apply(lambda x: round(x, 6))  # 控制P.adjust列的小数位数
    # 数据筛选
    kegg_result = kegg_result[kegg_result['P.adjust'] < p_adjust]  # 过滤P.adjust值
    kegg_result = kegg_result.sort_values(by='Count', ascending=False)  # 按照Count列降序排列
    kegg_result = kegg_result.iloc[:chart_num]  # 取前chart_num个数据
    if funciton_type == 'All':
        pass
    # 图表公共布局设置
    layout_args = {
        'title': "KEGG Enrichment Analysis",
        'yaxis_title': "Pathway",
        'yaxis': dict(autorange="reversed"),
        'font': dict(family="Arial", size=font_size),
        'template': "plotly_white",
        'width': width,
        'height': height
    }
    # 颜色轴设置
    color_axis_args = {
        'colorbar_title': "P.adjust",
        'colorbar_tickformat': ".3f",
        'colorbar': dict(dtick=0.005)
    }
    # 根据pic_type绘制不同类型的图表
    if pic_type == 'bubble':
        fig = px.scatter(
            kegg_result,
            x='GeneRatio',
            y='Pathway',
            size='Count',
            color='P.adjust',
            color_continuous_scale=color,
            opacity=0.85,
            hover_data=["ID",'P.adjust', 'Count'],
            size_max=chart_size
        )
    elif pic_type == 'bar':
        fig = px.bar(
            kegg_result,
            x='Count',
            y='Pathway',
            color='P.adjust',
            color_continuous_scale=color,
            opacity=0.85,
            hover_data=['ID','P.adjust', 'Count']
        )
    # 应用颜色轴设置
    fig.update_layout(**layout_args)
    fig.update_coloraxes(**color_axis_args)
    # 保存为png，scale设置为4
    output_image_path = os.path.join(workdir, "output_file", "kegg.png")
    fig.write_image(output_image_path, scale=4)
    # 保存为html
    output_html_path = os.path.join(workdir, "output_file", "kegg.html")
    fig.write_html(output_html_path)
    # 测试用
    return output_image_path, output_html_path
def enrichment_KEGG(gene_list,
                    gene_sets=['KEGG_2019_Human'],
                    organism='Human',
                    description='test_name',
                    outdir='enrichment_kegg',
                    cutoff=0.5):
    enr = gp.enrichr(gene_list=gene_list,
                 gene_sets=gene_sets,
                 organism=organism, # don't forget to set organism to the one you desired! e.g. Yeast
                 description=description,
                 outdir=outdir,
                 # no_plot=True,
                 cutoff=cutoff # test dataset, use lower value from range(0,1)
                )
    subp=dotplot(enr.res2d, title=description,cmap='seismic')
    print(subp)
    return enr.res2d
def enrichment_GO(gene_list,
                    go_mode='Bio',
                    organism='Human',
                    description='test_name',
                    outdir='enrichment_go',
                    cutoff=0.5):
    if(go_mode=='Bio'):
        geneset='GO_Biological_Process_2018'
    if(go_mode=='Cell'):
        geneset='GO_Cellular_Component_2018'
    if(go_mode=='Mole'):
        geneset='GO_Molecular_Function_2018'
    enr = gp.enrichr(gene_list=gene_list,
                 gene_sets=geneset,
                 organism=organism, # don't forget to set organism to the one you desired! e.g. Yeast
                 description=description,
                 outdir=outdir,
                 # no_plot=True,
                 cutoff=cutoff # test dataset, use lower value from range(0,1)
                )
    subp=dotplot(enr.res2d, title=description,cmap='seismic')
    print(subp)
    return enr.res2d
def enrichment_GSEA(data,
                   gene_sets='KEGG_2016',
                   processes=4,
                   permutation_num=100,
                   outdir='prerank_report_kegg',
                   seed=6):
    rnk=pd.DataFrame(columns=['genename','FoldChange'])
    rnk['genename']=data.index
    rnk['FoldChange']=data['FoldChange'].tolist()
    rnk1=rnk.drop_duplicates(['genename'])
    rnk1=rnk1.sort_values(by='FoldChange', ascending=False)
    pre_res = gp.prerank(rnk=rnk1, gene_sets=gene_sets,
                     processes=processes,
                     permutation_num=permutation_num, # reduce number to speed up testing
                     outdir=outdir, format='png', seed=seed)
    pre_res.res2d.sort_index().to_csv('GSEA_result.csv')
    return pre_res
def Plot_GSEA(data,num=0):
    terms = data.res2d.index
    from gseapy.plot import gseaplot
    # to save your figure, make sure that ofname is not None
    gseaplot(rank_metric=data.ranking, term=terms[num], **data.results[terms[num]])
def geneset_enrichment(gene_list:list,pathways_dict:dict,
                       pvalue_threshold:float=0.05,pvalue_type:str='auto',
                       organism:str='Human',description:str='None',
                       background:list=None,
                       outdir:str='./enrichr',cutoff:float=0.5)->pd.DataFrame:
    import gseapy as gp
    if background is None:
        if (organism == 'Mouse') or (organism == 'mouse') or (organism == 'mm'):
            background='mmusculus_gene_ensembl'
        elif (organism == 'Human') or (organism == 'human') or (organism == 'hs'):
            background='hsapiens_gene_ensembl'
    enr = gp.enrichr(gene_list=gene_list,
                 gene_sets=pathways_dict,
                 organism=organism, # don't forget to set organism to the one you desired! e.g. Yeast
                 description=description,
                 background=background,
                 outdir=outdir,
                 cutoff=cutoff # test dataset, use lower value from range(0,1)
                )
    if pvalue_type=='auto':
        if enr.res2d.shape[0]>100:
            enrich_res=enr.res2d[enr.res2d['Adjusted P-value']<pvalue_threshold]
            enrich_res['logp']=-np.log(enrich_res['Adjusted P-value'])
        else:
            enrich_res=enr.res2d[enr.res2d['P-value']<pvalue_threshold]
            enrich_res['logp']=-np.log(enrich_res['P-value'])
    elif pvalue_type=='adjust':
        enrich_res=enr.res2d[enr.res2d['Adjusted P-value']<pvalue_threshold]
        enrich_res['logp']=-np.log(enrich_res['Adjusted P-value'])
    else:
        enrich_res=enr.res2d[enr.res2d['P-value']<pvalue_threshold]
        enrich_res['logp']=-np.log(enrich_res['P-value'])
    enrich_res['logc']=np.log(enrich_res['Odds Ratio'])
    enrich_res['num']=[int(i.split('/')[0]) for i in enrich_res['Overlap']]
    enrich_res['fraction']=[int(i.split('/')[0])/int(i.split('/')[1]) for i in enrich_res['Overlap']]
    return enrich_res
def string_interaction(gene:list,species:int) -> pd.DataFrame:
    import requests ## python -m pip install requests
    string_api_url = "https://string-db.org/api"
    output_format = "tsv-no-header"
    method = "network"
    request_url = "/".join([string_api_url, output_format, method])
    my_genes = gene
    params = {
        "identifiers" : "%0d".join(my_genes), # your protein
        "species" : species, # species NCBI identifier 
        "caller_identity" : "www.awesome_app.org" # your app name
    }
    response = requests.post(request_url, data=params)
    res=pd.DataFrame()
    res['stringId_A']=[j.strip().split("\t")[0] for j in response.text.strip().split("\n")]
    res['stringId_B']=[j.strip().split("\t")[1] for j in response.text.strip().split("\n")]
    res['preferredName_A']=[j.strip().split("\t")[2] for j in response.text.strip().split("\n")]
    res['preferredName_B']=[j.strip().split("\t")[3] for j in response.text.strip().split("\n")]
    res['ncbiTaxonId']=[j.strip().split("\t")[4] for j in response.text.strip().split("\n")]
    res['score']=[j.strip().split("\t")[5] for j in response.text.strip().split("\n")]
    res['nscore']=[j.strip().split("\t")[6] for j in response.text.strip().split("\n")]
    res['fscore']=[j.strip().split("\t")[7] for j in response.text.strip().split("\n")]
    res['pscore']=[j.strip().split("\t")[8] for j in response.text.strip().split("\n")]
    res['ascore']=[j.strip().split("\t")[9] for j in response.text.strip().split("\n")]
    res['escore']=[j.strip().split("\t")[10] for j in response.text.strip().split("\n")]
    res['dscore']=[j.strip().split("\t")[11] for j in response.text.strip().split("\n")]
    res['tscore']=[j.strip().split("\t")[12] for j in response.text.strip().split("\n")]
    return res
def string_map(gene:list,species:int)->pd.DataFrame:
    import requests ## python -m pip install requests
    string_api_url = "https://string-db.org/api"
    output_format = "tsv-no-header"
    method = "get_string_ids"
    params = {
        "identifiers" : "\r".join(gene), # your protein list
        "species" : species, # species NCBI identifier 
        "limit" : 1, # only one (best) identifier per input protein
        "echo_query" : 1, # see your input identifiers in the output
        "caller_identity" : "www.awesome_app.org" # your app name
    }
    request_url = "/".join([string_api_url, output_format, method])
    response = requests.post(request_url, data=params)
    res=pd.DataFrame(columns=['queryItem','queryIndex','stringId','ncbiTaxonId','taxonName','preferredName','annotation'])
    res['queryItem']=[j.strip().split("\t")[0] for j in response.text.strip().split("\n")]
    res['queryIndex']=[j.strip().split("\t")[1] for j in response.text.strip().split("\n")]
    res['stringId']=[j.strip().split("\t")[2] for j in response.text.strip().split("\n")]
    res['ncbiTaxonId']=[j.strip().split("\t")[3] for j in response.text.strip().split("\n")]
    res['taxonName']=[j.strip().split("\t")[4] for j in response.text.strip().split("\n")]
    res['preferredName']=[j.strip().split("\t")[5] for j in response.text.strip().split("\n")]
    res['annotation']=[j.strip().split("\t")[6] for j in response.text.strip().split("\n")]
    return res
def max_interaction(gene,species):
    gene_len=len(gene)
    times=gene_len//1000
    shengyu=gene_len-times*1000
    if shengyu!=0:
        times+=1
    ge=[]
    for i in range(times):
        ge.append(gene[1000*i:1000*(i+1)])
    b=[]
    for p in itertools.combinations(range(times),2):
        b.append(string_interaction(ge[p[0]]+ge[p[1]],species))
    res=pd.concat(b,axis=0,ignore_index=True)
    res=res.drop_duplicates()
    return res
def generate_G(gene:list,species:int,score:float=0.4) -> nx.Graph:
    a=string_interaction(gene,species)
    b=a.drop_duplicates()
    b.head()
    G = nx.Graph()
    G.add_nodes_from(set(b['preferredName_A'].tolist()+b['preferredName_B'].tolist()))
    #Connect nodes
    for i in b.index:
        col_label = b.loc[i]['preferredName_A']
        row_label = b.loc[i]['preferredName_B']
        if(float(b.loc[i]['score'])>score):
            G.add_edge(col_label,row_label)
    return G
import scanpy as sc
import os
import anndata
import numpy as np
from lifelines import KaplanMeierFitter
from lifelines.statistics import logrank_test
import pandas as pd
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple, Optional, Union
class pyTCGA(object):
    def __init__(self,gdc_sample_sheep:str,gdc_download_files:str,clinical_cart:str):
        self.gdc_sample_sheep=gdc_sample_sheep
        self.gdc_download_files=gdc_download_files
        self.clinical_cart=clinical_cart
        exist_files=[i for i in os.listdir(gdc_download_files) if 'txt' not in i]
        self.sample_sheet=pd.read_csv(self.gdc_sample_sheep,sep='\t',index_col=0)
        exist_files=list(set(exist_files) & set(self.sample_sheet.index))
        self.sample_sheet=self.sample_sheet.loc[exist_files]
        self.clinical_sheet=pd.read_csv('{}/clinical.tsv'.format(self.clinical_cart),sep='\t',index_col=0)
        #self.clinical_sheet=self.clinical_sheet.loc[exist_files]
        sample_index=self.sample_sheet.index[0]
        sample_id=self.sample_sheet.loc[sample_index,'Sample ID']
        sample_file_id=sample_index
        sample_file_name=self.sample_sheet.loc[sample_index,'File Name']
        self.data_test=pd.read_csv('{}/{}/{}'.format(self.gdc_download_files,sample_file_id,sample_file_name),
                             sep='\t',index_col=0,skiprows=1)
        print('tcga module init success')
    def adata_read(self,path:str):
        print('... anndata reading')
        self.adata=sc.read(path)
    def adata_init(self):
        self.index_init()
        self.expression_init()
        self.matrix_construct()
    def adata_meta_init(self,var_names:list=['gene_name','gene_type'],
                  obs_names:list=['Case ID','Sample Type'])->anndata.AnnData:
        print('...anndata meta init',var_names,obs_names)
        adata=self.adata
        #var_pd=pd.DataFrame(index=self.adata.var.index)
        var_pd=self.data_test.loc[adata.var.index,var_names]
        var_pd['gene_id']=var_pd.index.tolist()
        var_pd.index=var_pd['gene_name'].values
        #obs_pd=pd.DataFrame(index=data_pd_count.columns)
        sample_sheet_tmp=self.sample_sheet.copy()
        sample_sheet_tmp.index=sample_sheet_tmp['Sample ID']
        obs_pd=sample_sheet_tmp.loc[adata.obs.index,obs_names]
        obs_pd=obs_pd[~obs_pd.index.duplicated(keep='first')]
        adata.obs=obs_pd.loc[adata.obs.index]
        adata.var=var_pd
        adata.var.index=adata.var['gene_name'].astype('str').values
        adata.var_names_make_unique()
        self.adata=adata
        return adata
    def survial_init(self):
        day_li=[]
        pd_c=self.clinical_sheet
        for i in pd_c.index:
            if pd_c.loc[i,'vital_status'].iloc[0]=='Alive':
                day_li.append(pd_c.loc[i,'days_to_last_follow_up'].iloc[0])
            elif pd_c.loc[i,'vital_status'].iloc[0]=='Dead':
                day_li.append(pd_c.loc[i,'days_to_death'].iloc[0])
            else:
                day_li.append(pd_c.loc[i,'days_to_last_follow_up'].iloc[0])
        pd_c['days']=day_li
        s_pd=pd_c[["case_submitter_id",
              "vital_status",
              "days_to_last_follow_up",
                "days_to_death",
                "age_at_index",
                "tumor_grade","days"]].copy()
        s_pd=s_pd.drop_duplicates(subset='case_submitter_id')
        s_pd.set_index(s_pd.columns[0],inplace=True)
        self.s_pd=s_pd
        self.adata.obs['vital_status']='Not Reported'
        self.adata.obs['days']=np.nan
        for i in self.adata.obs.index:
            if self.adata.obs.loc[i,'Case ID'] not in s_pd.index:
                self.adata=self.adata[self.adata.obs.index!=i]
                continue
            self.adata.obs.loc[i,'vital_status']=s_pd.loc[self.adata.obs.loc[i,'Case ID'],'vital_status']
            self.adata.obs.loc[i,'days']=s_pd.loc[self.adata.obs.loc[i,'Case ID'],'days']
    def index_init(self)->list:
        print('...index init')
        all_lncRNA_index=[]
        for sample_index in self.sample_sheet.index:
            sample_id=self.sample_sheet.loc[sample_index,'Sample ID']
            sample_file_id=sample_index
            sample_file_name=self.sample_sheet.loc[sample_index,'File Name']
            data_test=pd.read_csv('{}/{}/{}'.format(self.gdc_download_files,sample_file_id,sample_file_name),
                             sep='\t',index_col=0,skiprows=1)
            #data_test=data_test.loc[data_test['gene_type']=='lncRNA']
            data_c_s=data_test['tpm_unstranded'].sort_values(ascending=False)
            data_c_s=data_c_s[~data_c_s.index.duplicated(keep='first')]
            all_lncRNA_index=list(set(all_lncRNA_index) | set(data_c_s.index.tolist()))
        self.tcga_index=all_lncRNA_index
        return all_lncRNA_index
    def expression_init(self):
        print('... expression matrix init')
        data_pd_count=pd.DataFrame(index=self.tcga_index)
        data_pd_tpm=pd.DataFrame(index=self.tcga_index)
        data_pd_fpkm=pd.DataFrame(index=self.tcga_index)
        for sample_index in self.sample_sheet.index:
            sample_id=self.sample_sheet.loc[sample_index,'Sample ID']
            sample_file_id=sample_index
            sample_file_name=self.sample_sheet.loc[sample_index,'File Name']
            #print(sample_id)
            data_test=pd.read_csv('{}/{}/{}'.format(self.gdc_download_files,sample_file_id,sample_file_name),
                             sep='\t',index_col=0,skiprows=1)
            #data_test=data_test.loc[data_test['gene_type']=='lncRNA']
            data_c_s=data_test['unstranded'].sort_values(ascending=False)
            data_c_s=data_c_s[~data_c_s.index.duplicated(keep='first')]
            data_pd_count[sample_id]=0
            data_pd_count.loc[data_c_s.index,sample_id]=data_c_s.values
            data_c_s=data_test['tpm_unstranded'].sort_values(ascending=False)
            data_c_s=data_c_s[~data_c_s.index.duplicated(keep='first')]
            data_pd_tpm[sample_id]=0
            data_pd_tpm.loc[data_c_s.index,sample_id]=data_c_s.values
            data_c_s=data_test['fpkm_unstranded'].sort_values(ascending=False)
            data_c_s=data_c_s[~data_c_s.index.duplicated(keep='first')]
            data_pd_fpkm[sample_id]=0
            data_pd_fpkm.loc[data_c_s.index,sample_id]=data_c_s.values
        self.data_pd_count=data_pd_count
        self.data_pd_tpm=data_pd_tpm
        self.data_pd_fpkm=data_pd_fpkm
        self.data_test=data_test
    def matrix_construct(self):
        print('...anndata construct')
        var_pd=pd.DataFrame(index=self.data_pd_count.index)
        obs_pd=pd.DataFrame(index=self.data_pd_count.columns)
        adata=anndata.AnnData(self.data_pd_count.T,var=var_pd,obs=obs_pd)
        adata.layers['tpm']=self.data_pd_tpm.T.values
        adata.layers['fpkm']=self.data_pd_fpkm.T.values
        adata.layers['deseq_normalize']=self.matrix_normalize(self.data_pd_count).T.values
        self.adata=adata
        return adata
    def matrix_normalize(self,data:pd.DataFrame)->pd.DataFrame:
        avg1=data.apply(np.log,axis=1).mean(axis=1).replace([np.inf, -np.inf], np.nan).dropna()
        data1=data.loc[avg1.index]
        data_log=data1.apply(np.log,axis=1)
        scale=data_log.sub(avg1.values,axis=0).median(axis=0).apply(np.exp)
        return data/scale
    def survival_analysis(self,gene:str,layer:str='raw',plot:bool=False,gene_threshold:str='median')->Tuple[float,float]:
        goal_gene=gene
        s_pd=self.s_pd
        s_pd=s_pd.loc[self.adata.obs['Case ID']]
        if layer!='raw':
            if layer not in self.adata.layers.keys():
                s_pd[goal_gene]=self.adata[self.adata.obs.index,self.adata.var['gene_name']==goal_gene].X.mean(axis=1).toarray()
            else:
                s_pd[goal_gene]=self.adata[self.adata.obs.index,self.adata.var['gene_name']==goal_gene].layers[layer].mean(axis=1).toarray()
        else:
            s_pd[goal_gene]=self.adata[self.adata.obs.index,self.adata.var['gene_name']==goal_gene].X.mean(axis=1).toarray()
        if gene_threshold=='median':
            s_pd['{}_status'.format(goal_gene)]=['High' if i>s_pd[goal_gene].median() else 'Low' for i in s_pd[goal_gene] ]
        elif gene_threshold=='mean':
            s_pd['{}_status'.format(goal_gene)]=['High' if i>s_pd[goal_gene].mean() else 'Low' for i in s_pd[goal_gene] ]
        else:
            s_pd['{}_status'.format(goal_gene)]=['High' if i>gene_threshold else 'Low' for i in s_pd[goal_gene] ]
        s_pd=s_pd.loc[s_pd['days']!="'--"]
        s_pd['fustat'] = [0 if 'Alive'==i else 1 for i in s_pd['vital_status']]
        s_pd['gene_fustat'] = [0 if 'High'==i else 1 for i in s_pd['{}_status'.format(goal_gene)]]
        km = KaplanMeierFitter()
        T = s_pd['days'].astype(float) / 365
        E=s_pd['fustat']
        gender = (s_pd['{}_status'.format(goal_gene)] == 'High')
        lr = logrank_test(T[gender], T[~gender], E[gender], E[~gender], alpha=.95)
        if plot==True:
            fig, ax = plt.subplots(figsize=(3,3))
            km.fit(T[gender], event_observed=E[gender], label="High")
            km.plot(ax=ax,color='#941456')
            km.fit(T[~gender], event_observed=E[~gender], label="Low")
            km.plot(ax=ax,color='#368650')
            lr = logrank_test(T[gender], T[~gender], E[gender], E[~gender], alpha=.95)
            lr.p_value
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            ax.spines['bottom'].set_visible(True)
            ax.spines['left'].set_visible(True)
            plt.xlabel('Years')
            plt.ylabel('Pecent Survial')
            plt.title('Survial: {}\np-value: {}'.format(goal_gene,round(lr.p_value,3)))
            plt.grid(False)
        return lr.test_statistic,lr.p_value
    def survial_analysis_all(self):
        res_l_lnc=[]
        res_l_tt=[]
        for i in self.adata.var.index:
            res_l_tt.append(self.survival_analysis(i)[0])
            res_l_lnc.append(self.survival_analysis(i)[1])
        self.adata.var['survial_test_statistic']=res_l_tt
        self.adata.var['survial_p']=res_l_lnc
class pyPPI(object):
    def __init__(self,gene: list,species: int,gene_type_dict: dict,gene_color_dict: dict,
                 score: float = 0.4) -> None:
        self.gene=gene 
        self.species=species
        self.score=score
        self.gene_type_dict=gene_type_dict
        self.gene_color_dict=gene_color_dict
    def interaction_analysis(self) -> nx.Graph:
        G=generate_G(self.gene,
                          self.species,
                          self.score)
        self.G=G 
        return G
    def plot_network(self,**kwargs) -> Tuple[matplotlib.figure.Figure,matplotlib.axes._axes.Axes]:
        return plot_network(self.G,self.gene_type_dict,self.gene_color_dict,**kwargs)
def geneset_plot(enrich_res,num:int=10,node_size:list=[5,10,15],
                        cax_loc:int=2,cax_fontsize:int=12,
                        fig_title:str='',fig_xlabel:str='Fractions of genes',
                        figsize:tuple=(2,4),cmap:str='YlGnBu',
                        text_knock:int=2,text_maxsize:int=20)->matplotlib.axes._axes.Axes:
    fig, ax = plt.subplots(figsize=figsize)
    plot_data2=enrich_res.sort_values('P-value')[:num].sort_values('logc')
    st=ax.scatter(plot_data2['fraction'],range(len(plot_data2['logc'])),
            s=plot_data2['num']*10,linewidths=1,edgecolors='black',c=plot_data2['logp'],cmap=cmap)
    ax.yaxis.tick_right()
    plt.yticks(range(len(plot_data2['fraction'])),[plot_text_set(i.split('(')[0],text_knock=text_knock,text_maxsize=text_maxsize) for i in plot_data2['Term']],
            fontsize=10,)
    plt.xticks(fontsize=12,)
    plt.title(fig_title,fontsize=12)
    plt.xlabel(fig_xlabel,fontsize=12)
    #fig = plt.gcf()
    cax = fig.add_axes([cax_loc, 0.55, 0.5, 0.02])
    cb=fig.colorbar(st,shrink=0.25,cax=cax,orientation='horizontal')
    cb.set_label(r'$−Log_{10}(P_{adjusted})$',fontdict={'size':cax_fontsize})
    gl_li=[]
    for i in node_size:
        gl_li.append(ax.scatter([],[], s=i*10, marker='o', color='white',edgecolors='black'))
    plt.legend(gl_li,
        [str(i) for i in node_size],
        loc='lower left',
        ncol=3,bbox_to_anchor=(-0.45, -13),
        fontsize=cax_fontsize)
    return ax
class pyGSE(object):
    def __init__(self,gene_list:list,pathways_dict:dict,pvalue_threshold:float=0.05,pvalue_type:str='auto',
                 background=None,organism:str='Human',description:str='None',outdir:str='./enrichr',cutoff:float=0.5) -> None:
        self.gene_list=gene_list
        self.pathways_dict=pathways_dict
        self.pvalue_threshold=pvalue_threshold
        self.pvalue_type=pvalue_type
        self.organism=organism
        self.description=description
        self.outdir=outdir
        self.cutoff=cutoff
        if background is None:
            if (organism == 'Mouse') or (organism == 'mouse') or (organism == 'mm'):
                background='mmusculus_gene_ensembl'
            elif (organism == 'Human') or (organism == 'human') or (organism == 'hs'):
                background='hsapiens_gene_ensembl'
            self.background=background
        else:
            self.background=background
    def enrichment(self):
        enrich_res=geneset_enrichment(self.gene_list,self.pathways_dict,self.pvalue_threshold,self.pvalue_type,
                                  self.organism,self.description,self.background,self.outdir,self.cutoff)
        self.enrich_res=enrich_res
        return enrich_res
    def plot_enrichment(self,num:int=10,node_size:list=[5,10,15],
                        cax_loc:int=2,cax_fontsize:int=12,
                        fig_title:str='',fig_xlabel:str='Fractions of genes',
                        figsize:tuple=(2,4),cmap:str='YlGnBu',text_knock:int=2,text_maxsize:int=20)->matplotlib.axes._axes.Axes:
        return geneset_plot(self.enrich_res,num,node_size,cax_loc,cax_fontsize,
                            fig_title,fig_xlabel,figsize,cmap,text_knock,text_maxsize)
class pyGSEA(object):
    def __init__(self,gene_rnk:pd.DataFrame,pathways_dict:dict,
                 processes:int=8,permutation_num:int=100,
                 outdir:str='./enrichr_gsea',cutoff:float=0.5) -> None:
        self.gene_rnk=gene_rnk
        self.pathways_dict=pathways_dict
        self.processes=processes
        self.permutation_num=permutation_num
        self.outdir=outdir
        self.cutoff=cutoff
    def enrichment(self,format:str='png', seed:int=112)->pd.DataFrame:
        """gene set enrichment analysis.
        Arguments:
            format: Matplotlib figure format. Default: 'png'.
            seed: Random seed. Default: 112.
        Returns:
            enrich_res:A pandas.DataFrame object containing the enrichment results.
        """
        pre_res=geneset_enrichment_GSEA(self.gene_rnk,self.pathways_dict,
                                           self.processes,self.permutation_num,
                                           self.outdir,format,seed)
        self.pre_res=pre_res
        enrich_res=pre_res.res2d[pre_res.res2d['fdr']<0.05]
        enrich_res['logp']=-np.log(enrich_res['fdr']+0.0001)
        enrich_res['logc']=enrich_res['nes']
        enrich_res['num']=enrich_res['matched_size']
        enrich_res['fraction']=enrich_res['matched_size']/enrich_res['geneset_size']
        enrich_res['Term']=enrich_res.index.tolist()
        enrich_res['P-value']=enrich_res['fdr']
        self.enrich_res=enrich_res
        return enrich_res
    def plot_gsea(self,term_num:int=0,
                  gene_set_title:str='',
                  figsize:tuple=(3,4),
                  cmap:str='RdBu_r',
                  title_fontsize:int=12,
                  title_y:float=0.95)->matplotlib.figure.Figure:
        """Plot the gene set enrichment result.
        Arguments:
            term_num: The number of enriched terms to plot. Default is 0.
            gene_set_title: The title of the plot. Default is an empty string.
            figsize: The size of the plot. Default is (3,4).
            cmap: The colormap to use for the plot. Default is 'RdBu_r'.
            title_fontsize: The fontsize of the title. Default is 12.
            title_y: The y coordinate of the title. Default is 0.95.
        Returns:
            fig: A matplotlib.figure.Figure object.
        """
        from gseapy.plot import GSEAPlot
        terms = self.enrich_res.index
        g = GSEAPlot(
        rank_metric=self.pre_res.ranking, term=terms[term_num],figsize=figsize,cmap=cmap,
            **self.pre_res.results[terms[term_num]]
            )
        if gene_set_title=='':
            g.fig.suptitle(terms[term_num],fontsize=title_fontsize,y=title_y)
        else:
            g.fig.suptitle(gene_set_title,fontsize=title_fontsize,y=title_y)
        g.add_axes()
        return g.fig
    def plot_enrichment(self,num:int=10,node_size:list=[5,10,15],
                        cax_loc:int=2,cax_fontsize:int=12,
                        fig_title:str='',fig_xlabel:str='Fractions of genes',
                        figsize:tuple=(2,4),cmap:str='YlGnBu',
                        text_knock:int=2,text_maxsize:int=20)->matplotlib.axes._axes.Axes:
        """Plot the gene set enrichment result.
        Arguments:
            num: The number of enriched terms to plot. Default is 10.
            node_size: A list of integers defining the size of nodes in the plot. Default is [5,10,15].
            cax_loc: The location of the colorbar on the plot. Default is 2.
            cax_fontsize: The fontsize of the colorbar label. Default is 12.
            fig_title: The title of the plot. Default is an empty string.
            fig_xlabel: The label of the x-axis. Default is 'Fractions of genes'.
            figsize: The size of the plot. Default is (2,4).
            cmap: The colormap to use for the plot. Default is 'YlGnBu'.
            text_knock: The number of terms to knock out for text labels. Default is 2.
            text_maxsize: The maximum fontsize of text labels. Default is 20.
        Returns:
            ax: A matplotlib.axes.Axes object.
        """
        return geneset_plot(self.enrich_res,num,node_size,cax_loc,cax_fontsize,
                            fig_title,fig_xlabel,figsize,cmap,text_knock,text_maxsize)
def plot_go_chart(workdir, go_relust, width=1280, height=720, p_adjust=0.05, font_size=15, chart_num=30, chart_size=30, pic_type='bubble', color='rdbu_r', funciton_type='All'):
    """根据输入的GO富集分析结果，根据用户选择绘制气泡图或柱状图。
    Args:
        workdir (str): 工作目录。
        go_relust (DataFrame): GO富集分析结果。
        width (int): 图表宽度. 
        height (int): 图表高度. 
        p_adjust (float): P值阈值. 
        chart_num (int): 最多显示富集功能数量. 
        chart_size (int): 图表大小. 
        pic_type (str): 图表类型，可选bubble或bar. 
        color (str): 颜色. Defaults to 'Geyser'.
        funciton_type (str): GO类型，可选BP, CC, MF, All. 
    Returns:
    """
    # 读取数据
    go_relust = pd.read_csv(go_relust, sep='\t')
    # 数据处理
    go_relust = go_relust.copy()
    go_relust = go_relust[["category", "ID", "Description", "Count", 'GeneRatio', "p.adjust"]]
    go_relust.columns = ["Class", "ID", "Description", "Count", "GeneRatio", "P.adjust"]
    # 数据列处理
    go_relust["GeneRatio"] = go_relust["GeneRatio"].apply(lambda x: round(eval(x), 3))
    go_relust['P.adjust'] = go_relust['P.adjust'].apply(lambda x: round(x, 6))
    go_relust = go_relust.sort_values(by='Count', ascending=False)
    go_relust = go_relust[go_relust["P.adjust"] < p_adjust]
    go_relust = go_relust.iloc[:chart_num]
    # 过滤GO类型
    if funciton_type in ["BP", "CC", "MF"]:
        go_relust = go_relust[go_relust["Class"].str.contains(funciton_type)]
    # 图表公共布局设置
    layout_args = {
        'title': "GO Enrichment Analysis",
        'yaxis_title': "Description",
        'yaxis': dict(autorange="reversed"),
        # 'yaxis': dict(autorange="reversed", showgrid=False), # y轴反向，不显示网格线
        # 'xaxis': dict(showgrid=False), # 不显示网格线
        'font': dict(family="Arial", size=font_size),
        'template': "plotly_white", # 主题背景
        'width': width,
        'height': height
    }
    # 颜色轴设置
    color_axis_args = {
        'colorbar_title': "P.adjust",
        'colorbar_tickformat': ".3f",
        'colorbar': dict(dtick=0.005)
    }
    # 根据pic_type绘制不同类型的图表
    if pic_type == "bubble":
        fig = px.scatter(
            go_relust,
            x="GeneRatio",
            y="Description",
            size="Count",
            color="P.adjust",
            color_continuous_scale=color,
            opacity=0.85,
            hover_name="Class",
            hover_data=["ID", "Description", "Count", "GeneRatio", "P.adjust"],
            size_max=chart_size,
        )
    elif pic_type == "bar":
        fig = px.bar(
            go_relust,
            x='Count',
            y='Description',
            color='P.adjust',
            color_continuous_scale=color,
            opacity=0.85,
            hover_data=["Class", "ID", "GeneRatio", "P.adjust"],
        )
    # 应用颜色轴设置
    fig.update_layout(**layout_args)
    fig.update_coloraxes(**color_axis_args)
    # 保存为png，scale设置为4
    output_image_path = os.path.join(workdir, "output_file", "go.png")
    fig.write_image(output_image_path, scale=4)
    # 保存为html
    output_html_path = os.path.join(workdir, "output_file", "go.html")
    fig.write_html(output_html_path)
    return output_image_path, output_html_path
class BulkTrajBlend(object):
    def __init__(self,bulk_seq:pd.DataFrame,single_seq:anndata.AnnData,
                 celltype_key:str,bulk_group=None,
                 top_marker_num:int=500,ratio_num:int=1,gpu:Union[int,str]=0) -> None:
        self.bulk_seq = bulk_seq.copy()
        self.single_seq = single_seq.copy()
        self.celltype_key=celltype_key
        self.top_marker_num=top_marker_num
        self.ratio_num=ratio_num
        self.gpu=gpu
        self.group=bulk_group
        if gpu=='mps' and torch.backends.mps.is_available():
            print('Note that mps may loss will be nan, used it when torch is supported')
            self.used_device = torch.device("mps")
        else:
            self.used_device = torch.device(f"cuda:{gpu}") if gpu >= 0 and torch.cuda.is_available() else torch.device('cpu')
        self.history=[]
        data_dg_v=self.bulk_seq.mean(axis=1)
        data_dg=pd.DataFrame(index=data_dg_v.index)
        data_dg['group']=data_dg_v
        self.bulk_seq_group=data_dg
        pass
    def bulk_preprocess_lazy(self,)->None:
        print("......drop duplicates index in bulk data")
        self.bulk_seq=data_drop_duplicates_index(self.bulk_seq)
        print("......deseq2 normalize the bulk data")
        self.bulk_seq=deseq2_normalize(self.bulk_seq)
        print("......log10 the bulk data")
        self.bulk_seq=np.log10(self.bulk_seq+1)
        print("......calculate the mean of each group")
        if self.group is None:
            return None
        else:
            data_dg_v=self.bulk_seq[self.group].mean(axis=1)
            data_dg=pd.DataFrame(index=data_dg_v.index)
            data_dg['group']=data_dg_v
            self.bulk_seq_group=data_dg
        return None
    def single_preprocess_lazy(self,target_sum:int=1e4)->None:
        print("......normalize the single data")
        self.single_seq.obs_names_make_unique()
        self.single_seq.var_names_make_unique()
        sc.pp.normalize_total(self.single_seq, target_sum=target_sum)
        print("......log1p the single data")
        sc.pp.log1p(self.single_seq)
        return None
    def vae_configure(self,cell_target_num=None,**kwargs):
        self.vae_model=Bulk2Single(bulk_data=self.bulk_seq,single_data=self.single_seq,
                                   celltype_key=self.celltype_key,bulk_group=self.group,
                 top_marker_num=self.top_marker_num,ratio_num=self.ratio_num,gpu=self.gpu)
        if cell_target_num!=None:
            self.vae_model.cell_target_num=dict(zip(list(set(self.single_seq.obs[self.celltype_key])),
                                                [cell_target_num]*len(list(set(self.single_seq.obs[self.celltype_key])))))
        else:
            self.vae_model.predicted_fraction(*kwargs)
        self.vae_model.bulk_preprocess_lazy()
        self.vae_model.single_preprocess_lazy()
        self.vae_model.prepare_input()
    def vae_train(self,
                  vae_save_dir:str='save_model',
            vae_save_name:str='vae',
            generate_save_dir:str='output',
            generate_save_name:str='output',
            batch_size:int=512,
            learning_rate:int=1e-4,
            hidden_size:int=256,
            epoch_num:int=5000,
            patience:int=50,save:bool=True):
        self.vae_net=self.vae_model.train(
                batch_size=batch_size,
                learning_rate=learning_rate,
                hidden_size=hidden_size,
                epoch_num=epoch_num,
                vae_save_dir=vae_save_dir,
                vae_save_name=vae_save_name,
                generate_save_dir=generate_save_dir,
                generate_save_name=generate_save_name,
                patience=patience,save=save)
    def vae_load(self,vae_load_dir:str,hidden_size:int=256):
        print(f'loading model from {vae_load_dir}')
        vae_net = self.vae_model.load(vae_load_dir,hidden_size=hidden_size)
        self.vae_net=vae_net
    def vae_generate(self,highly_variable_genes:bool=True,max_value:float=10,
                     n_comps:int=100,svd_solver:str='auto',leiden_size:int=50)->anndata.AnnData:
        generate_adata=self.vae_model.generate()
        self.generate_adata_raw=generate_adata.copy()
        generate_adata.raw = generate_adata
        if highly_variable_genes:
            sc.pp.highly_variable_genes(generate_adata, min_mean=0.0125, max_mean=3, min_disp=0.5)
            generate_adata = generate_adata[:, generate_adata.var.highly_variable]
        sc.pp.scale(generate_adata, max_value=max_value)
        sc.tl.pca(generate_adata, n_comps=n_comps, svd_solver=svd_solver)
        sc.pp.neighbors(generate_adata, use_rep="X_pca")
        sc.tl.leiden(generate_adata)
        filter_leiden=list(generate_adata.obs['leiden'].value_counts()[generate_adata.obs['leiden'].value_counts()<leiden_size].index)
        generate_adata.uns['noisy_leiden']=filter_leiden
        print("The filter leiden is ",filter_leiden)
        generate_adata=generate_adata[~generate_adata.obs['leiden'].isin(filter_leiden)]
        self.generate_adata=generate_adata.copy()
        return generate_adata.raw.to_adata()
    def gnn_configure(self,gpu=0,hidden_size:int=128,
                     weight_decay:int=1e-2,
                     dropout:float=0.5,
                     batch_norm:bool=True,
                     lr:int=1e-3,
                     max_epochs:int=500,
                     display_step:int=25,
                     balance_loss:bool=True,
                     stochastic_loss:bool=True,
                     batch_size:int=2000,num_workers:int=5,):
        nocd_obj=scnocd(self.generate_adata,gpu=gpu)
        #nocd_obj.device = torch.device(f"cuda:{gpu}") if gpu >= 0 and torch.cuda.is_available() else torch.device('cpu')
        nocd_obj.matrix_transform(clustertype=self.celltype_key)
        nocd_obj.matrix_normalize()
        nocd_obj.GNN_configure(hidden_size=hidden_size,weight_decay=weight_decay,
                             dropout=dropout,batch_norm=batch_norm,lr=lr,
                             max_epochs=max_epochs,display_step=display_step,
                             balance_loss=balance_loss,stochastic_loss=stochastic_loss,
                             batch_size=batch_size)
        nocd_obj.GNN_preprocess(num_workers=num_workers)
        self.nocd_obj=nocd_obj
    def gnn_train(self,thresh:float=0.5,gnn_save_dir:str='save_model',
            gnn_save_name:str='gnn'):
        self.nocd_obj.GNN_model()
        self.nocd_obj.GNN_result(thresh=thresh)
        self.nocd_obj.cal_nocd()
        self.nocd_obj.save(gnn_save_dir=gnn_save_dir,gnn_save_name=gnn_save_name)
    def gnn_load(self,gnn_load_dir:str,thresh:float=0.5,):
        self.nocd_obj.load(gnn_load_dir)
        self.nocd_obj.GNN_result(thresh=thresh)
        self.nocd_obj.cal_nocd()
    def gnn_generate(self)->pd.DataFrame:
        unique_adata=self.nocd_obj.adata[~self.nocd_obj.adata.obs['nocd_n'].str.contains('-')]
        pair_dict_r={}
        repeat_celltype=dict(zip(list(set(unique_adata.obs[self.celltype_key])),np.zeros(len(list(set(unique_adata.obs[self.celltype_key]))))))
        for nocd_class in list(set(unique_adata.obs['nocd_n'])):
            now_celltype=unique_adata[unique_adata.obs['nocd_n']==nocd_class].obs.value_counts(self.celltype_key).index[0]
            if (now_celltype in pair_dict_r.values()):
                #print(now_celltype)
                pair_dict_r[str(nocd_class)]=now_celltype+'_'+str(int(repeat_celltype[now_celltype]))
                repeat_celltype[now_celltype]+=1
            else:
                pair_dict_r[str(nocd_class)]=now_celltype
                repeat_celltype[now_celltype]+=1
        def li_range(li,max_len):
            r=[0]*max_len   
            for i in li:
                r[int(i)]=1
            return r
        res_li=[li_range(i.split('-'),self.nocd_obj.K) for i in self.nocd_obj.adata.obs['nocd_n']]
        res_pd=pd.DataFrame(res_li,index=self.nocd_obj.adata.obs.index,columns=['nocd_'+i for i in [pair_dict_r[str(j)] for j in range(self.nocd_obj.K)]])
        print("The nocd result is ",res_pd.sum(axis=0))
        print("The nocd result has been added to adata.obs['nocd_']")
        self.nocd_obj.adata.obs=pd.concat([self.nocd_obj.adata.obs,res_pd],axis=1)
        return res_pd 
    def interpolation(self,celltype:str,adata:anndata.AnnData=None,)->anndata.AnnData:
        if adata is None:
            adata=self.single_seq
        test_adata=self.nocd_obj.adata[self.nocd_obj.adata.obs['nocd_{}'.format(celltype)]==1].raw.to_adata()
        if test_adata.shape[0]!=0:
            adata1=anndata.concat([test_adata,
                        adata],merge='same')
        else:
            adata1=adata 
            print("The cell type {} is not in the nocd result".format(celltype))
        return adata1
def create_st(generate_sc_data, generate_sc_meta, spot_num, cell_num, gene_num, marker_used):
    sc = generate_sc_data
    sc_ct = generate_sc_meta
    cell_name = sorted(list(set(sc_ct.Cell)))
    last_cell_pool = []
    spots = pd.DataFrame()
    meta = pd.DataFrame(columns=['Cell', 'Celltype', 'Spot'])
    sc_ct.index = sc_ct['Cell']
    for i in range(spot_num):
        cell_pool = random.sample(cell_name, cell_num)
        while set(cell_pool) == set(last_cell_pool):
            cell_pool = random.sample(cell_name, cell_num)
        last_cell_pool = cell_pool
        syn_spot = sc[cell_pool].sum(axis=1)
        if syn_spot.sum() > 25000:
            syn_spot *= 20000 / syn_spot.sum()
        spot_name = f'spot_{i + 1}'
        spots.insert(len(spots.columns), spot_name, syn_spot)
        for cell in cell_pool:
            celltype = sc_ct.loc[cell, 'Cell_type']
            row = {'Cell': cell, 'Celltype': celltype, 'Spot': spot_name}
            #meta = pd.concat([meta,pd.DataFrame(row)],axis=0)
            meta.loc[len(meta)]=pd.Series(row)
            #meta = meta.append(row, ignore_index=True)
    if marker_used:
        adata = scanpy.AnnData(sc.T)
        adata.obs = sc_ct[['Cell_type']]
        scanpy.tl.rank_genes_groups(adata, 'Cell_type', method='wilcoxon')
        marker_df = pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(gene_num)
        marker_array = np.array(marker_df)
        marker_array = np.ravel(marker_array)
        marker_array = np.unique(marker_array)
        marker = list(marker_array)
        sc = sc.loc[marker, :]
        spots = spots.loc[marker, :]
    return sc, sc_ct, spots, meta
def create_sample(sc, st, meta, multiple):
    cell_name = meta.Cell.values.tolist()
    spot_name = meta.Spot.values.tolist()
    # get wrong spot name for negative data
    all_spot = list(set(meta.Spot))
    wrong_spot_name = []
    for sn in spot_name:
        last_spot = all_spot.copy()  # --
        last_spot.remove(sn)  # --
        mul_wrong = random.sample(last_spot, multiple)
        wrong_spot_name.extend(mul_wrong)
    cfeat_p_list, cfeat_n_list = [], []
    for c in cell_name:
        cell_feat = sc[c].values.tolist()
        cfeat_p_list.append(cell_feat)
        cfeat_m = [cell_feat * multiple]
        cfeat_n_list.extend(cfeat_m)
    cfeat_p = np.array(cfeat_p_list)  # [n, d]
    cfeat_n = np.array(cfeat_n_list).reshape(-1, cfeat_p.shape[1])  # [n*m, d]
    # positive spot features
    sfeat_p_list = []
    for s in spot_name:
        spot_feat = st[s].values.tolist()
        sfeat_p_list.append(spot_feat)
        # sfeat_p = np.vstack((sfeat_p, spot_feat))
    sfeat_p = np.array(sfeat_p_list)  # [n, d]
    mfeat_p = sfeat_p - cfeat_p
    feat_p = np.hstack((cfeat_p, sfeat_p))
    feat_p = np.hstack((feat_p, mfeat_p))
    print('sucessfully create positive data')
    # negative spot features
    sfeat_n_list = []
    for s in wrong_spot_name:
        spot_feat = st[s].values.tolist()
        sfeat_n = sfeat_n_list.append(spot_feat)
    sfeat_n = np.array(sfeat_n_list)
    mfeat_n = sfeat_n - cfeat_n
    feat_n = np.hstack((cfeat_n, sfeat_n))
    feat_n = np.hstack((feat_n, mfeat_n))
    print('sucessfully create negative data')
    return feat_p, feat_n
def get_data(pos, neg):
    X = np.vstack((pos, neg))
    y = np.concatenate((np.ones(pos.shape[0]), np.zeros(neg.shape[0])))
    return X, y
def create_data_pyomic(single_data:anndata.AnnData,
                       spatial_data:anndata.AnnData,
                       celltype_key:str,spot_key:list=['xcoord','ycoord'],
                       ):
    print("...loading data")
    input_data = {}
    sc_gene=single_data.var._stat_axis.values.tolist()
    st_gene=spatial_data.var._stat_axis.values.tolist()
    intersection_genes=[]
    for i in sc_gene:
        if i in st_gene:
            intersection_genes.append(i)
    intersect_gene = intersection_genes
    generate_sc_data = single_data[:,intersect_gene].to_df().T
    st_data = spatial_data[:,intersect_gene].to_df().T
    ism=pd.DataFrame(index=single_data.obs.index)
    ism['Cell']=single_data.obs.index
    ism['Cell_type']=single_data.obs[celltype_key].values
    generate_sc_meta = ism
    ssm=pd.DataFrame(index=spatial_data.obs.index)
    ssm['Spot']=spatial_data.obs.index
    ssm['xcoord']=spatial_data.obs[spot_key[0]].values
    ssm['ycoord']=spatial_data.obs[spot_key[1]].values
    input_data["input_sc_meta"] = ism
    input_data["input_sc_data"] = generate_sc_data
    input_data["input_st_data"] = st_data
    input_data["input_st_meta"] = ssm
    input_data["sc_gene"]=sc_gene
    input_data["st_gene"]=st_gene
    input_data["intersect_gene"]=intersect_gene
    ssm=pd.DataFrame(index=spatial_data.obs.index)
    ssm['Spot']=spatial_data.obs.index
    ssm['xcoord']=spatial_data.obs[spot_key[0]].values
    ssm['ycoord']=spatial_data.obs[spot_key[1]].values
    return input_data
def create_data(generate_sc_meta, generate_sc_data, st_data, spot_num, cell_num, top_marker_num, marker_used,
                mul_train):
    sc_gene = generate_sc_data._stat_axis.values.tolist()
    st_gene = st_data._stat_axis.values.tolist()
    intersection_genes=[]
    for i in sc_gene:
        if i in st_gene:
            intersection_genes.append(i)
    intersect_gene = intersection_genes
    generate_sc_data = generate_sc_data.loc[intersect_gene]
    st_data = st_data.loc[intersect_gene]
    sc_train, _, st_train, meta_train = create_st(generate_sc_data, generate_sc_meta,
                                                  spot_num, cell_num,
                                                  top_marker_num, marker_used)
    pos_train, neg_train = create_sample(sc_train, st_train, meta_train, mul_train)
    xtrain, ytrain = get_data(pos_train, neg_train)
    return xtrain, ytrain
def creat_pre_data(st, cell_name, spot_name, spot_indx, cfeat, return_np=False):
    spot = spot_name[spot_indx]
    spot_feat = st[spot].values
    tlist = np.isnan(spot_feat).tolist()
    tlist = [i for i, x in enumerate(tlist) if x == True]
    assert len(tlist) == 0
    sfeat = np.tile(spot_feat, (len(cell_name), 1))
    mfeat = sfeat - cfeat
    feat = np.hstack((cfeat, sfeat))
    feat = np.hstack((feat, mfeat))
    if not return_np:
        feat = torch.from_numpy(feat).type(torch.FloatTensor)
    tlist = np.isnan(sfeat).tolist()
    tlist = [i for i, x in enumerate(tlist) if x == True]
    assert len(tlist) == 0
    tlist = np.isnan(cfeat).tolist()
    tlist = [i for i, x in enumerate(tlist) if x == True]
    assert len(tlist) == 0
    return feat
def creat_pre_data_batch(st, cell_name, spot_name, start_indx, end_indx, cfeat, return_np=False):
    feats = []
    for i in range(start_indx, end_indx):
        spot = spot_name[i]
        spot_feat = st[spot].values
        tlist = np.isnan(spot_feat).tolist()
        tlist = [j for j, x in enumerate(tlist) if x == True]
        assert len(tlist) == 0
        sfeat = np.tile(spot_feat, (len(cell_name), 1))
        mfeat = sfeat - cfeat
        feat = np.hstack((cfeat, sfeat))
        feat = np.hstack((feat, mfeat))
        feats.append(feat)
    feats = np.array(feats)
    if not return_np:
        feats = torch.from_numpy(feats).type(torch.FloatTensor)
    tlist = np.isnan(sfeat).tolist()
    tlist = [i for i, x in enumerate(tlist) if x == True]
    assert len(tlist) == 0
    tlist = np.isnan(cfeat).tolist()
    tlist = [i for i, x in enumerate(tlist) if x == True]
    assert len(tlist) == 0
    return feats
def predict_for_one_spot(model, st_test, cell_name, spot_name, spot_indx, cfeat):
    feats = creat_pre_data(st_test, cell_name, spot_name, spot_indx, cfeat, return_np=True)
    outputs = model.predict_proba(feats)[:, 1]
    # outputs = np.where(outputs>0.5, 1, 0)
    predict = outputs.tolist()
    return spot_indx, predict
def predict_for_one_spot_svm(model, st_test, cell_name, spot_name, spot_indx, cfeat):
    feats = creat_pre_data(st_test, cell_name, spot_name, spot_indx, cfeat, return_np=True)
    outputs = model(torch.tensor(feats)).detach().numpy()[:, 1]
    # outputs = np.where(outputs>0.5, 1, 0)
    predict = outputs.tolist()
    return spot_indx, predict
def predict_for_one_spot_net(model, st_test, cell_name, spot_name, spot_indx, cfeat):
    feats = creat_pre_data(st_test, cell_name, spot_name, spot_indx, cfeat, return_np=True)
    outputs = model(torch.tensor(feats)).detach().numpy().reshape(-1)
    # outputs = np.where(outputs>0.5, 1, 0)
    predict = outputs.tolist()
    return spot_indx, predict
def predict_for_one_spot_net_batch(model, st_test, cell_name, spot_name, spot_indx, cfeat):
    feats = creat_pre_data_batch(st_test, cell_name, spot_name, spot_indx, cfeat, return_np=True)
    outputs = model(torch.tensor(feats)).detach().numpy().reshape(-1,len(cell_name))
    return outputs
    # outputs = np.where(outputs>0.5, 1, 0)
    predict = outputs.tolist()
    return spot_indx, predict
class SVM(nn.Module):
    def __init__(self, X_train,num_classes=2):
        super().__init__()
        torch.manual_seed(2)
        self.linear = nn.Linear(X_train.shape[1], num_classes)
    def forward(self, x):
        return self.linear(x)
class Net(nn.Module):
    def __init__(self,size):
        super(Net, self).__init__()
        torch.manual_seed(2)
        self.linear1 = nn.Linear(size, 256)
        self.relu1 = nn.ReLU()
        self.linear2 = nn.Linear(256, 1)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        x = self.linear1(x)
        x = self.relu1(x)
        x = self.linear2(x)
        x = self.sigmoid(x)
        return x
class DFRunner:
    def __init__(self,
                 generate_sc_data,
                 generate_sc_meta,
                 st_data, st_meta,
                 marker_used,
                 top_marker_num,
                 random_seed=0,
                 n_jobs=1,device=None,):
        self.sc_test_allgene = generate_sc_data  # pandas.DataFrame, generated gene-cell expression data
        self.cell_type = generate_sc_meta  # pandas.DataFrame, cell type
        self.st_data = st_data  # pandas.DataFrame, gene-sport expression data
        self.meta_test = st_meta  # pandas.DataFrame, spot coordinate.
        self.n_jobs=n_jobs
        self.sc_test = self.sc_test_allgene  # pd.DataFrame, test cell-gene expression data.
        self.st_test = self.st_data  # pd.DataFrame, test spot-gene expression data.
        sc_gene = self.sc_test._stat_axis.values.tolist()
        st_gene = self.st_test._stat_axis.values.tolist()
        intersect_gene = list(set(sc_gene).intersection(set(st_gene)))
        self.sc_test = self.sc_test.loc[intersect_gene]
        self.st_test = self.st_test.loc[intersect_gene]
        self.used_device=device
        if marker_used:
            print('select top %d marker genes of each cell type...' % top_marker_num)
            sc = scanpy.AnnData(self.sc_test.T)
            sc.obs = self.cell_type[['Cell_type']]
            scanpy.tl.rank_genes_groups(sc, 'Cell_type', method='wilcoxon')
            marker_df = pd.DataFrame(sc.uns['rank_genes_groups']['names']).head(top_marker_num)
            marker_array = np.array(marker_df)
            marker_array = np.ravel(marker_array)
            marker_array = np.unique(marker_array)
            marker = list(marker_array)
            self.sc_test = self.sc_test.loc[marker, :]
            self.st_test = self.st_test.loc[marker, :]
        #self.model = CascadeForestClassifier(random_state=random_seed, n_jobs=n_jobs,
        #                                     verbose=0)
        #self.model = KNN(k=2)
        breed = self.cell_type['Cell_type']
        breed_np = breed.values
        breed_set = set(breed_np)
        self.id2label = sorted(list(breed_set))
        self.label2id = {label: idx for idx, label in enumerate(self.id2label)}
        self.cell2label = dict()
        self.label2cell = defaultdict(set)
        for row in self.cell_type.itertuples():
            cell_name = getattr(row, 'Cell')
            cell_type = self.label2id[getattr(row, 'Cell_type')]
            self.cell2label[cell_name] = cell_type
            self.label2cell[cell_type].add(cell_name)
    def run(self, xtrain, ytrain, max_cell_in_diff_spot_ratio, k, save_dir, save_name, load_path=None,
            num_epochs=1000,batch_size=1000,predicted_size=32):
        self.model = Net(xtrain.shape[1]).to(self.used_device)
        print('...The model size is %d' % xtrain.shape[1])
        if load_path is None:
            print('...Net training')
            #xtrain = torch.tensor(xtrain, dtype=torch.float32).to(self.used_device)
            #ytrain = torch.tensor(ytrain, dtype=torch.float32).to(self.used_device)
            # 定义损失函数和优化器
            criterion = nn.BCELoss()
            optimizer = torch.optim.SGD(self.model.parameters(), lr=0.001)
            inputs = torch.tensor(xtrain, dtype=torch.float32).to(self.used_device)
            labels = torch.tensor(ytrain.reshape(len(ytrain),1), dtype=torch.float32).to(self.used_device)
            # 将数据分成小批量
            batch_size = batch_size
            dataset = TensorDataset(inputs, labels)
            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
            # 训练模型
            with trange(num_epochs) as t:
                for epoch in t:
                    running_loss = 0.0
                    num_samples = 0
                    for i, data in enumerate(dataloader):
                        # 从数据集中获取输入和标签
                        inputs, labels = data
                        # 将输入和标签转换为 PyTorch 的变量
                        inputs = inputs.requires_grad_()
                        labels = labels.float()
                        # 计算模型输出和损失
                        outputs = self.model(inputs)
                        loss = criterion(outputs, labels)
                        # 反向传播和优化
                        optimizer.zero_grad()
                        loss.backward()
                        optimizer.step()
                        # 记录损失值
                        running_loss += loss.item()*inputs.size(0)
                        num_samples += inputs.size(0)
                    # 输出平均损失值
                    average_loss = running_loss / num_samples
                    t.set_description('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, average_loss))
            #self.model.fit(xtrain, ytrain)  # train model
            print('...Net training done!')
            path_save = os.path.join(save_dir, f"{save_name}.pth")
            if not os.path.exists(save_dir):
                os.makedirs(save_dir)
            torch.save(self.model.state_dict(), path_save)
            print(f"...save trained net in {path_save}.")
        else:
            self.model.load_state_dict(torch.load(load_path,map_location=self.used_device))
            #self._load_model(load_path)
        #return self.cre_csv(max_cell_in_diff_spot_ratio, k)
        df_meta, df_spot = self.cre_csv(max_cell_in_diff_spot_ratio, k,predicted_size)  # save predicted results.
        return df_meta, df_spot
    def load(self, load_path,modelsize,max_cell_in_diff_spot_ratio, k,predicted_size):
        self.model = Net(modelsize).to(self.used_device)
        self.model.load_state_dict(torch.load(load_path,map_location=self.used_device))
        #self._load_model(load_path
        df_meta, df_spot = self.cre_csv(max_cell_in_diff_spot_ratio, k,predicted_size)  # save predicted results.
        return df_meta, df_spot
    def cre_csv(self, max_cell_in_diff_spot_ratio, k,batch_size):
        # data
        cell_name = self.sc_test.columns.values.tolist()
        spot_name = self.st_test.columns.tolist()  # list of spot name ['spot_1', 'spot_2', ...]
        cfeat = self.sc_test.values.T
        cell_num = len(cell_name)
        spot_num = len(spot_name)
        batch_size=batch_size
        if max_cell_in_diff_spot_ratio is None:
            max_cell_in_diff_spot = None
        else:
            max_cell_in_diff_spot = int(max_cell_in_diff_spot_ratio * k * spot_num / cell_num)
        def joint_predict(ratio):            
            score_triple_list = list()
            spot2cell = defaultdict(set)
            cell2spot = defaultdict(set)
            spot2ratio = dict()
            print('Calculating scores...')
            #batch_size = batch_size  # adjust this value based on the available memory and the size of the data
            batch_list = [(i, min(i+batch_size, spot_num)) for i in range(0, spot_num, batch_size)]
            with trange(len(batch_list)) as t:
                for batch_indx in t:
                    start_indx, end_indx = batch_list[batch_indx]
                    feat=creat_pre_data_batch(self.st_test, cell_name, spot_name, start_indx,end_indx, cfeat, return_np=True)
                    predict=self.model(torch.tensor(feat, dtype=torch.float32).to(self.used_device)).cpu().detach().numpy().reshape(-1,len(cell_name))
                    for spot_indx,spot_p_num in zip(range(start_indx, end_indx),range(len(spot_name[start_indx:end_indx]))):
                        spot = spot_name[spot_indx]  # spotname
                        for c, p in zip(cell_name,predict[spot_p_num]):
                            score_triple_list.append((c, spot, p))
                        spot2ratio[spot] = np.round(ratio[spot_indx] * k)
                        t.set_description('Now calculating scores for spot %d/%d' % (spot_indx + 1, len(spot_name)))
            print('Calculating scores done.')
            # spot2ratio: map spot to cell type ratio in it.
            score_triple_list = sorted(score_triple_list, key=lambda x: x[2], reverse=True)
            # sort by score
            for c, spt, score in score_triple_list:
                # cell name, spot name, score
                if max_cell_in_diff_spot is not None and len(cell2spot[c]) == max_cell_in_diff_spot:
                    # The number of this cell in different spots reaches a maximum
                    continue
                if len(spot2cell[spt]) == k:
                    # The maximum number of cells in this spot
                    continue
                cell_class = self.cell2label.get(c)
                if cell_class is None:
                    continue
                if spot2ratio[spt][cell_class] > 0:
                    # Put this cell in this spot
                    spot2ratio[spt][cell_class] -= 1
                    spot2cell[spt].add(c)
                    cell2spot[c].add(spt)
                else:
                    continue
            cell_list, spot_list, spot_len = list(), list(), list()
            df_spots = pd.DataFrame()
            order_list = spot_name
            for spot in order_list:
                if spot2cell.get(spot):
                    cells = spot2cell.get(spot)
                    cell_num = len(cells)
                    cell_list.extend(sorted(list(cells)))
                    spot_list.extend([spot] * cell_num)
                    spot_len.append(cell_num)
                    cell_pool = list(cells)
                    cell_pool.sort()
                    predict_spot = self.sc_test_allgene[cell_pool]
                    df_spots = pd.concat([df_spots, predict_spot], axis=1)
            return cell_list, spot_list, spot_len, df_spots
        ratio = self.__calc_ratio()  # [spot_num, class_num]
        cell_list, spot_list, spot_len, df_spots = joint_predict(ratio)
        #return joint_predict(ratio)
        meta = {'Cell': cell_list, 'Spot': spot_list}
        df = pd.DataFrame(meta)
        self.cell_type = self.cell_type.reset_index(drop=True)
        df_meta = pd.merge(df, self.cell_type, how='left')
        df_meta = df_meta[['Cell', 'Cell_type', 'Spot']]
        df_meta = pd.merge(df_meta, self.meta_test, how='inner')
        df_meta = df_meta.rename(columns={'xcoord': 'Spot_xcoord', 'ycoord': 'Spot_ycoord'})
        coord = self.meta_test[['xcoord', 'ycoord']].to_numpy()
        nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(coord)
        distances, indices = nbrs.kneighbors(coord)
        radius = distances[:, -1] / 2
        radius = radius.tolist()
        all_coord = df_meta[['Spot_xcoord', 'Spot_ycoord']].to_numpy()
        all_radius = list()
        for i in range(len(spot_len)):
            a = [radius[i]] * spot_len[i]
            all_radius.extend(a)
        length = np.random.uniform(0, all_radius)
        angle = np.pi * np.random.uniform(0, 2, all_coord.shape[0])
        x = all_coord[:, 0] + length * np.cos(angle)
        y = all_coord[:, 1] + length * np.sin(angle)
        cell_coord = {'Cell_xcoord': np.around(x, 2).tolist(), 'Cell_ycoord': np.around(y, 2).tolist()}
        df_cc = pd.DataFrame(cell_coord)
        df_meta = pd.concat([df_meta, df_cc], axis=1)
        cell_rename = [f'C_{i}' for i in range(1, df_spots.shape[1] + 1)]
        df_spots.columns = cell_rename
        df_meta = df_meta.drop(['Cell'], axis=1)
        df_meta.insert(0, "Cell", cell_rename)
        return df_meta, df_spots
    def __calc_ratio(self):
        label_devide_data = dict()
        for label, cells in self.label2cell.items():
            label_devide_data[label] = self.sc_test[list(cells)]
        single_cell_splitby_breed_np = {}
        for key in label_devide_data.keys():
            single_cell_splitby_breed_np[key] = label_devide_data[key].values  # [gene_num, cell_num]
            single_cell_splitby_breed_np[key] = single_cell_splitby_breed_np[key].mean(axis=1)
        max_decade = len(single_cell_splitby_breed_np.keys())
        single_cell_matrix = []
        for i in range(max_decade):
            single_cell_matrix.append(single_cell_splitby_breed_np[i].tolist())
        single_cell_matrix = np.array(single_cell_matrix)
        single_cell_matrix = np.transpose(single_cell_matrix)  # (gene_num, label_num)
        num_spot = self.st_test.values.shape[1]
        spot_ratio_values = np.zeros((num_spot, max_decade))  # (spot_num, label_num)
        spot_values = self.st_test.values  # (gene_num, spot_num)
        for i in range(num_spot):
            ratio_list = [0 for x in range(max_decade)]
            spot_rep = spot_values[:, i].reshape(-1, 1)
            spot_rep = spot_rep.reshape(spot_rep.shape[0], )
            ratio = nnls(single_cell_matrix, spot_rep)[0]
            ratio_list = [r for r in ratio]
            ratio_list = (ratio_list / np.sum([ratio_list], axis=1)[0]).tolist()
            for j in range(max_decade):
                spot_ratio_values[i, j] = ratio_list[j]
        return spot_ratio_values
    def _load_model(self, save_path):
        self.model.load(save_path)
        print(f"loading model from {save_path}")
    def _save_model(self, save_path):
        self.model.save(save_path)
        print(f"saving model to {save_path}")
        return save_path
def aprior(gamma_hat, axis=None):
    m = np.mean(gamma_hat, axis=axis)
    s2 = np.var(gamma_hat, ddof=1, axis=axis)
    return (2 * s2 + np.power(m, 2)) / s2
def bprior(gamma_hat, axis=None):
    m = np.mean(gamma_hat, axis=axis)
    s2 = np.var(gamma_hat, ddof=1, axis=axis)
    return (m * s2 + np.power(m, 3)) / s2
def postmean(g_hat, g_bar, n, d_star, t2):
    return (t2 * n * g_hat + d_star * g_bar) / (t2 * n + d_star)
def postvar(sum2, n, a, b):
    return (0.5 * sum2 + b) / (n / 2 + a - 1)
def it_sol(sdat, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001):
    n = np.sum(~np.isnan(sdat), axis=1)
    g_old = g_hat
    d_old = d_hat
    change = 1
    count = 0
    while change > conv:
        g_new = postmean(g_hat, g_bar, n, d_old, t2)
        sum2 = np.nansum(np.power(sdat - np.dot(np.expand_dims(g_new, axis=1), np.ones((1, sdat.shape[1]))), 2), axis=1)
        d_new = postvar(sum2, n, a, b)
        change = np.max((np.max(np.abs(g_new - g_old) / g_old), np.max(np.abs(d_new - d_old) / d_old)))
        g_old = g_new
        d_old = d_new
        count += 1
    return np.concatenate((np.expand_dims(g_new, axis=1), np.expand_dims(d_new, axis=1)), axis=1)
def joint_analysis(dat, batch, mod=None, par_prior=True, proir_plots=False, mean_only=False, ref_batch=None):
    rownames = dat.index
    colnames = dat.columns
    dat = np.array(dat)
    batch_levels = batch.drop_duplicates()
    batches = []
    ref_index = 0
    zero_rows_list = []
    for i, batch_level in enumerate(batch_levels):
        idx = batch.isin([batch_level])
        if batch_level == ref_batch:
            ref_index = i
        batches.append(idx.reset_index().loc[lambda d: d.Batch == True].index)
        batch_dat = dat[:, idx]
        for row in range(np.size(batch_dat, 0)):
            if np.var(batch_dat[row, :], ddof=1) == 0:
                zero_rows_list.append(row)
    zero_rows = list(set(zero_rows_list))
    keep_rows = list(set(range(dat.shape[0])).difference(set(zero_rows_list)))
    dat_origin = dat
    dat = dat[keep_rows, :]
    batchmod = pd.get_dummies(batch, drop_first=False, prefix='batch')
    batchmod['batch_' + ref_batch] = 1
    ref = batchmod.columns.get_loc('batch_' + ref_batch)
    design = np.array(batchmod)
    n_batch = batch_levels.shape[0]
    n_batches = [len(x) for x in batches]
    B_hat = solve(np.dot(design.T, design), np.dot(design.T, dat.T))
    grand_mean = np.expand_dims(B_hat[ref, :], axis=1)
    ref_dat = dat[:, batches[ref_index]]
    var_pool = np.expand_dims(np.dot(np.square(ref_dat - np.dot(design[batches[ref_index], :], \
                                                                B_hat).T),
                                     np.ones(n_batches[ref_index]).T * 1 / n_batches[ref_index]), axis=1)
    stand_mean = np.dot(grand_mean, np.ones((1, batch.shape[0])))
    s_data = (dat - stand_mean) / np.dot(np.sqrt(var_pool), np.ones((1, batch.shape[0])))
    batch_design = design
    gamma_hat = solve(np.dot(batch_design.T, batch_design), np.dot(batch_design.T, s_data.T))
    delta_hat = np.empty([0, s_data.shape[0]])
    for i in batches:
        row_vars = np.expand_dims(np.nanvar(s_data[:, i], axis=1, ddof=1), axis=0)
        delta_hat = np.concatenate((delta_hat, row_vars), axis=0)
    gamma_bar = np.mean(gamma_hat, axis=1)
    t2 = np.var(gamma_hat, axis=1, ddof=1)
    a_prior = aprior(delta_hat, axis=1)
    b_prior = bprior(delta_hat, axis=1)
    results = []
    gamma_star = np.empty((n_batch, s_data.shape[0]))
    delta_star = np.empty((n_batch, s_data.shape[0]))
    for j, batch_level in enumerate(batch_levels):
        i = batchmod.columns.get_loc('batch_' + batch_level)
        results.append(it_sol(s_data[:, batches[j]], gamma_hat[i, :], delta_hat[j, :], gamma_bar[i], t2[i], a_prior[j],
                              b_prior[j]).T)
    for j, batch_level in enumerate(batch_levels):
        gamma_star[j, :] = results[j][0]
        delta_star[j, :] = results[j][1]
    gamma_star[ref_index, :] = 0
    delta_star[ref_index, :] = 1
    bayesdata = s_data
    for i, batch_index in enumerate(batches):
        bayesdata[:, batch_index] = (bayesdata[:, batch_index] - np.dot(batch_design[batch_index, :], gamma_star).T) / \
                                    np.dot(np.sqrt(np.expand_dims(delta_star[i], axis=1)), np.ones((1, n_batches[i])))
    bayesdata = (bayesdata * np.dot(np.sqrt(var_pool), np.ones((1, dat.shape[1])))) + stand_mean
    bayesdata[:, batches[ref_index]] = dat[:, batches[ref_index]]
    if len(zero_rows) > 0:
        dat_origin[keep_rows, :] = bayesdata
        bayesdata = pd.DataFrame(dat_origin, index=rownames, columns=colnames)
    bayesdata[bayesdata < 0] = 0
    return bayesdata
def knn(data, query, k):
    tree = KDTree(data)
    dist, ind = tree.query(query, k)
    return dist, ind
def run_heatmap(workdir, input_path, output_path, color_up="default", color_down="default", color_mid="default", show_border=False, scale='row', cluster_rows=True, cluster_cols=False, cellwidth="20", cellheight="20", fontsize="10"):
    # R脚本的路径
    script_path =os.path.join(workdir,'heatmap.R')
    # Rscript heatmap.R --input input_file/expression_matrix_heatmap.csv --output output_file/heatmap.png --color_up "default" --color_down "default" --color_mid "default" --show_border TRUE --scale "z-score" --cluster_rows TRUE --cluster_cols FALSE --cellwidth 20 --cellheight 20 --fontsize 10
    cmd = [
        'Rscript', script_path,
        '--input', input_path,
        '--output', output_path,
        '--color_up', color_up,
        '--color_down', color_down,
        '--color_mid', color_mid,
        '--show_border', str(show_border).upper(),
        '--scale', scale,
        '--cluster_rows', str(cluster_rows).upper(),
        '--cluster_cols', str(cluster_cols).upper(),
        '--cellwidth', str(cellwidth),  # 转换为字符串
        '--cellheight', str(cellheight),  # 转换为字符串
        '--fontsize', str(fontsize)  # 转换为字符串
    ]
    # 执行R脚本并捕获输出
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
def run_pca(workdir, input_count_path, input_sample_path, output_png_path,output_hmtl_path):
    # R脚本的路径，需要师哥你改路径
    script_path = os.path.join(workdir, 'PCA.R')
    # Rscript PCA.R -c 'input_file/expression_matrix.csv' -s 'input_file/sample_info.csv' -o 'output_file/pca.png' -t 'output_file/pca.html'
    cmd = [
        'Rscript', script_path,
        '--input_count', input_count_path,
        '--input_sample', input_sample_path,
        '--output_png', output_png_path,
        '--output_html', output_hmtl_path,
    ]
    # 执行R脚本并捕获输出
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        return e.stderr
#!/usr/bin/env Rscript
library(DESeq2)
library(ggplot2)
library(plotly)
library(optparse)
## 测试
# Rscript Deseq2.R -i 'input_file/expression_matrix_deseq2.csv' -o 'output_file/deseq2.tsv' -n 3
# 定义命令行参数
option_list <- list(
  make_option(c("-i", "--input"), type = "character", default = "", help = "输入文件路径"),
  make_option(c("-o", "--output"), type = "character", default = "", help = "输出文件路径"),
  make_option(c("-n", "--repetition"), type = "integer", default = 3, help = "样本重复数") 
)
# 解析命令行参数
opt_parser <- OptionParser(option_list = option_list)
opt <- parse_args(opt_parser)
# 检查输入文件是否存在
if (!file.exists(opt$input)) {
  stop("输入文件不存在: ", opt$input)
}
# 读取基因表达谱
count_df <- read.csv(file = opt$input, header = TRUE, row.names = 1)
rownames(count_df) <- gsub("-", ".", rownames(count_df))
# 过滤掉垃圾基因
count_df <- count_df[rowSums(count_df) > 10 & apply(count_df,1,function(x){ all(x > 0) }),]
# 根据重复数分组
n <- opt$repetition
colnames_split <- strsplit(colnames(count_df), "_")
group_names <- unique(sapply(colnames_split, `[`, 1))
if (length(group_names) != 2) {
  stop("输入的count表必须只包含两组数据")
}
# 创建样本分组信息
sample_info <- data.frame(row.names = colnames(count_df))
sample_info$group <- rep(group_names, each = n)
# 创建dds对象用于差异表达分析
dds <- DESeqDataSetFromMatrix(countData = count_df,
                              colData = DataFrame(sample_info),
                              design = ~ group)
# 执行DESeq分析
dds <- DESeq(dds)
# 获取差异表达结果
res <- results(dds)
# 保存结果为TSV文件
# write.table(as.data.frame(res), file = opt$output, sep = "\t", quote = FALSE, row.names = TRUE)
# write.csv(as.data.frame(res), file = opt$output, row.names = TRUE)
# 将差异表达分析的结果转换为data.frame
res_df <- as.data.frame(res)
# 将行名（基因名）作为新列“Gene”添加到data.frame
res_df$Gene <- rownames(res_df)
# 重新排列列，使得"Gene"列成为第一列
res_df <- res_df[, c("Gene", setdiff(names(res_df), "Gene"))]
# 保存结果为TSV文件，第一列列名为"Gene"
write.table(res_df, file = opt$output, sep = "\t", quote = FALSE, row.names = FALSE)
#!/usr/bin/env Rscript
library(optparse)
library(clusterProfiler)
library(dplyr)
## 测试
#  Rscript go_enrich.R -p 0.05 -i 'input_file/gene_list_nc.txt' -o 'output_file/go.tsv' -g 'enrich_background_file/go_gene_nc.txt'
# 设置命令行选项
option_list <- list(
  make_option(c("-p", "--pvalue"), type = "numeric", default = 0.05, help = "pvalue阈值", metavar = "pvalue"),
  make_option(c("-i", "--input"), type = "character", default = NULL, help = "输入文件路径", metavar = "file"),
  make_option(c("-g", "--go_file"), type = "character", default = NULL, help = "GO文件", metavar = "file"),
  make_option(c("-o", "--output"), type = "character", default = NULL, help = "输出文件路径", metavar = "file")
)
# 解析命令行参数
opt_parser = OptionParser(option_list=option_list)
opt = parse_args(opt_parser)
# 检查输入文件是否存在
if (!file.exists(opt$input)) {
  stop("输入文件不存在: ", opt$input)
}
# 背景文件，需要师哥你修改一下路径
go2name <- read.delim('/Users/dongjiacheng/Desktop/web/omicpy/enrich_background_file/go2name.txt', stringsAsFactors=FALSE)
go2gene_df <- read.delim(opt$go_file, stringsAsFactors=FALSE)
# 读取gene list
genelist <- read.csv(file = opt$input, row.names = 1)
genelist <- as.character(rownames(genelist))
# 如果genelist中有重复的基因，只保留一个
genelist <- unique(genelist)
go2gene_list <- split(go2gene_df, f = go2gene_df$CLASS)
enrich_MF = enricher(genelist, TERM2GENE=go2gene_list[['MF']][c(1,2)], TERM2NAME=go2name, pvalueCutoff = 1, qvalueCutoff = 1)
enrich_BP = enricher(genelist, TERM2GENE=go2gene_list[['BP']][c(1,2)], TERM2NAME=go2name, pvalueCutoff = 1, qvalueCutoff = 1)
enrich_CC = enricher(genelist, TERM2GENE=go2gene_list[['CC']][c(1,2)], TERM2NAME=go2name, pvalueCutoff = 1, qvalueCutoff = 1)
# 合并三个表
enrich_MF = as.data.frame(enrich_MF)
enrich_BP = as.data.frame(enrich_BP)
enrich_CC = as.data.frame(enrich_CC)
enrich_all <- bind_rows(
  mutate(enrich_MF, category = "MF"),
  mutate(enrich_BP, category = "BP"),
  mutate(enrich_CC, category = "CC")
)
# 保存结果到指定的输出文件
write.table(enrich_all, file = opt$output, sep = "\t", quote = FALSE, row.names = FALSE)
library(optparse)
library(pheatmap)
library(dplyr)
library(ggplot2)
# Rscript heatmap.R --input input_file/expression_matrix_heatmap.csv --output output_file/heatmap.png --color_up "default" --color_down "default" --color_mid "default" --show_border TRUE --scale "z-score" --cluster_rows TRUE --cluster_cols FALSE --cellwidth 20 --cellheight 20 --fontsize 10
# 定义命令行选项
option_list <- list(
    make_option(c("--input"), type = "character", default = NULL, help = "输入文件"),
    make_option(c("--output"), type = "character", default = NULL, help = "输出文件"),
    make_option(c("--color_up"), type = "character", default = "default", help = "高表达量颜色"),
    make_option(c("--color_down"), type = "character", default = "default", help = "低表达量颜色"),
    make_option(c("--color_mid"), type = "character", default = "default", help = "中等表达量颜色"),
    make_option(c("--show_border"), type = "logical", default = FALSE, help = "是否显示边框"),
    make_option(c("--cluster_rows"), type = "logical", default = TRUE, help = "是否对基因进行聚类"),
    make_option(c("--cluster_cols"), type = "logical", default = FALSE, help = "是否对样本进行聚类"),
    make_option(c("--scale"), type = "character", default = "log2", help = "标准化方式"),
    make_option(c("--cellwidth"), type = "numeric", default = 20, help = "单元格宽度"),
    make_option(c("--cellheight"), type = "numeric", default = 20, help = "单元格高度"),
    make_option(c("--fontsize"), type = "numeric", default = 10, help = "字体大小")
)
# 解析命令行选项
args <- parse_args(OptionParser(option_list = option_list))
row_zscores <- function(x) {
  row_means <- rowMeans(x)
  row_sds <- apply(x, 1, sd)
  scale(x, center = row_means, scale = row_sds)
}
draw_heatmap <- function(args) {
    data <- read.csv(args$input) # 读取csv文件
    data <- as.data.frame(data) # 转换为data.frame
    row.names(data) <- make.unique(data[,1]) # 使用make.unique确保行名的唯一性
    data <- data[,-1]
    # data <- row_zscores(data) # z-score标准化
    # 如果上下调与中等表达量参数为default，则运行
    if(args$color_up == "default" & args$color_down == "default" & args$color_mid == "default") {
      p <- pheatmap(data,
          # border_color = ifelse(args$show_border, args$color_border, NA),
          border = args$show_border,
          breaks = seq(-2, 2, length.out = 100),
          scale = "row",
          cluster_rows = args$cluster_rows,
          cluster_cols = args$cluster_cols,
          cellwidth = args$cellwidth, 
          cellheight = args$cellheight,
          fontsize = args$fontsize,
          legend = TRUE,
          silent = TRUE
    )
    }
    else {
      my_colors <- colorRampPalette(c(args$color_down, args$color_mid, args$color_up))(n = 256)
      p <- pheatmap(data,
        color = my_colors,
        # border_color = ifelse(args$show_border, args$color_border, NA),
        border = args$show_border,
        breaks = seq(-2, 2, length.out = 256),
        scale = "row",
        cluster_rows = args$cluster_rows,
        cluster_cols = args$cluster_cols,
        cellwidth = args$cellwidth, 
        cellheight = args$cellheight,
        fontsize = args$fontsize,
        legend = TRUE,
        silent = TRUE
    )
    }
    return(p)
}
draw_heatmap_log <- function(args) {
    data <- read.csv(args$input) # 读取csv文件
    data <- as.data.frame(data) # 转换为data.frame
    row.names(data) <- make.unique(data[,1]) # 使用make.unique确保行名的唯一性
    data <- data[,-1]
    data <- log2(data+1) # log2化
    # 如果上下调与中等表达量参数为default，则运行
    if(args$color_up == "default" & args$color_down == "default" & args$color_mid == "default") {
      p <- pheatmap(data,
          # border_color = ifelse(args$show_border, args$color_border, NA),
          border = args$show_border,
          cluster_rows = args$cluster_rows,
          cluster_cols = args$cluster_cols,
          cellwidth = args$cellwidth, 
          cellheight = args$cellheight,
          fontsize = args$fontsize,
          legend = TRUE,
          silent = TRUE
    )
    }
    else {
      my_colors <- colorRampPalette(c(args$color_down, args$color_mid, args$color_up))(n = 256)
      p <- pheatmap(data,
          color = my_colors,
          # border_color = ifelse(args$show_border, args$color_border, NA),
          border = args$show_border,
          cluster_rows = args$cluster_rows,
          cluster_cols = args$cluster_cols,
          cellwidth = args$cellwidth, 
          cellheight = args$cellheight,
          fontsize = args$fontsize,
          legend = TRUE,
          silent = TRUE
      )
    return(p)
}
}
# 如果标准化方式为z-score，则运行
if(args$scale == "z-score")
    p <- draw_heatmap(args)
# 如果标准化方式为log2，则运行
if(args$scale == "log2")
    p <- draw_heatmap_log(args)
# 保存图片
ggsave(args$output, plot = p, width = 10, height = 10, dpi = 600)
#!/usr/bin/env Rscript
library(optparse)
library(clusterProfiler)
library(dplyr)
## 测试
#  Rscript kegg_enrich.R -p 0.05 -i input_file/gene_list_nc.txt -o output_file/kegg.tsv -k input_file/kegg_pathway_nc.txt -f input_file/kegg_gene_nc.txt
# 设置命令行选项
option_list <- list(
  make_option(c("-p", "--pvalue"), type = "numeric", default = 0.05, help = "pvalue阈值", metavar = "pvalue"),
  make_option(c("-i", "--input"), type = "character", default = NULL, help = "输入文件路径", metavar = "file"),
  make_option(c("-o", "--output"), type = "character", default = NULL, help = "输出文件路径", metavar = "file"),
  make_option(c("-k", "--kegg_file1"), type = "character", default = NULL, help = "kegg通路文件", metavar = "file"),
  make_option(c("-f", "--kegg_file2"), type = "character", default = NULL, help = "kegg通路基因文件", metavar = "file")
)
# 解析命令行参数
opt_parser <- OptionParser(option_list = option_list)
opt <- parse_args(opt_parser)
# 检查输入文件是否存在
if (!file.exists(opt$input)) {
  stop("输入文件不存在: ", opt$input)
}
# 背景文件，需要师哥你修改一下路径
ko2name <- read.delim(file = opt$kegg_file1,stringsAsFactors = FALSE)
ko2gene <- read.delim(file = opt$kegg_file2,stringsAsFactors = FALSE)
# 读取gene list
genelist <- read.csv(file = opt$input, row.names = 1)
genelist <- as.character(rownames(genelist))
# 如果genelist中有重复的基因，只保留一个
genelist <- unique(genelist)
# kegg
enrich_kegg <- enricher(genelist,
  TERM2GENE = ko2gene,
  TERM2NAME = ko2name,
  pAdjustMethod = "BH", # 使用FDR进行校正
  pvalueCutoff = opt$pvalue,
  qvalueCutoff = 1
)
# 保存结果到指定的输出文件
write.table(enrich_kegg, file = opt$output, sep = "\t", quote = FALSE, row.names = FALSE)
#!/usr/bin/env Rscript
library(DESeq2)
library(ggplot2)
library(plotly)
library(optparse)
library(htmlwidgets)
## 测试
# Rscript PCA.R -c 'input_file/expression_matrix.csv' -s 'input_file/sample_info.csv' -o 'output_file/pca.png' -t 'output_file/pca.html'
pdf(file = NULL)
option_list <- list(
  make_option(c("-c", "--input_count"), type = "character", default = "", help = "基因表达谱(Count)"), 
  make_option(c("-s", "--input_sample"), type = "character", default = "", help = "样本分组信息"),
  make_option(c("-o", "--output_png"), type = "character", default = "", help = "输出的PCA静态图"),
  make_option(c("-t", "--output_html"), type = "character", default = "", help = "输出的PCA交互式图的html文件")
)
# 解析命令行参数
opt_parser <- OptionParser(option_list = option_list)
opt <- parse_args(opt_parser)
# 检查输入文件是否存在
if (!file.exists(opt$input_count)) {
  stop("输入文件不存在: ", opt$input_count)
}
if (!file.exists(opt$input_sample)) {
  stop("输入文件不存在: ", opt$input_sample)
}
# 读取基因表达谱
count_df <- read.csv(file = opt$input_count, header = TRUE, row.names = 1)
# 读取样本分组信息
sample_df <- read.csv(file = opt$input_sample, header = TRUE, row.names = 1)
# 对样本信息进行预处理
rownames(count_df) <- gsub("-", ".", rownames(count_df))
rownames(sample_df) <- gsub("-", ".", rownames(sample_df))
sample_df$Group <- gsub("-", ".", sample_df$Group)
# 创建dds对象用于差异表达分析
sample_df$Group <- factor(sample_df$Group)
deseq2.obj1 <- DESeqDataSetFromMatrix(
    countData = count_df,
    colData = DataFrame(sample_df),
    design = ~Group
)
# 执行DESeq分析
deseq2.obj2 <- DESeq(deseq2.obj1)
# 准备数据进行PCA分析
dds <- deseq2.obj2
dds <- estimateSizeFactors(dds)
rld <- rlog(dds)
# 绘制PCA图
pcaData <- plotPCA(rld, intgroup="Group", returnData=TRUE)
percentVar <- attr(pcaData, "percentVar")
ggplot(pcaData, aes(x=PC1, y=PC2, color=Group)) +
  geom_point(size=6) +  # 调整点的大小为6，可以根据需要调整
  xlab(paste0("PC1: ",round(percentVar[1]*100),"% variance")) +
  ylab(paste0("PC2: ",round(percentVar[2]*100),"% variance")) +
  coord_fixed() +
  ggtitle("") +
  theme_bw() +  # 使用theme_bw作为基础主题
  theme(
    panel.border = element_rect(colour = "#000000b0", fill=NA, linewidth=0.6)  # 设置边框颜色和线宽
  )
# 保存图片
ggsave(opt$output_png, width=6, height=4, dpi=300)
##  3D PCA
pca_data <- prcomp(t(assay(rld)))
plot_df <- as.data.frame(pca_data$x)
# 计算方差百分比
pca_var <- (pca_data$sdev^2 / sum(pca_data$sdev^2))[1:3] * 100
fig_3d <- plot_ly(
  data = plot_df, x = ~PC1, y = ~PC2, z = ~PC3, text = rownames(plot_df), 
  color = sample_df$Group,
  width = 900, height = 600,
  marker = list(size = 6, line = list(width = 1, color = 'DarkSlateGray'))
) %>%
  add_markers() %>%
  layout(
    scene = list(
      xaxis = list(title = paste0('PC1: ', round(pca_var[1], 2), '%')),
      yaxis = list(title = paste0('PC2: ', round(pca_var[2], 2), '%')),
      zaxis = list(title = paste0('PC3: ', round(pca_var[3], 2), '%')),
      aspectmode = 'cube',  # 设置坐标轴比例为立方体
      aspectratio = list(x = 1, y = 1, z = 1)  # 设置三个轴的比例都为1
    ),
    template = "simple_white"
  )
saveWidget(fig_3d, file = opt$output_html, selfcontained = TRUE)
